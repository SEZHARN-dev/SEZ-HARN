{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3juA8-pbZXvL","executionInfo":{"status":"ok","timestamp":1684247824713,"user_tz":-330,"elapsed":25754,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"ffc8d910-4a5c-4dd3-ca2d-de62c387436f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B37O-QG2ZSWW","executionInfo":{"status":"ok","timestamp":1684247829622,"user_tz":-330,"elapsed":4914,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"17327a44-9690-4f62-96ea-c4737f55e966"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkIfhXqxWhOw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684247839490,"user_tz":-330,"elapsed":9870,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"f38da188-7e7f-4298-924f-2a1b6869c0b6"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-bfb45282ee18>:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import math\n","\n","import torchinfo\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8"]},{"cell_type":"code","source":["class norm_data(nn.Module):\n","    def __init__(self, dim=3, joints=20):\n","        super(norm_data, self).__init__()\n","\n","        self.bn = nn.BatchNorm1d(dim*joints)\n","\n","    def forward(self, x):\n","        bs, c, num_joints, step = x.size()\n","        x = x.view(bs, -1, step)\n","        x = self.bn(x)\n","        x = x.view(bs, -1, num_joints, step).contiguous()\n","        return x\n","\n","class embed(nn.Module):\n","    def __init__(self, dim=3, joint=20, hidden_dim=128, norm=True, bias=False):\n","        super(embed, self).__init__()\n","\n","        if norm:\n","            self.cnn = nn.Sequential(\n","                norm_data(dim, joint),\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","        else:\n","            self.cnn = nn.Sequential(\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class cnn1x1(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 =3, bias = True):\n","        super(cnn1x1, self).__init__()\n","        self.cnn = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class local(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 = 3, bias = False):\n","        super(local, self).__init__()\n","        self.maxpool = nn.AdaptiveMaxPool2d((1, None))\n","        self.cnn1 = nn.Conv2d(dim1, dim1, kernel_size=(1, 3), padding=(0, 1), bias=bias)\n","        self.bn1 = nn.BatchNorm2d(dim1)\n","        self.relu = nn.ReLU()\n","        self.cnn2 = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","        self.bn2 = nn.BatchNorm2d(dim2)\n","        self.dropout = nn.Dropout2d(0.2)\n","\n","    def forward(self, x1):\n","        x1 = self.maxpool(x1)\n","        x = self.cnn1(x1)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.cnn2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x\n","\n","class gcn_spa(nn.Module):\n","    def __init__(self, in_feature, out_feature, bias = False):\n","        super(gcn_spa, self).__init__()\n","        self.bn = nn.BatchNorm2d(out_feature)\n","        self.relu = nn.ReLU()\n","        self.w = cnn1x1(in_feature, out_feature, bias=False)\n","        self.w1 = cnn1x1(in_feature, out_feature, bias=bias)\n","\n","\n","    def forward(self, x1, g):\n","        x = x1.permute(0, 3, 2, 1).contiguous()\n","        x = g.matmul(x)\n","        x = x.permute(0, 3, 2, 1).contiguous()\n","        x = self.w(x) + self.w1(x1)\n","        x = self.relu(self.bn(x))\n","        return x\n","\n","class compute_g_spa(nn.Module):\n","    def __init__(self, dim1 = 64 *3, dim2 = 64*3, bias = False):\n","        super(compute_g_spa, self).__init__()\n","        self.dim1 = dim1\n","        self.dim2 = dim2\n","        self.g1 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.g2 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x1):\n","\n","        g1 = self.g1(x1).permute(0, 3, 2, 1).contiguous()\n","        g2 = self.g2(x1).permute(0, 3, 1, 2).contiguous()\n","        g3 = g1.matmul(g2)\n","        g = self.softmax(g3)\n","        return g\n","    \n","\n","class SGN(nn.Module):\n","    def __init__(self, num_joint, seg, hidden_size=128, bs=32, is_3d=True, train=True, bias=True, device='cpu'):\n","        super(SGN, self).__init__()\n","\n","        self.dim1 = hidden_size\n","        self.dim_unit = hidden_size // 4 \n","        self.seg = seg\n","        self.num_joint = num_joint\n","        self.bs = bs\n","\n","        if is_3d:\n","          self.spatial_dim = 3\n","        else:\n","          self.spatial_dim = 2\n","\n","        if train:\n","            self.spa = self.one_hot(bs, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(bs, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","        else:\n","            self.spa = self.one_hot(32 * 5, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(32 * 5, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","\n","        self.tem_embed = embed(self.seg, joint=self.num_joint, hidden_dim=self.dim_unit*4, norm=False, bias=bias)\n","        self.spa_embed = embed(num_joint, joint=self.num_joint, hidden_dim=self.dim_unit, norm=False, bias=bias)\n","        self.joint_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.dif_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n","        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n","        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n","        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n","        \n","        self.embed_maxpool = nn.AdaptiveMaxPool2d([self.dim1, 2])\n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","\n","        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n","\n","\n","    def forward(self, input):\n","        \n","        # Dynamic Representation\n","        input = input.view((self.bs, self.seg, self.num_joint, self.spatial_dim))\n","        input = input.permute(0, 3, 2, 1).contiguous()\n","        dif = input[:, :, :, 1:] - input[:, :, :, 0:-1]\n","        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n","        # print(input.shape)\n","        pos = self.joint_embed(input)\n","        tem1 = self.tem_embed(self.tem)\n","        spa1 = self.spa_embed(self.spa)\n","        dif = self.dif_embed(dif)\n","        dy = pos + dif\n","        # Joint-level Module\n","        input= torch.cat([dy, spa1], 1)\n","        g = self.compute_g1(input)\n","        input = self.gcn1(input, g)\n","        input = self.gcn2(input, g)\n","        input = self.gcn3(input, g)\n","        # Frame-level Module\n","        input = input + tem1\n","        input = self.cnn(input)\n","        output_feat = torch.squeeze(input)\n","        output_feat = self.embed_maxpool(output_feat)\n","        output_feat = torch.flatten(output_feat, 1)\n","\n","        return output_feat\n","\n","    def one_hot(self, bs, spa, tem):\n","\n","        y = torch.arange(spa).unsqueeze(-1)\n","        y_onehot = torch.FloatTensor(spa, spa)\n","\n","        y_onehot.zero_()\n","        y_onehot.scatter_(1, y, 1)\n","\n","        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n","        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n","\n","        return y_onehot\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","class BiLSTMDecoder(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMDecoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","\n","        if bidirectional:\n","            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n","        else:\n","            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n","\n","        # define LSTM layer\n","        self.layers = []\n","        # add lstm\n","        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=True,\n","                            batch_first=bidirectional)\n","\n","                        \n","        # add linear layers \n","        if bidirectional:\n","            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n","        else:\n","            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n","\n","        for __id,layer_in in enumerate(self.linear_filters):\n","            if __id == len(linear_filters)-1:\n","                self.layers.append(nn.Linear(layer_in,self.input_size))\n","            else:\n","                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n","\n","        self.net = nn.Sequential(*self.layers)\n","\n","        \n","        \n","\n","    def forward(self,encoder_hidden):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        \n","        hidden_shape = encoder_hidden.shape\n","        encoder_hidden = self.input_linear(encoder_hidden)\n","        \n","        if self.bidirectional:\n","            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h1,h2,c1,c2 = torch.unbind(hidden,0)\n","            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n","            bs = h.size()[1]\n","        else:\n","            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h,c = torch.unbind(hidden,0)\n","            bs = h.size()[1]\n","        \n","        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n","        \n","        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n","        x = self.net(lstm_out)\n","        \n","        return x\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","        "],"metadata":{"id":"QNhRUvwyFUQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=128\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_config(file_loc,device):\n","    file = torch.load(file_loc,map_location=device)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","\n","model_params, model_config, config = get_config(\n","    f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_NTURGB120_skeleton_SGN_classifier/10__epoch50_emb400_xy.pt\",\n","    device\n","    )"],"metadata":{"id":"LnVVDWf8YulU","executionInfo":{"status":"ok","timestamp":1684249047243,"user_tz":-330,"elapsed":343,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["model_config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGBvkVM4ApJX","executionInfo":{"status":"ok","timestamp":1684249035883,"user_tz":-330,"elapsed":611,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"9771768e-58e3-4c62-9a06-8f7bf6d29f6b"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'num_joint': 12,\n"," 'seq_len': 60,\n"," 'decoder_hidden_size': 1024,\n"," 'linear_filters': [128, 256, 512, 1024],\n"," 'embedding_size': 1024,\n"," 'num_classes': 82,\n"," 'num_layers': 1,\n"," 'is_3d': False,\n"," 'bidirectional': True,\n"," 'batch_size': 128,\n"," 'dev': device(type='cuda'),\n"," 'encoder_hidden_size': 512,\n"," 'input_size': 24}"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["encoder = SGN( \n","    num_joint=config[\"model\"][\"num_joint\"], \n","    seg=config[\"model\"][\"seq_len\"], \n","    hidden_size=config[\"model\"][\"encoder_hidden_size\"], \n","    bs=batch_size, \n","    is_3d=config[\"model\"][\"is_3d\"],\n","    device = device,\n","    train=True).to(device)\n","\n","classifier = SGNClassifier(\n","    num_classes=config[\"model\"][\"num_classes\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n",").to(device)\n","\n","decoder = BiLSTMDecoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    hidden_size=config[\"model\"][\"decoder_hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=device).to(device)\n","\n","bilstm_model = EncDecModel(\n","    encoder = encoder,\n","    decoder = decoder,\n","    classifier = classifier\n",").to(device)"],"metadata":{"id":"SxOl8YuW6f71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bilstm_model.load_state_dict(model_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2XFouQeZDFW","executionInfo":{"status":"ok","timestamp":1684248368583,"user_tz":-330,"elapsed":5,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"6de8544e-5d6a-4691-a106-43455765dded"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["encoder = bilstm_model.encoder\n","decoder = bilstm_model.decoder"],"metadata":{"id":"2mfeX5hXdGZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder(torch.rand(128,60,24).to(device)).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZT-IBb4dMIX","executionInfo":{"status":"ok","timestamp":1684248377955,"user_tz":-330,"elapsed":6060,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"0da5c35c-227a-42d9-b304-b02639aebb91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([128, 400])"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["decoder(torch.rand(128,400).to(device)).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O71wDyTrdfRY","executionInfo":{"status":"ok","timestamp":1684248385651,"user_tz":-330,"elapsed":7712,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"2e447527-0a22-4832-f2bf-4bef13861411"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([128, 60, 24])"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","source":["torchinfo.summary(bilstm_model, input_size=(batch_size, config[\"model\"][\"seq_len\"], config[\"model\"][\"input_size\"]), col_names=(\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBdLYSVvL0f3","executionInfo":{"status":"ok","timestamp":1684248399450,"user_tz":-330,"elapsed":13801,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"7540df6b-4727-478d-dc92-c163d0d279b9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================================================================================================\n","Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n","==========================================================================================================================================================================\n","EncDecModel                                   [128, 60, 24]             [128, 60, 24]             --                        --                        --\n","├─SGN: 1-1                                    [128, 60, 24]             [128, 400]                --                        --                        --\n","│    └─embed: 2-1                             [128, 2, 12, 60]          [128, 50, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-1                   [128, 2, 12, 60]          [128, 50, 12, 60]         3,490                     --                        317,220,864\n","│    └─embed: 2-2                             [128, 60, 12, 60]         [128, 200, 12, 60]        --                        --                        --\n","│    │    └─Sequential: 3-2                   [128, 60, 12, 60]         [128, 200, 12, 60]        16,904                    --                        1,557,872,640\n","│    └─embed: 2-3                             [128, 12, 12, 60]         [128, 50, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-3                   [128, 12, 12, 60]         [128, 50, 12, 60]         4,082                     --                        376,197,120\n","│    └─embed: 2-4                             [128, 2, 12, 60]          [128, 50, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-4                   [128, 2, 12, 60]          [128, 50, 12, 60]         3,490                     --                        317,220,864\n","│    └─compute_g_spa: 2-5                     [128, 100, 12, 60]        [128, 60, 12, 12]         --                        --                        --\n","│    │    └─cnn1x1: 3-5                       [128, 100, 12, 60]        [128, 200, 12, 60]        20,200                    --                        1,861,632,000\n","│    │    └─cnn1x1: 3-6                       [128, 100, 12, 60]        [128, 200, 12, 60]        20,200                    --                        1,861,632,000\n","│    │    └─Softmax: 3-7                      [128, 60, 12, 12]         [128, 60, 12, 12]         --                        --                        --\n","│    └─gcn_spa: 2-6                           [128, 100, 12, 60]        [128, 100, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-8                       [128, 100, 12, 60]        [128, 100, 12, 60]        10,000                    --                        921,600,000\n","│    │    └─cnn1x1: 3-9                       [128, 100, 12, 60]        [128, 100, 12, 60]        10,100                    --                        930,816,000\n","│    │    └─BatchNorm2d: 3-10                 [128, 100, 12, 60]        [128, 100, 12, 60]        200                       --                        25,600\n","│    │    └─ReLU: 3-11                        [128, 100, 12, 60]        [128, 100, 12, 60]        --                        --                        --\n","│    └─gcn_spa: 2-7                           [128, 100, 12, 60]        [128, 200, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-12                      [128, 100, 12, 60]        [128, 200, 12, 60]        20,000                    --                        1,843,200,000\n","│    │    └─cnn1x1: 3-13                      [128, 100, 12, 60]        [128, 200, 12, 60]        20,200                    --                        1,861,632,000\n","│    │    └─BatchNorm2d: 3-14                 [128, 200, 12, 60]        [128, 200, 12, 60]        400                       --                        51,200\n","│    │    └─ReLU: 3-15                        [128, 200, 12, 60]        [128, 200, 12, 60]        --                        --                        --\n","│    └─gcn_spa: 2-8                           [128, 200, 12, 60]        [128, 200, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-16                      [128, 200, 12, 60]        [128, 200, 12, 60]        40,000                    --                        3,686,400,000\n","│    │    └─cnn1x1: 3-17                      [128, 200, 12, 60]        [128, 200, 12, 60]        40,200                    --                        3,704,832,000\n","│    │    └─BatchNorm2d: 3-18                 [128, 200, 12, 60]        [128, 200, 12, 60]        400                       --                        51,200\n","│    │    └─ReLU: 3-19                        [128, 200, 12, 60]        [128, 200, 12, 60]        --                        --                        --\n","│    └─local: 2-9                             [128, 200, 12, 60]        [128, 400, 1, 60]         --                        --                        --\n","│    │    └─AdaptiveMaxPool2d: 3-20           [128, 200, 12, 60]        [128, 200, 1, 60]         --                        --                        --\n","│    │    └─Conv2d: 3-21                      [128, 200, 1, 60]         [128, 200, 1, 60]         120,200                   [1, 3]                    923,136,000\n","│    │    └─BatchNorm2d: 3-22                 [128, 200, 1, 60]         [128, 200, 1, 60]         400                       --                        51,200\n","│    │    └─ReLU: 3-23                        [128, 200, 1, 60]         [128, 200, 1, 60]         --                        --                        --\n","│    │    └─Dropout2d: 3-24                   [128, 200, 1, 60]         [128, 200, 1, 60]         --                        --                        --\n","│    │    └─Conv2d: 3-25                      [128, 200, 1, 60]         [128, 400, 1, 60]         80,400                    [1, 1]                    617,472,000\n","│    │    └─BatchNorm2d: 3-26                 [128, 400, 1, 60]         [128, 400, 1, 60]         800                       --                        102,400\n","│    │    └─ReLU: 3-27                        [128, 400, 1, 60]         [128, 400, 1, 60]         --                        --                        --\n","│    └─AdaptiveMaxPool2d: 2-10                [128, 400, 60]            [128, 200, 2]             --                        --                        --\n","├─SGNClassifier: 1-2                          [128, 400]                [128, 82]                 --                        --                        --\n","│    └─Linear: 2-11                           [128, 400]                [128, 82]                 32,882                    --                        4,208,896\n","├─BiLSTMDecoder: 1-3                          [128, 400]                [128, 60, 24]             --                        --                        --\n","│    └─Linear: 2-12                           [128, 400]                [128, 4096]               1,642,496                 --                        210,239,488\n","│    └─LSTM: 2-13                             [128, 60, 1024]           [128, 60, 2048]           16,793,600                --                        128,974,848,000\n","│    └─Sequential: 2-14                       [128, 60, 2048]           [128, 60, 24]             --                        --                        --\n","│    │    └─Linear: 3-28                      [128, 60, 2048]           [128, 60, 1024]           2,098,176                 --                        268,566,528\n","│    │    └─Linear: 3-29                      [128, 60, 1024]           [128, 60, 512]            524,800                   --                        67,174,400\n","│    │    └─Linear: 3-30                      [128, 60, 512]            [128, 60, 256]            131,328                   --                        16,809,984\n","│    │    └─Linear: 3-31                      [128, 60, 256]            [128, 60, 128]            32,896                    --                        4,210,688\n","│    │    └─Linear: 3-32                      [128, 60, 128]            [128, 60, 24]             3,096                     --                        396,288\n","==========================================================================================================================================================================\n","Total params: 21,670,940\n","Trainable params: 21,670,940\n","Non-trainable params: 0\n","Total mult-adds (G): 150.33\n","==========================================================================================================================================================================\n","Input size (MB): 0.74\n","Forward/backward pass size (MB): 2173.85\n","Params size (MB): 86.68\n","Estimated Total Size (MB): 2261.27\n","=========================================================================================================================================================================="]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":[],"metadata":{"id":"xk5C9JWTBbq_"},"execution_count":null,"outputs":[]}]}