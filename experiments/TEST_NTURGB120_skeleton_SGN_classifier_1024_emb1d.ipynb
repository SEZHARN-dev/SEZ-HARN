{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"4921a522de6e4721a0153b27847e6993":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a149c1ea8de4943a9e061b346e073c5","IPY_MODEL_5f8545f7120d47179a93395bb90b7d88","IPY_MODEL_198e9d5a4b8041bbb7fa0281bf2e6d00"],"layout":"IPY_MODEL_9a67b299d46d47a2803c388534068b2e"}},"4a149c1ea8de4943a9e061b346e073c5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a717ccfe35a4778b8f846607b3331fb","placeholder":"​","style":"IPY_MODEL_a118cf6456f74df09a6d934e3e3b3f7c","value":"eval:   0%"}},"5f8545f7120d47179a93395bb90b7d88":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_0557b08a73d54aeab518821f6d5028fb","max":229,"min":0,"orientation":"horizontal","style":"IPY_MODEL_67c661fb8a01428bb164961efba3e5e2","value":0}},"198e9d5a4b8041bbb7fa0281bf2e6d00":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25b7e30fcf8542cfb883340e8049b7b6","placeholder":"​","style":"IPY_MODEL_2d29d3b598ba4de3a4b3bd1a7bce85e6","value":" 0/229 [00:53&lt;?, ?batch/s]"}},"9a67b299d46d47a2803c388534068b2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a717ccfe35a4778b8f846607b3331fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a118cf6456f74df09a6d934e3e3b3f7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0557b08a73d54aeab518821f6d5028fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67c661fb8a01428bb164961efba3e5e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"25b7e30fcf8542cfb883340e8049b7b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d29d3b598ba4de3a4b3bd1a7bce85e6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"EnmESXTBv5qX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682830001558,"user_tz":-330,"elapsed":36935,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"ca38faf7-34b9-46d7-bf69-c9645fe25c70"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! pip install --q torchinfo\n","! git clone https://github.com/nipdep/HAR-ZSL-XAI.git --branch AE --single-branch\n","! mv /content/HAR-ZSL-XAI/AETraining/dataset /content/"],"metadata":{"id":"MiO-ye1zBkOE","executionInfo":{"status":"ok","timestamp":1682830022731,"user_tz":-330,"elapsed":21181,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c321014f-dae1-42a6-bf68-f2b4760e16b3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'HAR-ZSL-XAI'...\n","remote: Enumerating objects: 1508, done.\u001b[K\n","remote: Counting objects: 100% (28/28), done.\u001b[K\n","remote: Compressing objects: 100% (21/21), done.\u001b[K\n","remote: Total 1508 (delta 14), reused 12 (delta 7), pack-reused 1480\u001b[K\n","Receiving objects: 100% (1508/1508), 147.00 MiB | 13.37 MiB/s, done.\n","Resolving deltas: 100% (695/695), done.\n","Updating files: 100% (1163/1163), done.\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkIfhXqxWhOw","executionInfo":{"status":"ok","timestamp":1682830165600,"user_tz":-330,"elapsed":469,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"a20c98b7-1773-402c-e8bb-a8da88d13702"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-27-38921e47e0df>:83: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n"]}],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","import torchinfo\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","\n","\"\"\"\n","Collection of functions which enable the evaluation of a classifier's performance,\n","by showing confusion matrix, accuracy, recall, precision etc.\n","\"\"\"\n","\n","import numpy as np\n","import sys\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","\n","def save_history(history, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","\n","    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n","        json.dump(history, f0)\n","\n","def get_config(file_loc,device):\n","    file = torch.load(file_loc,map_location=device)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","    \n","def save_model(model, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","    torch.save({\n","        \"n_epochs\": config[\"n_epochs\"],\n","        \"model_state_dict\": model.state_dict(),\n","        \"model_config\": config[\"model\"],\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}.pt\")\n","\n","def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n","    \"\"\"Plot confusion matrix in a separate window\"\"\"\n","    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    if label_strings:\n","        tick_marks = np.arange(len(label_strings))\n","        plt.xticks(tick_marks, label_strings, rotation=90)\n","        plt.yticks(tick_marks, label_strings)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n","    \"\"\"\n","    Returns a string of a report for given metric arrays (array length equals the number of classes).\n","    Called internally by `analyze_classification`.\n","        digits: number of digits after . for displaying results\n","        number_of_thieves: number of biggest thieves to report\n","        maxcharlength: max. number of characters to use when displaying thief names\n","    \"\"\"\n","\n","    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n","    sorted_class_indices = np.argsort(relative_freq)[\n","                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n","\n","    last_line_heading = 'avg / total'\n","\n","    width = max(len(cn) for cn in existing_class_names)\n","    width = max(width, len(last_line_heading), digits)\n","\n","    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n","    fmt = '%% %ds' % width  # first column: class name\n","    fmt += '  '\n","    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n","    fmt += '|\\t % 5s'\n","    fmt += '\\n'\n","\n","    headers = [\"\"] + headers\n","    report = fmt % tuple(headers)\n","    report += '\\n'\n","\n","    for i in sorted_class_indices:\n","        values = [existing_class_names[i]]\n","        for v in (precision[i], recall[i], f1[i],\n","                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n","            values += [\"{0:0.{1}f}\".format(v, digits)]\n","        values += [\"{}\".format(support[i])]\n","        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n","                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n","        thieves = thieves[thieves != i]  # exclude self at this point\n","        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n","        thieves_names = [\n","            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n","            in thieves]  # a little inefficient but inconsequential\n","        string_about_stealing = \"\"\n","        for j in range(len(thieves)):\n","            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n","        values += [string_about_stealing]\n","\n","        report += fmt % tuple(values)\n","\n","    report += '\\n' + 100 * '-' + '\\n'\n","\n","    # compute averages/sums\n","    values = [last_line_heading]\n","    for v in (np.average(precision, weights=relative_freq),\n","                np.average(recall, weights=relative_freq),\n","                np.average(f1, weights=relative_freq)):\n","        values += [\"{0:0.{1}f}\".format(v, digits)]\n","    values += ['{0}'.format(np.sum(relative_freq))]\n","    values += ['{0}'.format(np.sum(support))]\n","    values += ['']\n","\n","    # make last (\"Total\") line for report\n","    report += fmt % tuple(values)\n","\n","    return report\n","\n","\n","def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n","    \"\"\"\n","    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n","    Input:\n","        y_pred: 1D array of predicted labels (class indices)\n","        y_true: 1D array of true labels (class indices)\n","        class_names: 1D array or list of class names in the order of class indices.\n","            Could also be integers [0, 1, ..., num_classes-1].\n","        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n","    \"\"\"\n","\n","    # Trim class_names to include only classes existing in y_pred OR y_true\n","    in_pred_labels = set(list(y_pred))\n","    in_true_labels = set(list(y_true))\n","    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n","\n","    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n","    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n","    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n","    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n","\n","    # Confusion matrix\n","    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n","\n","    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n","    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n","\n","    if show_plot:\n","        plt.figure()\n","        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n","                                title='Confusion matrix normalized by row')\n","        plt.show(block=False)\n","\n","    # Analyze results\n","    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n","    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n","\n","    # returns metrics for each class, in the same order as existing_class_names\n","    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n","    # Print report\n","    if print_report:\n","        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n","\n","    # Calculate average precision and recall\n","    # prec_avg, rec_avg = get_avg_prec_recall(ConfMatrix, existing_class_names, excluded_classes)\n","    # if excluded_classes:\n","    #     print(\n","    #         \"\\nAverage PRECISION: {:.2f}\\n(using class frequencies as weights, excluding classes with no predictions and predictions in '{}')\".format(\n","    #             prec_avg, ', '.join(excluded_classes)))\n","    #     print(\n","    #         \"\\nAverage RECALL (= ACCURACY): {:.2f}\\n(using class frequencies as weights, excluding classes in '{}')\".format(\n","    #             rec_avg, ', '.join(excluded_classes)))\n","\n","    # Make a histogram with the distribution of classes with respect to precision and recall\n","    # prec_rec_histogram(precision, recall)\n","\n","    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id"],"metadata":{"id":"p0XIAvcQX3l0","executionInfo":{"status":"ok","timestamp":1682830030938,"user_tz":-330,"elapsed":5,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["model_ident = \"NTURGB120_skeleton_SGN_classifier_emb1d\"\n","unique_iden = \"epoch50_emb1024_xy\"\n","\n","main_dir = \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE\"\n","data_dir = os.path.join(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/skel\")\n","remove_files = [\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/NTU_RGBD120_samples_with_missing_skeletons.txt\",\n","                \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/NTU_RGBD_samples_with_missing_skeletons.txt\"]\n","\n","epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n","models_saves = os.path.join(main_dir,\"model_saves\")\n","embeddings_save = os.path.join(main_dir,\"embedding_save\")\n","prototypes_save = os.path.join(main_dir,\"prototypes\")\n","test_vids = os.path.join(main_dir,\"test_vids\")\n","train_ratio = 0.90\n","val_ratio = 0.1\n","batch_size = 32\n","\n","os.makedirs(epoch_vids,exist_ok=True)\n","os.makedirs(models_saves,exist_ok=True)\n","os.makedirs(embeddings_save,exist_ok=True)\n","\n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/nturgb120_label_map.json\",\"r\") as f0:\n","    full_id2cls = json.load(f0)\n","    \n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/sel_cls_list - Single_person.txt\",\"r\") as f0:\n","    class_names = [full_id2cls[x] for x in f0.read().split(\" \")]"],"metadata":{"id":"y7gAeWUDX7X6","executionInfo":{"status":"ok","timestamp":1682830033461,"user_tz":-330,"elapsed":2526,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["config = {\n","    \"n_epochs\":20,\n","    \"model_name\":\"BidirectionalLSTM\",\n","    \"model\":{\n","        \"num_joint\":12,\n","        \"seq_len\":60,\n","        \"decoder_hidden_size\":1024,\n","        \"linear_filters\":[128,256,512,1024],\n","        \"embedding_size\":1024,\n","        \"num_classes\":len(class_names),\n","        \"num_layers\":1,\n","        \"is_3d\":False,\n","        \"bidirectional\":True,\n","        \"batch_size\":batch_size,\n","        \"dev\":device\n","        },\n","    \"lr\":1e-4,\n","    'alpha_target': 0.9\n","}\n","\n","config[\"model\"][\"encoder_hidden_size\"] = config[\"model\"][\"embedding_size\"]//2\n","config[\"model\"][\"input_size\"] = config[\"model\"][\"num_joint\"]*3 if config[\"model\"][\"is_3d\"] else config[\"model\"][\"num_joint\"]*2\n","\n","id2clsname, clsname2id = classname_id(class_names)"],"metadata":{"id":"IrsB7xhhaKF5","executionInfo":{"status":"ok","timestamp":1682830033461,"user_tz":-330,"elapsed":7,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrpFnUcsHcDo","executionInfo":{"status":"ok","timestamp":1682830033461,"user_tz":-330,"elapsed":6,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"5545b673-5397-408d-a873-9c4a341587b8"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'n_epochs': 20,\n"," 'model_name': 'BidirectionalLSTM',\n"," 'model': {'num_joint': 12,\n","  'seq_len': 60,\n","  'decoder_hidden_size': 1024,\n","  'linear_filters': [128, 256, 512, 1024],\n","  'embedding_size': 1024,\n","  'num_classes': 82,\n","  'num_layers': 1,\n","  'is_3d': False,\n","  'bidirectional': True,\n","  'batch_size': 32,\n","  'dev': device(type='cpu'),\n","  'encoder_hidden_size': 512,\n","  'input_size': 24},\n"," 'lr': 0.0001,\n"," 'alpha_target': 0.9}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from dataset.SkeletonData.data import *\n","\n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/shapes_keys.json\",\"r\") as f0:\n","    id2shapes = json.load(f0)\n","\n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/id_list.txt\",\"r\") as f0:\n","    id_list = f0.read().strip().split(\"\\n\")\n","\n","files_to_remove = set()\n","for __f in remove_files:\n","    with open(__f,\"r\") as f0:\n","        for val in f0.read().split(\"\\n\"):\n","            files_to_remove.add(val)\n","\n","print(\"Number of Files to remove:= \",len(files_to_remove))\n","\n","total_files = set([x.split(\".\")[0] for x in id_list]) - files_to_remove\n","total_files_loc = set([f\"{os.path.join(data_dir,x)}.skeleton\" for x in total_files])\n","\n","#split list\n","rows = [(full_id2cls[str(int(x.split(\".\")[0][-3:]))],x) for x in total_files_loc]\n","info_pd = pd.DataFrame(data=rows,columns=[\"target\",\"file_loc\"])\n","\n","#select needed classes.\n","info_pd = info_pd.loc[info_pd[\"target\"].isin(class_names)]\n","train_df, val_df = train_test_split(info_pd,stratify=info_pd[\"target\"],train_size=train_ratio)\n","\n","print(\"Number of Files to Total:= \",len(total_files))\n","\n","train_builder = SkeletonFileBuilder(file_names=set(train_df[\"file_loc\"].to_list()),ignore_files=remove_files)\n","val_builder = SkeletonFileBuilder(file_names=set(val_df[\"file_loc\"].to_list()),ignore_files=remove_files)\n","\n","print(\"Number of Files to Train:= \",len(train_builder))\n","print(\"Number of Files to Val:= \",len(val_builder))\n","\n","train_file_iterator = iter(train_builder)\n","val_file_iterator = iter(val_builder)"],"metadata":{"id":"25RyOW-8gA46","executionInfo":{"status":"ok","timestamp":1682830035574,"user_tz":-330,"elapsed":2116,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a818f265-d8b4-4f12-a1d6-74e692234bac"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of Files to remove:=  535\n","Number of Files to Total:=  73600\n","Number of Files to Train:=  66240\n","Number of Files to Val:=  7360\n"]}]},{"cell_type":"code","source":["def load_file_to_memory(id2shape,save_dict,each_file):\n","  file_id = each_file.filepath.split(os.path.sep)[-1].split(\".\")[0]\n","  #if not os.path.exists(f\"{data_dir}/{file_id}.skeleton\"):\n","  #  return None\n","\n","  num_frame, body_data = each_file.load_data()\n","  orig_vid_size = id2shape[file_id]\n","  \n","  #for frame_data in body_data:\n","  #  if frame_data[\"body_count\"] != 1:\n","  #      return None\n","  \n","  skel_data = []\n","  for frame_data in body_data:\n","      frame_jd = []\n","      for jd in frame_data[\"bodies\"][0][\"joint_details\"]:\n","          x = jd[\"colorX\"] / orig_vid_size[1]\n","          y = jd[\"colorY\"] / orig_vid_size[0]\n","\n","          frame_jd.append([x, y])\n","\n","      skel_data.append(frame_jd)\n","\n","  skel_data = np.asarray(skel_data)\n","  save_dict[file_id] = (file_id,orig_vid_size,str(int(file_id[-3:])),skel_data)\n","  return file_id\n","\n","class SkeletonDataset(Dataset):\n","    def __init__(self,\n","                 data_builder, \n","                 fileid2shape,\n","                 full_label_map,\n","                 cls2id,\n","                 data=None,\n","                 transform=None,\n","                 seq_len = 100,\n","                 window_size = 200,\n","                 target_transform=None,\n","                 active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28],\n","                 file_name=False, \n","                 is_2d=False):\n","        self.data_builder = data_builder\n","        self.data = data\n","        self.transform = transform\n","        self.fileid2shape = fileid2shape\n","        self.window_size = window_size\n","        self.seq_len = seq_len\n","        self.target_transform = target_transform\n","        self.active_locations = active_locations\n","        self.file_name = file_name\n","        self.is_2d = is_2d\n","        self.cls2id = cls2id\n","        self.full_label_map = full_label_map\n","\n","        if self.active_locations:\n","          self.join_translation_map = {k:i for i,k in enumerate(self.active_locations)}\n","        \n","        if not self.data:\n","          self.data = {}\n","          with ThreadPoolExecutor() as executor:\n","            self.indexes = list(\n","                tqdm(\n","                  executor.map(\n","                    partial(load_file_to_memory,self.fileid2shape,self.data),\n","                    self.data_builder), \n","                  total=len(self.data_builder),\n","                  desc=\"Loaded Files\"\n","                )\n","            )\n","\n","          self.full_indexes = [x for x in self.indexes if x != None]\n","        else:\n","          self.full_indexes = list(self.data.keys())\n","          \n","        self.indexes = random.sample(self.full_indexes,(len(self.full_indexes)//batch_size)*batch_size)\n","    \n","\n","    def __len__(self):\n","        return len(self.indexes)\n","      \n","    def select_frames(self,sequence):\n","      if sequence.shape[0]<self.seq_len:\n","        times = self.seq_len//sequence.shape[0] + 1\n","\n","        sequence = sequence.repeat(times,1,1)\n","\n","      if sequence.shape[0]>self.window_size:\n","        start = random.randint(0,sequence.shape[0]-self.window_size-1)\n","        sequence = sequence[start:start+self.window_size,...]\n","                               \n","      sel_index = sorted(random.sample(range(sequence.shape[0]),self.seq_len))\n","        \n","      return sequence[sel_index,...]\n","    \n","    def create_connection_map(self,original_map):\n","      if not self.active_locations:\n","        return original_map\n","      \n","      all_possible_comb = product(self.active_locations,self.active_locations)\n","      all_possible_comb = set(all_possible_comb)\n","      \n","      original_map = set(original_map)\n","      sel_connections = list(all_possible_comb.intersection(original_map))\n","      sel_connections = [(self.join_translation_map[x[0]],self.join_translation_map[x[1]]) for x in sel_connections]\n","      \n","      return sel_connections \n","\n","    def __getitem__(self, idx):\n","        idx = self.indexes[idx]\n","        \n","        orig_target = self.data[idx][2]\n","        file_path = self.data[idx][0]\n","        vid_size = self.data[idx][1]\n","        coords = self.data[idx][3]\n","        \n","        target = self.cls2id[self.full_label_map[orig_target]]\n","        \n","        if self.active_locations:\n","          coords = coords[:,self.active_locations,:]\n","\n","        if self.is_2d:\n","            coords = coords[...,0:2]\n","\n","        coords = torch.from_numpy(coords).float()\n","        coords = self.select_frames(coords)\n","\n","        shape = coords.shape\n","        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n","        label = torch.clone(coords)\n","\n","        if self.transform:\n","            coords = self.transform(coords)\n","        if self.target_transform:\n","            label = self.target_transform(coords)\n","\n","        coords[torch.isnan(coords)] = 0\n","        label[torch.isnan(label)] = 0\n","\n","        if self.file_name:\n","            return coords, label, target,vid_size,file_path\n","        return coords, label, target,vid_size"],"metadata":{"id":"Res0UAsOjjki","executionInfo":{"status":"ok","timestamp":1682830035575,"user_tz":-330,"elapsed":6,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["  with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/val_data.pkl\",\"rb\") as f0:\n","    val_data_load = pickle.load(f0)"],"metadata":{"id":"zLF82zVTWDnw","executionInfo":{"status":"ok","timestamp":1682830037566,"user_tz":-330,"elapsed":1995,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["val_ds = SkeletonDataset(val_file_iterator,\n","                           id2shapes,\n","                           full_id2cls,\n","                           clsname2id,\n","                           data=val_data_load,\n","                           seq_len=config[\"model\"][\"seq_len\"],\n","                           file_name=True,\n","                           is_2d=True,\n","                           active_locations=[10,9,8,4,5,6,\n","                                               16,17,18,12,13,14])"],"metadata":{"id":"o0Mlr88WEjWt","executionInfo":{"status":"ok","timestamp":1682830037566,"user_tz":-330,"elapsed":13,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["len(val_ds.indexes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iThGfDYvch4Q","executionInfo":{"status":"ok","timestamp":1682830037566,"user_tz":-330,"elapsed":11,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"e78e7aaf-0306-42e9-dd32-0a8ed0d3abe0"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7328"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["#train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n","#test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"],"metadata":{"id":"B6mtux6OEmIk","executionInfo":{"status":"ok","timestamp":1682830037566,"user_tz":-330,"elapsed":9,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def gen_skeleton(frame, \n","                 height,\n","                 width,\n","                 mapping_list = [(0, 1), (1, 3), (3, 5), \n","                                 (0, 2), (2, 4), (0, 6), \n","                                 (1, 7), (6, 7), (6, 8), \n","                                 (7, 9), (8, 10), (9, 11)]):\n","    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n","    img_3.fill(255)\n","\n","    # add circles\n","    for coord in frame:\n","        x, y = int(width*coord[0]), int(height*coord[1])\n","        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n","\n","    # add lines\n","    for line in mapping_list:\n","        i, j = line\n","        st = frame[i, :]\n","        start_point = (int(width*st[0]), int(height*st[1]))\n","\n","        en = frame[j, :]\n","        end_point = (int(width*en[0]), int(height*en[1]))\n","\n","        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n","\n","    return img_3\n","\n","def gen_video(points, \n","              save_file, \n","              frame_h, \n","              frame_w, \n","              is_3d=True,\n","              mapping_list = [(0, 1), (1, 3), (3, 5), \n","                                 (0, 2), (2, 4), (0, 6), \n","                                 (1, 7), (6, 7), (6, 8), \n","                                 (7, 9), (8, 10), (9, 11)]):\n","    # make 3D if points are flatten\n","    if len(points.shape) == 2:\n","        if is_3d:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 3))\n","          y_cds = list(range(1, fts, 3))\n","          z_cds = list(range(2, fts, 3))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds], \n","                                          points[:, z_cds]]), (1,2,0))\n","        else:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 2))\n","          y_cds = list(range(1, fts, 2))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds]]), (1,2,0))\n","\n","    size = (frame_w, frame_h)\n","    result = cv2.VideoWriter(save_file,\n","                         cv2.VideoWriter_fourcc(*'MJPG'),\n","                         10, size)\n","\n","    for __id,frame in enumerate(points):\n","        skel_image = gen_skeleton(frame, frame_h, frame_w,mapping_list=mapping_list)\n","        result.write(skel_image)\n","\n","    result.release()"],"metadata":{"id":"gv4MRtlZbLYt","executionInfo":{"status":"ok","timestamp":1682830037567,"user_tz":-330,"elapsed":10,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["joint_map = [(3,2),(2,20),(20,4),(4,5),(5,6),(6,7),(7,21),(7,22),(20,8),(8,9),(9,10),(10,11),(11,23),(11,24),\n","            (20,1),(1,0),(0,12),(12,13),(13,14),(14,15),(0,16),(16,17),(17,18),(18,19),(8,16),(4,12),(8,4),(16,12)]\n","\n","joint_map = val_ds.create_connection_map(joint_map)"],"metadata":{"id":"kpiMgL96h1ax","executionInfo":{"status":"ok","timestamp":1682830037567,"user_tz":-330,"elapsed":10,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","save_vids_dir = \"checking_vids/init\"\n","for adata in tqdm(train_dl):\n","  selected_ind = random.randint(0,adata[0].shape[0]-1)\n","  data = adata[0][selected_ind].numpy()\n","  file_id = adata[4][selected_ind].split(\".\")[0]\n","  target = id2clsname[int(adata[2][selected_ind])]\n","  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n","\n","  try:\n","    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\n","      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n","      gen_video(data, \n","                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\n","                vid_size[0], \n","                vid_size[1],\n","                is_3d=False,\n","                mapping_list=joint_map\n","                )\n","  except ValueError:\n","    continue\n","\"\"\""],"metadata":{"id":"OJzG7jyh7pU_","executionInfo":{"status":"ok","timestamp":1682830037567,"user_tz":-330,"elapsed":9,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"colab":{"base_uri":"https://localhost:8080/","height":107},"outputId":"d015e65d-ef18-4954-e228-ba89fe548325"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nsave_vids_dir = \"checking_vids/init\"\\nfor adata in tqdm(train_dl):\\n  selected_ind = random.randint(0,adata[0].shape[0]-1)\\n  data = adata[0][selected_ind].numpy()\\n  file_id = adata[4][selected_ind].split(\".\")[0]\\n  target = id2clsname[int(adata[2][selected_ind])]\\n  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\\n\\n  try:\\n    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\\n      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\\n      gen_video(data, \\n                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\\n                vid_size[0], \\n                vid_size[1],\\n                is_3d=False,\\n                mapping_list=joint_map\\n                )\\n  except ValueError:\\n    continue\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["class norm_data(nn.Module):\n","    def __init__(self, dim=3, joints=20):\n","        super(norm_data, self).__init__()\n","\n","        self.bn = nn.BatchNorm1d(dim*joints)\n","\n","    def forward(self, x):\n","        bs, c, num_joints, step = x.size()\n","        x = x.view(bs, -1, step)\n","        x = self.bn(x)\n","        x = x.view(bs, -1, num_joints, step).contiguous()\n","        return x\n","\n","class embed(nn.Module):\n","    def __init__(self, dim=3, joint=20, hidden_dim=128, norm=True, bias=False):\n","        super(embed, self).__init__()\n","\n","        if norm:\n","            self.cnn = nn.Sequential(\n","                norm_data(dim, joint),\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","        else:\n","            self.cnn = nn.Sequential(\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class cnn1x1(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 =3, bias = True):\n","        super(cnn1x1, self).__init__()\n","        self.cnn = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class local(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 = 3, bias = False):\n","        super(local, self).__init__()\n","        self.maxpool = nn.AdaptiveMaxPool2d((1, None))\n","        self.cnn1 = nn.Conv2d(dim1, dim1, kernel_size=(1, 3), padding=(0, 1), bias=bias)\n","        self.bn1 = nn.BatchNorm2d(dim1)\n","        self.relu = nn.ReLU()\n","        self.cnn2 = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","        self.bn2 = nn.BatchNorm2d(dim2)\n","        self.dropout = nn.Dropout2d(0.2)\n","\n","    def forward(self, x1):\n","        x1 = self.maxpool(x1)\n","        x = self.cnn1(x1)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.cnn2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x\n","\n","class gcn_spa(nn.Module):\n","    def __init__(self, in_feature, out_feature, bias = False):\n","        super(gcn_spa, self).__init__()\n","        self.bn = nn.BatchNorm2d(out_feature)\n","        self.relu = nn.ReLU()\n","        self.w = cnn1x1(in_feature, out_feature, bias=False)\n","        self.w1 = cnn1x1(in_feature, out_feature, bias=bias)\n","\n","\n","    def forward(self, x1, g):\n","        x = x1.permute(0, 3, 2, 1).contiguous()\n","        x = g.matmul(x)\n","        x = x.permute(0, 3, 2, 1).contiguous()\n","        x = self.w(x) + self.w1(x1)\n","        x = self.relu(self.bn(x))\n","        return x\n","\n","class compute_g_spa(nn.Module):\n","    def __init__(self, dim1 = 64 *3, dim2 = 64*3, bias = False):\n","        super(compute_g_spa, self).__init__()\n","        self.dim1 = dim1\n","        self.dim2 = dim2\n","        self.g1 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.g2 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x1):\n","\n","        g1 = self.g1(x1).permute(0, 3, 2, 1).contiguous()\n","        g2 = self.g2(x1).permute(0, 3, 1, 2).contiguous()\n","        g3 = g1.matmul(g2)\n","        g = self.softmax(g3)\n","        return g\n","    \n","\n","class SGN(nn.Module):\n","    def __init__(self, num_joint, seg, hidden_size=128, bs=32, is_3d=True, train=True, bias=True, device='cpu'):\n","        super(SGN, self).__init__()\n","\n","        self.dim1 = hidden_size\n","        self.dim_unit = hidden_size // 4 \n","        self.seg = seg\n","        self.num_joint = num_joint\n","        self.bs = bs\n","\n","        if is_3d:\n","          self.spatial_dim = 3\n","        else:\n","          self.spatial_dim = 2\n","\n","        if train:\n","            self.spa = self.one_hot(bs, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(bs, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","        else:\n","            self.spa = self.one_hot(32 * 5, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(32 * 5, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","\n","        self.tem_embed = embed(self.seg, joint=self.num_joint, hidden_dim=self.dim_unit*4, norm=False, bias=bias)\n","        self.spa_embed = embed(num_joint, joint=self.num_joint, hidden_dim=self.dim_unit, norm=False, bias=bias)\n","        self.joint_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.dif_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n","        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n","        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n","        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n","        \n","        self.embed_maxpool = nn.AdaptiveMaxPool2d([self.dim1, 2])\n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","\n","        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n","\n","\n","    def forward(self, input):\n","        \n","        # Dynamic Representation\n","        input = input.view((self.bs, self.seg, self.num_joint, self.spatial_dim))\n","        input = input.permute(0, 3, 2, 1).contiguous()\n","        dif = input[:, :, :, 1:] - input[:, :, :, 0:-1]\n","        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n","        # print(input.shape)\n","        pos = self.joint_embed(input)\n","        tem1 = self.tem_embed(self.tem)\n","        spa1 = self.spa_embed(self.spa)\n","        dif = self.dif_embed(dif)\n","        dy = pos + dif\n","        # Joint-level Module\n","        input= torch.cat([dy, spa1], 1)\n","        g = self.compute_g1(input)\n","        input = self.gcn1(input, g)\n","        input = self.gcn2(input, g)\n","        input = self.gcn3(input, g)\n","        # Frame-level Module\n","        input = input + tem1\n","        input = self.cnn(input)\n","        output_feat = torch.squeeze(input)\n","        output_feat = self.embed_maxpool(output_feat)\n","        output_feat = torch.flatten(output_feat, 1)\n","\n","        return output_feat\n","\n","    def one_hot(self, bs, spa, tem):\n","\n","        y = torch.arange(spa).unsqueeze(-1)\n","        y_onehot = torch.FloatTensor(spa, spa)\n","\n","        y_onehot.zero_()\n","        y_onehot.scatter_(1, y, 1)\n","\n","        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n","        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n","\n","        return y_onehot\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","class BiLSTMDecoder(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMDecoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","\n","        if bidirectional:\n","            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n","        else:\n","            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n","\n","        # define LSTM layer\n","        self.layers = []\n","        # add lstm\n","        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=True,\n","                            batch_first=bidirectional)\n","\n","                        \n","        # add linear layers \n","        if bidirectional:\n","            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n","        else:\n","            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n","\n","        for __id,layer_in in enumerate(self.linear_filters):\n","            if __id == len(linear_filters)-1:\n","                self.layers.append(nn.Linear(layer_in,self.input_size))\n","            else:\n","                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n","\n","        self.net = nn.Sequential(*self.layers)\n","\n","        \n","        \n","\n","    def forward(self,encoder_hidden):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        \n","        hidden_shape = encoder_hidden.shape\n","        encoder_hidden = self.input_linear(encoder_hidden)\n","        \n","        if self.bidirectional:\n","            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h1,h2,c1,c2 = torch.unbind(hidden,0)\n","            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n","            bs = h.size()[1]\n","        else:\n","            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h,c = torch.unbind(hidden,0)\n","            bs = h.size()[1]\n","        \n","        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n","        \n","        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n","        x = self.net(lstm_out)\n","        \n","        return x\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","        "],"metadata":{"id":"QNhRUvwyFUQb","executionInfo":{"status":"ok","timestamp":1682830037997,"user_tz":-330,"elapsed":438,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["encoder = SGN( \n","    num_joint=config[\"model\"][\"num_joint\"], \n","    seg=config[\"model\"][\"seq_len\"], \n","    hidden_size=config[\"model\"][\"encoder_hidden_size\"], \n","    bs=batch_size, \n","    is_3d=config[\"model\"][\"is_3d\"],\n","    device = device,\n","    train=True).to(device)\n","\n","classifier = SGNClassifier(\n","    num_classes=len(class_names),\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n",").to(device)\n","\n","decoder = BiLSTMDecoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    hidden_size=config[\"model\"][\"decoder_hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"]).to(device)\n","\n","bilstm_model = EncDecModel(\n","    encoder = encoder,\n","    decoder = decoder,\n","    classifier = classifier\n",").to(device)"],"metadata":{"id":"SxOl8YuW6f71","executionInfo":{"status":"ok","timestamp":1682830037998,"user_tz":-330,"elapsed":6,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["torchinfo.summary(bilstm_model, input_size=(batch_size, config[\"model\"][\"seq_len\"], config[\"model\"][\"input_size\"]), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBdLYSVvL0f3","executionInfo":{"status":"ok","timestamp":1682830047562,"user_tz":-330,"elapsed":9568,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"db44643f-065d-4aeb-cc54-9695a20443de"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  action_fn=lambda data: sys.getsizeof(data.storage()),\n","/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return super().__sizeof__() + self.nbytes()\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================================================================================================\n","Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n","==========================================================================================================================================================================\n","EncDecModel                                   [32, 60, 24]              [32, 60, 24]              --                        --                        --\n","├─SGN: 1-1                                    [32, 60, 24]              [32, 1024]                --                        --                        --\n","│    └─embed: 2-1                             [32, 2, 12, 60]           [32, 128, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-1                   [32, 2, 12, 60]           [32, 128, 12, 60]         8,560                     --                        196,118,016\n","│    └─embed: 2-2                             [32, 60, 12, 60]          [32, 512, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-2                   [32, 60, 12, 60]          [32, 512, 12, 60]         37,184                    --                        856,719,360\n","│    └─embed: 2-3                             [32, 12, 12, 60]          [32, 128, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-3                   [32, 12, 12, 60]          [32, 128, 12, 60]         9,152                     --                        210,862,080\n","│    └─embed: 2-4                             [32, 2, 12, 60]           [32, 128, 12, 60]         --                        --                        --\n","│    │    └─Sequential: 3-4                   [32, 2, 12, 60]           [32, 128, 12, 60]         8,560                     --                        196,118,016\n","│    └─compute_g_spa: 2-5                     [32, 256, 12, 60]         [32, 60, 12, 12]          --                        --                        --\n","│    │    └─cnn1x1: 3-5                       [32, 256, 12, 60]         [32, 512, 12, 60]         131,584                   --                        3,031,695,360\n","│    │    └─cnn1x1: 3-6                       [32, 256, 12, 60]         [32, 512, 12, 60]         131,584                   --                        3,031,695,360\n","│    │    └─Softmax: 3-7                      [32, 60, 12, 12]          [32, 60, 12, 12]          --                        --                        --\n","│    └─gcn_spa: 2-6                           [32, 256, 12, 60]         [32, 256, 12, 60]         --                        --                        --\n","│    │    └─cnn1x1: 3-8                       [32, 256, 12, 60]         [32, 256, 12, 60]         65,536                    --                        1,509,949,440\n","│    │    └─cnn1x1: 3-9                       [32, 256, 12, 60]         [32, 256, 12, 60]         65,792                    --                        1,515,847,680\n","│    │    └─BatchNorm2d: 3-10                 [32, 256, 12, 60]         [32, 256, 12, 60]         512                       --                        16,384\n","│    │    └─ReLU: 3-11                        [32, 256, 12, 60]         [32, 256, 12, 60]         --                        --                        --\n","│    └─gcn_spa: 2-7                           [32, 256, 12, 60]         [32, 512, 12, 60]         --                        --                        --\n","│    │    └─cnn1x1: 3-12                      [32, 256, 12, 60]         [32, 512, 12, 60]         131,072                   --                        3,019,898,880\n","│    │    └─cnn1x1: 3-13                      [32, 256, 12, 60]         [32, 512, 12, 60]         131,584                   --                        3,031,695,360\n","│    │    └─BatchNorm2d: 3-14                 [32, 512, 12, 60]         [32, 512, 12, 60]         1,024                     --                        32,768\n","│    │    └─ReLU: 3-15                        [32, 512, 12, 60]         [32, 512, 12, 60]         --                        --                        --\n","│    └─gcn_spa: 2-8                           [32, 512, 12, 60]         [32, 512, 12, 60]         --                        --                        --\n","│    │    └─cnn1x1: 3-16                      [32, 512, 12, 60]         [32, 512, 12, 60]         262,144                   --                        6,039,797,760\n","│    │    └─cnn1x1: 3-17                      [32, 512, 12, 60]         [32, 512, 12, 60]         262,656                   --                        6,051,594,240\n","│    │    └─BatchNorm2d: 3-18                 [32, 512, 12, 60]         [32, 512, 12, 60]         1,024                     --                        32,768\n","│    │    └─ReLU: 3-19                        [32, 512, 12, 60]         [32, 512, 12, 60]         --                        --                        --\n","│    └─local: 2-9                             [32, 512, 12, 60]         [32, 1024, 1, 60]         --                        --                        --\n","│    │    └─AdaptiveMaxPool2d: 3-20           [32, 512, 12, 60]         [32, 512, 1, 60]          --                        --                        --\n","│    │    └─Conv2d: 3-21                      [32, 512, 1, 60]          [32, 512, 1, 60]          786,944                   [1, 3]                    1,510,932,480\n","│    │    └─BatchNorm2d: 3-22                 [32, 512, 1, 60]          [32, 512, 1, 60]          1,024                     --                        32,768\n","│    │    └─ReLU: 3-23                        [32, 512, 1, 60]          [32, 512, 1, 60]          --                        --                        --\n","│    │    └─Dropout2d: 3-24                   [32, 512, 1, 60]          [32, 512, 1, 60]          --                        --                        --\n","│    │    └─Conv2d: 3-25                      [32, 512, 1, 60]          [32, 1024, 1, 60]         525,312                   [1, 1]                    1,008,599,040\n","│    │    └─BatchNorm2d: 3-26                 [32, 1024, 1, 60]         [32, 1024, 1, 60]         2,048                     --                        65,536\n","│    │    └─ReLU: 3-27                        [32, 1024, 1, 60]         [32, 1024, 1, 60]         --                        --                        --\n","│    └─AdaptiveMaxPool2d: 2-10                [32, 1024, 60]            [32, 512, 2]              --                        --                        --\n","├─SGNClassifier: 1-2                          [32, 1024]                [32, 82]                  --                        --                        --\n","│    └─Linear: 2-11                           [32, 1024]                [32, 82]                  84,050                    --                        2,689,600\n","├─BiLSTMDecoder: 1-3                          [32, 1024]                [32, 60, 24]              --                        --                        --\n","│    └─Linear: 2-12                           [32, 1024]                [32, 4096]                4,198,400                 --                        134,348,800\n","│    └─LSTM: 2-13                             [32, 60, 1024]            [32, 60, 2048]            16,793,600                --                        32,243,712,000\n","│    └─Sequential: 2-14                       [32, 60, 2048]            [32, 60, 24]              --                        --                        --\n","│    │    └─Linear: 3-28                      [32, 60, 2048]            [32, 60, 1024]            2,098,176                 --                        67,141,632\n","│    │    └─Linear: 3-29                      [32, 60, 1024]            [32, 60, 512]             524,800                   --                        16,793,600\n","│    │    └─Linear: 3-30                      [32, 60, 512]             [32, 60, 256]             131,328                   --                        4,202,496\n","│    │    └─Linear: 3-31                      [32, 60, 256]             [32, 60, 128]             32,896                    --                        1,052,672\n","│    │    └─Linear: 3-32                      [32, 60, 128]             [32, 60, 24]              3,096                     --                        99,072\n","==========================================================================================================================================================================\n","Total params: 26,429,642\n","Trainable params: 26,429,642\n","Non-trainable params: 0\n","Total mult-adds (G): 63.68\n","==========================================================================================================================================================================\n","Input size (MB): 0.18\n","Forward/backward pass size (MB): 1219.18\n","Params size (MB): 105.72\n","Estimated Total Size (MB): 1325.08\n","=========================================================================================================================================================================="]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["bilstm_model.to(device);"],"metadata":{"id":"RETBAwj46i3Z","executionInfo":{"status":"ok","timestamp":1682830047562,"user_tz":-330,"elapsed":22,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["label_map = [(k,v) for k,v in id2clsname.items()]\n","labelToId = {x[0]: i for i, x in enumerate(label_map)}"],"metadata":{"id":"8t79v6xM6wx7","executionInfo":{"status":"ok","timestamp":1682830047562,"user_tz":-330,"elapsed":20,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n","    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n","    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n","    loss =  recon_loss + tar_loss\n","\n","    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n","    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n","\n","    return loss, {\n","        \"reconstruction_loss\":recon_loss.item(),\n","        \"target_loss\":tar_loss.item()\n","    }\n","\n"],"metadata":{"id":"Mfa8NjY57En3","executionInfo":{"status":"ok","timestamp":1682830047563,"user_tz":-330,"elapsed":21,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=1e-3, weight_decay=0.01)\n","std_loss = {\n","    \"reconstruction_loss\" :nn.L1Loss(),\n","    \"target_loss\" :nn.CrossEntropyLoss()\n","}"],"metadata":{"id":"brTkTW9BEa3i","executionInfo":{"status":"ok","timestamp":1682830047563,"user_tz":-330,"elapsed":21,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=4)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"],"metadata":{"id":"RUigg8dNcWJO","executionInfo":{"status":"ok","timestamp":1682830047563,"user_tz":-330,"elapsed":20,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def eval_step(model, dataloader,loss_module, device, class_names,  print_report=False, show_plot=False):\n","    model = model.eval()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    per_batch = {'targets': [], 'predictions': [], 'metrics': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n","\n","    with torch.no_grad():\n","      with tqdm(dataloader, unit=\"batch\", desc=\"eval\") as tepoch:\n","        for input_sequence,target_sequence,target_action,target_vid_size,_ in tepoch:\n","\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = target_sequence.to(device)\n","            target_action = target_action.to(device)\n","\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(False):\n","            # with autocast():\n","                predicted_sequence,_,predicted_label  = model(input_sequence)\n","            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n","            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n","            loss =  (1-config['alpha_target'])*recon_loss + config['alpha_target']*tar_loss\n","            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n","            \n","            pred_action = torch.argmax(predicted_label,dim=1)\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(target_action)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += loss_detail[\"reconstruction_loss\"]\n","                metrics['classi. loss'] += loss_detail[\"target_loss\"]\n","\n","            per_batch['targets'].append(target_action.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n","    metrics_dict.update(metrics)\n","    return metrics_dict"],"metadata":{"id":"TmXFlJJdEN9N","executionInfo":{"status":"ok","timestamp":1682830047563,"user_tz":-330,"elapsed":20,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["model_params, model_config, config = get_config(\n","    f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_{model_ident}/40__epoch50_emb1024_xy.pt\",\n","    device\n","    )\n","bilstm_model.load_state_dict(model_params)"],"metadata":{"id":"gEfzDnIP81qB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682830201769,"user_tz":-330,"elapsed":2384,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"b464ef46-ba6b-4245-c3fd-4676951d3d99"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["save_model(\n","        bilstm_model.decoder, \n","        f\"temp_{model_ident}\", \n","        f\"40__{unique_iden}_decoder\",\n","         models_saves, \n","         config)"],"metadata":{"id":"DxYr7xY-gSUF","executionInfo":{"status":"ok","timestamp":1682830634862,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["best_model_wts = copy.deepcopy(bilstm_model.state_dict())\n","best_acc = 0.0\n","show_interval = 10\n","\n","eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=True, show_plot=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385,"referenced_widgets":["4921a522de6e4721a0153b27847e6993","4a149c1ea8de4943a9e061b346e073c5","5f8545f7120d47179a93395bb90b7d88","198e9d5a4b8041bbb7fa0281bf2e6d00","9a67b299d46d47a2803c388534068b2e","1a717ccfe35a4778b8f846607b3331fb","a118cf6456f74df09a6d934e3e3b3f7c","0557b08a73d54aeab518821f6d5028fb","67c661fb8a01428bb164961efba3e5e2","25b7e30fcf8542cfb883340e8049b7b6","2d29d3b598ba4de3a4b3bd1a7bce85e6"]},"id":"wyUEFUlp7HOt","outputId":"b1d1531a-6b53-4268-be92-3733bcd959cb","executionInfo":{"status":"error","timestamp":1682830258968,"user_tz":-330,"elapsed":53428,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["eval:   0%|          | 0/229 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4921a522de6e4721a0153b27847e6993"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-b22ef7f49330>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mshow_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0meval_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbilstm_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstd_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mprint_report\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_plot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-8435656d5128>\u001b[0m in \u001b[0;36meval_step\u001b[0;34m(model, dataloader, loss_module, device, class_names, print_report, show_plot)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# with autocast():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted_label\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0;31m# loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mrecon_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reconstruction_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c69ec412923d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0mclassifier_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mdecoder_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c69ec412923d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# Frame-level Module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtem1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c69ec412923d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, g)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-c69ec412923d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["decoder = bilstm_model.decoder\n","\n","decoder(torch.rand((batch_size,config[\"model\"][\"embedding_size\"])).to(device)).size()"],"metadata":{"id":"Ftmsh7z8SjcU","executionInfo":{"status":"aborted","timestamp":1682830049915,"user_tz":-330,"elapsed":12,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!rm -r checking_vids"],"metadata":{"id":"xlB4KvJmvjQP","executionInfo":{"status":"aborted","timestamp":1682830049915,"user_tz":-330,"elapsed":12,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_vids_dir = \"checking_vids/init\"\n","for adata in tqdm(val_dl):\n","  selected_ind = random.randint(0,adata[0].shape[0]-1)\n","  data = adata[0][selected_ind]\n","  file_id = adata[4][selected_ind].split(\".\")[0]\n","  target = id2clsname[int(adata[2][selected_ind])]\n","  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n","\n","  pred,_,pred_label = bilstm_model(data.repeat(batch_size,1,1).to(device))\n","  pred = pred[0].cpu().detach().numpy()\n","  pred_label = id2clsname[int(np.argmax(pred_label[0].cpu().detach().numpy()))]\n","\n","  try:\n","    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\n","      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n","      gen_video(data.numpy(), \n","                f\"{save_vids_dir}/{file_id}/true_cls_{target}.mp4\",\n","                vid_size[0], \n","                vid_size[1],\n","                is_3d=False,\n","                mapping_list=joint_map\n","                )\n","      \n","      gen_video(pred, \n","                f\"{save_vids_dir}/{file_id}/pred_cls_{pred_label}.mp4\",\n","                vid_size[0], \n","                vid_size[1],\n","                is_3d=False,\n","                mapping_list=joint_map\n","                )\n","  except ValueError:\n","    continue"],"metadata":{"id":"eFiA9eJRomYp","executionInfo":{"status":"aborted","timestamp":1682830049915,"user_tz":-330,"elapsed":12,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!zip -r checking_vids.zip checking_vids/"],"metadata":{"id":"xwRnz2rmqH7z","executionInfo":{"status":"aborted","timestamp":1682830049916,"user_tz":-330,"elapsed":13,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"bVArTOkcwQ5o","executionInfo":{"status":"aborted","timestamp":1682830049916,"user_tz":-330,"elapsed":13,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"execution_count":null,"outputs":[]}]}