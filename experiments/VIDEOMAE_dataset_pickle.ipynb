{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X1_71tmt-C7R","executionInfo":{"status":"ok","timestamp":1685075117145,"user_tz":-330,"elapsed":3880,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"7eb80edd-0e5b-454b-d09d-39b2975f36d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install --quiet decord pytorchvideo timm==0.4.12  transformers evaluate einops torchinfo\n","!git clone https://github.com/MCG-NJU/VideoMAE.git\n","!cp -r VideoMAE/* ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KyM7257q2th5","executionInfo":{"status":"ok","timestamp":1685075134404,"user_tz":-330,"elapsed":17263,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"b06a1112-0f5b-43bd-c90b-53d5be55f7ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'VideoMAE' already exists and is not an empty directory.\n"]}]},{"cell_type":"code","source":["from functools import partial\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n","from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n","from timm.models.registry import register_model\n","import torch.utils.checkpoint as checkpoint\n","import math\n","\n","def pretrain_trunc_normal_(tensor, mean=0., std=1.):\n","    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n","        **kwargs\n","    }\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","    \n","    def extra_repr(self) -> str:\n","        return 'p={}'.format(self.drop_prob)\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        # x = self.drop(x)\n","        # commit this for the orignal BERT implement \n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","                 attn_head_dim=None):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if init_values > 0:\n","            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        else:\n","            self.gamma_1, self.gamma_2 = None, None\n","\n","    def forward(self, x):\n","        if self.gamma_1 is None:\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        else:\n","            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        self.tubelet_size = int(tubelet_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim, \n","                            kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]), \n","                            stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n","\n","    def forward(self, x, **kwargs):\n","        B, C, T, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","    \n","# sin-cos position encoding\n","# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n","def get_sinusoid_encoding_table(n_position, d_hid): \n","    ''' Sinusoid position encoding table ''' \n","    # TODO: make it with torch instead of numpy \n","    def get_position_angle_vec(position): \n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n","\n","    return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0) \n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, \n","                 img_size=224, \n","                 patch_size=16, \n","                 in_chans=3, \n","                 num_classes=1000, \n","                 embed_dim=768, \n","                 depth=12,\n","                 num_heads=12, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 fc_drop_rate=0., \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False, \n","                 init_scale=0.,\n","                 all_frames=16,\n","                 tubelet_size=2,\n","                 use_checkpoint=False,\n","                 use_mean_pooling=True):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.tubelet_size = tubelet_size\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=all_frames, tubelet_size=self.tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings is on the way\n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n","        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n","        self.fc_dropout = nn.Dropout(p=fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        pretrain_trunc_normal_(self.head.weight, std=.02)\n","        self.apply(self._init_weights)\n","\n","        self.head.weight.data.mul_(init_scale)\n","        self.head.bias.data.mul_(init_scale)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            pretrain_trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        x = self.patch_embed(x)\n","        B, _, _ = x.size()\n","\n","        if self.pos_embed is not None:\n","            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        x = self.pos_drop(x)\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        x = self.norm(x)\n","        if self.fc_norm is not None:\n","            return self.fc_norm(x.mean(1))\n","        else:\n","            return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(self.fc_dropout(x))\n","        return x\n","\n","\n","@register_model\n","def vit_small_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_512(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_huge_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","class PretrainVisionTransformerEncoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n","                 use_learnable_pos_emb=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,tubelet_size=tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        # TODO: Add the cls token\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings \n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        _, _, T, _, _ = x.shape\n","        x = self.patch_embed(x)\n","        \n","        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n","\n","        B, _, C = x.shape\n","        x_vis = x.reshape(B, -1, C) # ~mask means visible\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x_vis = checkpoint.checkpoint(blk, x_vis)\n","        else:   \n","            for blk in self.blocks:\n","                x_vis = blk(x_vis)\n","\n","        x_vis = self.norm(x_vis)\n","        return x_vis\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","class PretrainVisionTransformerDecoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n","                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n","                 norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n","                 ):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        assert num_classes == 3 * tubelet_size * patch_size ** 2 \n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_size = patch_size\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x, return_token_num):\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        if return_token_num > 0:\n","            x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n","        else:\n","            x = self.head(self.norm(x))\n","\n","        return x\n","\n","class PretrainVisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self,\n","                 img_size=224, \n","                 patch_size=16, \n","                 encoder_in_chans=3, \n","                 encoder_num_classes=0, \n","                 encoder_embed_dim=768, \n","                 encoder_depth=12,\n","                 encoder_num_heads=12, \n","                 decoder_num_classes=1536, #  decoder_num_classes=768, \n","                 decoder_embed_dim=512, \n","                 decoder_depth=8,\n","                 decoder_num_heads=8, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False,\n","                 use_checkpoint=False,\n","                 tubelet_size=2,\n","                 num_classes=0, # avoid the error from create_fn in timm\n","                 in_chans=0, # avoid the error from create_fn in timm\n","                 ):\n","        super().__init__()\n","        self.encoder = PretrainVisionTransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=encoder_in_chans, \n","            num_classes=encoder_num_classes, \n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint,\n","            use_learnable_pos_emb=use_learnable_pos_emb)\n","\n","        self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=patch_size, \n","            num_patches=self.encoder.patch_embed.num_patches,\n","            num_classes=decoder_num_classes, \n","            embed_dim=decoder_embed_dim, \n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint)\n","\n","        self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n","\n","        trunc_normal_(self.mask_token, std=.02)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'mask_token'}\n","\n","    def forward(self, x):\n","        _, _, T, _, _ = x.shape\n","        x_vis = self.encoder(x) # [B, N_vis, C_e]\n","        x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","        B, N, C = x_vis.shape\n","        # we don't unshuffle the correct visible token order, \n","        # but shuffle the pos embedding accorddingly.\n","        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","        pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","        x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","        x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","\n","        return x\n","\n","@register_model\n","def pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=768, \n","        encoder_depth=12, \n","        encoder_num_heads=12,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536,\n","        decoder_embed_dim=384,\n","        decoder_num_heads=6,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n"," \n","@register_model\n","def pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1024, \n","        encoder_depth=24, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=512,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1280, \n","        encoder_depth=32, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=640,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model"],"metadata":{"id":"ILcFMOUYRn5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tMM4lf3h1hOL"},"outputs":[],"source":["import os\n","import numpy as np\n","from numpy.lib.function_base import disp\n","import torch\n","import decord\n","from PIL import Image\n","from torchvision import transforms\n","from random_erasing import RandomErasing\n","import warnings\n","from decord import VideoReader, cpu\n","from torch.utils.data import Dataset\n","import video_transforms as video_transforms \n","import volume_transforms as volume_transforms\n","from kinetics import *\n","\n","class VideoClsDataset(Dataset):\n","    \"\"\"Load your own video classification dataset.\"\"\"\n","\n","    def __init__(self, anno_path, data_path, mode='train', clip_len=8,\n","                 frame_sample_rate=2, crop_size=224, short_side_size=256,\n","                 new_height=256, new_width=340, keep_aspect_ratio=True,\n","                 num_segment=1, num_crop=1, test_num_segment=10, test_num_crop=3,args=None):\n","        self.anno_path = anno_path\n","        self.mode = mode\n","        self.clip_len = clip_len\n","        self.frame_sample_rate = frame_sample_rate\n","        self.crop_size = crop_size\n","        self.short_side_size = short_side_size\n","        self.new_height = new_height\n","        self.new_width = new_width\n","        self.keep_aspect_ratio = keep_aspect_ratio\n","        self.num_segment = num_segment\n","        self.test_num_segment = test_num_segment\n","        self.num_crop = num_crop\n","        self.test_num_crop = test_num_crop\n","        self.args = args\n","        self.aug = False\n","        self.rand_erase = False\n","        if self.mode in ['train']:\n","            self.aug = True\n","            if self.args.reprob > 0:\n","                self.rand_erase = True\n","        if VideoReader is None:\n","            raise ImportError(\"Unable to import `decord` which is required to read videos.\")\n","\n","        import pandas as pd\n","        cleaned = pd.read_csv(self.anno_path, header=None, delimiter='   ')\n","        self.dataset_samples = list(cleaned.values[:, 0])\n","        self.label_array = list(cleaned.values[:, 1])\n","\n","        if (mode == 'train'):\n","            pass\n","\n","        elif (mode == 'validation'):\n","            self.data_transform = video_transforms.Compose([\n","                video_transforms.Resize((self.short_side_size,self.short_side_size), interpolation='bilinear'),\n","                video_transforms.CenterCrop(size=(self.crop_size, self.crop_size)),\n","                volume_transforms.ClipToTensor(),\n","                video_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                           std=[0.229, 0.224, 0.225])\n","            ])\n","        elif mode == 'test':\n","            self.data_resize = video_transforms.Compose([\n","                video_transforms.Resize(size=(self.short_side_size,self.short_side_size), interpolation='bilinear')\n","            ])\n","            self.data_transform = video_transforms.Compose([\n","                volume_transforms.ClipToTensor(),\n","                video_transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                           std=[0.229, 0.224, 0.225])\n","            ])\n","            self.test_seg = []\n","            self.test_dataset = []\n","            self.test_label_array = []\n","            for ck in range(self.test_num_segment):\n","                for cp in range(self.test_num_crop):\n","                    for idx in range(len(self.label_array)):\n","                        sample_label = self.label_array[idx]\n","                        self.test_label_array.append(sample_label)\n","                        self.test_dataset.append(self.dataset_samples[idx])\n","                        self.test_seg.append((ck, cp))\n","\n","    def __getitem__(self, index):\n","        if self.mode == 'train':\n","            args = self.args \n","            scale_t = 1\n","\n","            sample = self.dataset_samples[index]\n","            buffer = self.loadvideo_decord(sample, sample_rate_scale=scale_t) # T H W C\n","            if len(buffer) == 0:\n","                while len(buffer) == 0:\n","                    warnings.warn(\"video {} not correctly loaded during training\".format(sample))\n","                    index = np.random.randint(self.__len__())\n","                    sample = self.dataset_samples[index]\n","                    buffer = self.loadvideo_decord(sample, sample_rate_scale=scale_t)\n","\n","            if args.num_sample > 1:\n","                frame_list = []\n","                label_list = []\n","                index_list = []\n","                for _ in range(args.num_sample):\n","                    new_frames = self._aug_frame(buffer, args)\n","                    label = self.label_array[index]\n","                    frame_list.append(new_frames)\n","                    label_list.append(label)\n","                    index_list.append(index)\n","                return frame_list, label_list, index_list, {}\n","            else:\n","                buffer = self._aug_frame(buffer, args)\n","            \n","            return buffer, self.label_array[index], index, {}\n","\n","        elif self.mode == 'validation':\n","            sample = self.dataset_samples[index]\n","            buffer = self.loadvideo_decord(sample)\n","            if len(buffer) == 0:\n","                while len(buffer) == 0:\n","                    warnings.warn(\"video {} not correctly loaded during validation\".format(sample))\n","                    index = np.random.randint(self.__len__())\n","                    sample = self.dataset_samples[index]\n","                    buffer = self.loadvideo_decord(sample)\n","            buffer = self.data_transform(buffer)\n","            return buffer, self.label_array[index], sample.split(\"/\")[-1].split(\".\")[0]\n","\n","        elif self.mode == 'test':\n","            sample = self.test_dataset[index]\n","            chunk_nb, split_nb = self.test_seg[index]\n","            buffer = self.loadvideo_decord(sample)\n","\n","            while len(buffer) == 0:\n","                warnings.warn(\"video {}, temporal {}, spatial {} not found during testing\".format(\\\n","                    str(self.test_dataset[index]), chunk_nb, split_nb))\n","                index = np.random.randint(self.__len__())\n","                sample = self.test_dataset[index]\n","                chunk_nb, split_nb = self.test_seg[index]\n","                buffer = self.loadvideo_decord(sample)\n","\n","            buffer = self.data_resize(buffer)\n","            if isinstance(buffer, list):\n","                buffer = np.stack(buffer, 0)\n","                \n","            if self.test_num_segment>1:\n","                temporal_step = max(1.0 * (buffer.shape[0] - self.clip_len) \\\n","                                    / (self.test_num_segment - 1), 0)\n","                temporal_start = int(chunk_nb * temporal_step)\n","                buffer = buffer[temporal_start:temporal_start + self.clip_len, \\\n","                        :, :, :]\n","            else:\n","                temporal_step = buffer.shape[0]//self.clip_len\n","                buffer = buffer[::temporal_step, \\\n","                        :, :, :]\n","\n","            buffer = self.data_transform(buffer)\n","            return buffer, self.test_label_array[index], sample.split(\"/\")[-1].split(\".\")[0], \\\n","                   chunk_nb, split_nb\n","        else:\n","            raise NameError('mode {} unkown'.format(self.mode))\n","\n","    def _aug_frame(\n","        self,\n","        buffer,\n","        args,\n","    ):\n","\n","        aug_transform = video_transforms.create_random_augment(\n","            input_size=(self.crop_size, self.crop_size),\n","            auto_augment=args.aa,\n","            interpolation=args.train_interpolation,\n","        )\n","\n","        buffer = [\n","            transforms.ToPILImage()(frame) for frame in buffer\n","        ]\n","\n","        buffer = aug_transform(buffer)\n","\n","        buffer = [transforms.ToTensor()(img) for img in buffer]\n","        buffer = torch.stack(buffer) # T C H W\n","        buffer = buffer.permute(0, 2, 3, 1) # T H W C \n","        \n","        # T H W C \n","        buffer = tensor_normalize(\n","            buffer, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n","        )\n","        # T H W C -> C T H W.\n","        buffer = buffer.permute(3, 0, 1, 2)\n","        # Perform data augmentation.\n","        scl, asp = (\n","            [0.08, 1.0],\n","            [0.75, 1.3333],\n","        )\n","\n","        buffer = spatial_sampling(\n","            buffer,\n","            spatial_idx=-1,\n","            min_scale=256,\n","            max_scale=320,\n","            crop_size=self.crop_size,\n","            random_horizontal_flip=False if args.data_set == 'SSV2' else True ,\n","            inverse_uniform_sampling=False,\n","            aspect_ratio=asp,\n","            scale=scl,\n","            motion_shift=False\n","        )\n","\n","        if self.rand_erase:\n","            erase_transform = RandomErasing(\n","                args.reprob,\n","                mode=args.remode,\n","                max_count=args.recount,\n","                num_splits=args.recount,\n","                device=\"cpu\",\n","            )\n","            buffer = buffer.permute(1, 0, 2, 3)\n","            buffer = erase_transform(buffer)\n","            buffer = buffer.permute(1, 0, 2, 3)\n","\n","        return buffer\n","\n","\n","    def loadvideo_decord(self, sample, sample_rate_scale=1):\n","        \"\"\"Load video content using Decord\"\"\"\n","        fname = sample\n","\n","        if not (os.path.exists(fname)):\n","            return []\n","\n","        # avoid hanging issue\n","        if os.path.getsize(fname) < 1 * 1024:\n","            print('SKIP: ', fname, \" - \", os.path.getsize(fname))\n","            return []\n","        try:\n","            if self.keep_aspect_ratio:\n","                vr = VideoReader(fname, num_threads=1, ctx=cpu(0))\n","            else:\n","                vr = VideoReader(fname, width=self.new_width, height=self.new_height,\n","                                 num_threads=1, ctx=cpu(0))\n","        except:\n","            print(\"video cannot be loaded by decord: \", fname)\n","            return []\n","\n","        if self.mode == 'test':\n","            all_index = [x for x in range(0, len(vr), self.frame_sample_rate)]\n","            while len(all_index) < self.clip_len:\n","                all_index.append(all_index[-1])\n","            vr.seek(0)\n","            buffer = vr.get_batch(all_index).asnumpy()\n","            return buffer\n","\n","        # handle temporal segments\n","        converted_len = int(self.clip_len * self.frame_sample_rate)\n","        seg_len = len(vr) // self.num_segment\n","\n","        all_index = []\n","        for i in range(self.num_segment):\n","            if seg_len <= converted_len:\n","                index = np.linspace(0, seg_len, num=seg_len // self.frame_sample_rate)\n","                index = np.concatenate((index, np.ones(self.clip_len - seg_len // self.frame_sample_rate) * seg_len))\n","                index = np.clip(index, 0, seg_len - 1).astype(np.int64)\n","            else:\n","                end_idx = np.random.randint(converted_len, seg_len)\n","                str_idx = end_idx - converted_len\n","                index = np.linspace(str_idx, end_idx, num=self.clip_len)\n","                index = np.clip(index, str_idx, end_idx - 1).astype(np.int64)\n","            index = index + i*seg_len\n","            all_index.extend(list(index))\n","\n","        all_index = all_index[::int(sample_rate_scale)]\n","        vr.seek(0)\n","        buffer = vr.get_batch(all_index).asnumpy()\n","        return buffer\n","\n","    def __len__(self):\n","        if self.mode != 'test':\n","            return len(self.dataset_samples)\n","        else:\n","            return len(self.test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bvd3YeVT1hOT"},"outputs":[],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import itertools\n","import pandas as pd\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hfSfQFsR1hOT"},"outputs":[],"source":["model_ident = \"MHEALTHvids_small_ae_classifier_1024_emb1d\"\n","unique_iden = \"epoch20_emb400\"\n","dataset_ident = \"HMDB51\"\n","\n","main_dir = \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE\"\n","data_dir = os.path.join(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2/Videos\")\n","\n","epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n","models_saves = os.path.join(main_dir,\"model_saves\")\n","embeddings_save = os.path.join(main_dir,\"embedding_save\")\n","prototypes_save = os.path.join(main_dir,\"prototypes\")\n","test_vids = os.path.join(main_dir,\"test_vids\")\n","setting_fol = os.path.join(main_dir,\"settings\")\n","os.makedirs(f\"{setting_fol}/{model_ident}\",exist_ok=True)\n","train_ratio = 1\n","val_ratio = 0.0\n","batch_size = 8\n","\n","\n","\n","train_file_list = {k:random.sample(os.listdir(os.path.join(data_dir,k)),int(train_ratio*len(os.listdir(os.path.join(data_dir,k))))) for k in os.listdir(data_dir)}\n","val_file_list = {k:[x for x in os.listdir(os.path.join(data_dir,k)) if x not in train_file_list[k]] for k in os.listdir(data_dir)}\n","\n","train_items = []\n","val_items = []\n","for k in train_file_list.keys():\n","  train_items.extend(itertools.product([k],train_file_list[k]))\n","  \n","for k in val_file_list.keys():\n","  val_items.extend(itertools.product([k],val_file_list[k]))\n","\n","val_items = [f\"{data_dir}/{item[0]}/{item[1]}   {item[0]}\" for item in val_items]\n","train_items = [f\"{data_dir}/{item[0]}/{item[1]}   {item[0]}\" for item in train_items]\n","\n","random.shuffle(val_items)\n","random.shuffle(train_items)\n","\n","with open(f\"{setting_fol}/{model_ident}/{dataset_ident}_train_setings.txt\",\"w\") as f0:\n","  f0.write(\"\\n\".join(train_items))\n","\n","with open(f\"{setting_fol}/{model_ident}/{dataset_ident}_val_setings.txt\",\"w\") as f0:\n","  f0.write(\"\\n\".join(val_items))\n","\n","os.makedirs(epoch_vids,exist_ok=True)\n","os.makedirs(models_saves,exist_ok=True)\n","os.makedirs(embeddings_save,exist_ok=True)\n","\n","class_names = os.listdir(data_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nl4h2QVE1hOU","executionInfo":{"status":"ok","timestamp":1685075148983,"user_tz":-330,"elapsed":14,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"e579f7ff-0cfc-4bf8-f1db-8b20a426ae43"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(177, 0)"]},"metadata":{},"execution_count":7}],"source":["len(train_items),len(val_items)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z7m9TyH11hOV"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id\n","\n","id2clsname, clsname2id = classname_id(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":146},"id":"LgVLJHNL1hOV","executionInfo":{"status":"ok","timestamp":1685075148985,"user_tz":-330,"elapsed":13,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"d5f0ab18-ecdd-4b8b-9b63-1d0980822a5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-c553c596fc1a>:47: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n","  cleaned = pd.read_csv(self.anno_path, header=None, delimiter='   ')\n"]},{"output_type":"execute_result","data":{"text/plain":["'val_dataset = VideoClsDataset(\\n            anno_path=f\"{setting_fol}/{model_ident}/{dataset_ident}_val_setings.txt\",\\n            data_path=\\'/\\',\\n            mode=\"test\",\\n            clip_len=16,\\n            frame_sample_rate=10,\\n            num_segment=1,\\n            test_num_segment=1,\\n            test_num_crop=1,\\n            num_crop=1,\\n            keep_aspect_ratio=True,\\n            crop_size=224,\\n            short_side_size=224,\\n            new_height=256,\\n            new_width=320)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}],"source":["test_mode = True\n","mean=[0.485, 0.456, 0.406],\n","std=[0.229, 0.224, 0.225]\n","\n","\n","\n","\n","train_dataset = VideoClsDataset(\n","            anno_path=f\"{setting_fol}/{model_ident}/{dataset_ident}_train_setings.txt\",\n","            data_path='/',\n","            mode=\"test\",\n","            clip_len=16,\n","            frame_sample_rate=10,\n","            num_segment=1,\n","            test_num_segment=1,\n","            test_num_crop=1,\n","            num_crop=1,\n","            keep_aspect_ratio=True,\n","            crop_size=224,\n","            short_side_size=224,\n","            new_height=256,\n","            new_width=320)\n","\n","train_dl = DataLoader(train_dataset,batch_size=batch_size)\n","\n","\"\"\"val_dataset = VideoClsDataset(\n","            anno_path=f\"{setting_fol}/{model_ident}/{dataset_ident}_val_setings.txt\",\n","            data_path='/',\n","            mode=\"test\",\n","            clip_len=16,\n","            frame_sample_rate=10,\n","            num_segment=1,\n","            test_num_segment=1,\n","            test_num_crop=1,\n","            num_crop=1,\n","            keep_aspect_ratio=True,\n","            crop_size=224,\n","            short_side_size=224,\n","            new_height=256,\n","            new_width=320)\"\"\""]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","ae = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        decoder_depth=4,\n","        drop_path_rate=0.0,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        use_checkpoint=False\n","        )\n","ae.default_cfg = _cfg()"],"metadata":{"id":"u4LLkbtBRbS6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ae_params = torch.load(\n","    \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/pretrained_weights/ae_small_pretrained_kinetic400_1600e_.pth\",\n","    map_location=device\n","    )\n","\n","ae.load_state_dict(ae_params[\"model\"]);\n","ae.to(device);\n","\n","def get_embedding(model,input):\n","    return model(input)"],"metadata":{"id":"gNEYh7rZRdI6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4M3cuYQ-1hOW"},"outputs":[],"source":["def save_data_numpy(save_path,i):\n","    video, label, id, chunk_nb, split_nb = train_dataset[i]\n","    \n","    np.savez_compressed(f\"{save_path}/{label}_{id}_{chunk_nb}_{split_nb}.npz\",\n","          video = video,\n","          label = label,\n","          id = id,\n","          chunk_nb = chunk_nb,\n","          split_nb = split_nb)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUkDGuEh1hOW","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["fd53fa8b9b0d4f1ea54eb0c6f54d4a29","95bde074e5b54d22a710651df8437603","2e13b723706e4159bc3a86ed1df6d41b","ed5c362fcce141eeb8c97c9fed9c2e66","f437048489dc4d389b90b0bc40fd61f4","4d4c65ff08b44097ab88448c551d6b58","a9536b479ada416e811f48625faf8021","35c8f679401a4cb49ccc239d103b9ea0","78bf97910c82402a8dc7589aaca6f891","95992d11372e48a5a957298fb0e3ed20","c23bab24c132412b8d36942bec5412c2"]},"outputId":"7c008645-8fba-4fd3-9280-4d326f7f7ee5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/23 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd53fa8b9b0d4f1ea54eb0c6f54d4a29"}},"metadata":{}}],"source":["from tqdm.autonotebook import tqdm\n","\n","os.makedirs(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/preprocessing/PAMPA2_emb/train\",exist_ok=True)\n","\n","for batch in tqdm(train_dl):\n","    video, label, id, chunk_nb, split_nb = batch\n","    \n","    embedding = get_embedding(ae.encoder,video.to(device))\n","    \n","    for embedding0,video0, label0, id0, chunk_nb0, split_nb0 in zip(embedding.detach().cpu().numpy(),\n","                                                                    video.detach().cpu().numpy(), \n","                                                                    label, \n","                                                                    id, \n","                                                                    chunk_nb, \n","                                                                    split_nb):\n","        np.savez_compressed(f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/preprocessing/PAMPA2_emb/train/{label0}_{id0}_{chunk_nb0}_{split_nb0}.npz\",\n","            video = video0,\n","            embedding = embedding0,\n","            label = label0,\n","            id = id0,\n","            chunk_nb = chunk_nb0,\n","            split_nb = split_nb0)"]},{"cell_type":"code","source":["from tqdm.autonotebook import tqdm\n","\n","os.makedirs(\"D:/FYP/VideoMAE/preprocessed/HMDB51_1seg_embed/val\",exist_ok=True)\n","\n","for batch in tqdm(val_dl):\n","    video, label, id, chunk_nb, split_nb = batch\n","    \n","    embedding = get_embedding(ae.encoder,video.to(device))\n","    \n","    for embedding0,video0, label0, id0, chunk_nb0, split_nb0 in zip(embedding.detach().cpu().numpy(),\n","                                                                    video.detach().cpu().numpy(), \n","                                                                    label, \n","                                                                    id, \n","                                                                    chunk_nb, \n","                                                                    split_nb):\n","        np.savez_compressed(f\"D:/FYP/VideoMAE/preprocessed/HMDB51_1seg_embed/val/{label0}_{id0}_{chunk_nb0}_{split_nb0}.npz\",\n","            video = video0,\n","            embedding = embedding0,\n","            label = label0,\n","            id = id0,\n","            chunk_nb = chunk_nb0,\n","            split_nb = split_nb0)"],"metadata":{"id":"hxGCD-87XKMT","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["52e2e4cd70da4bf69330aaee00273083","9b3c75199e2049f59269d85122d08468","33639eb1d8e14ead8ab54382da4ca394","41989dcca5ca4a1785539578f9d6b635","846b4d182f8f43ebb7664e11cfdb9862","24740e58fac34bcea9d41b04f0e38b76","4bd9315861934f69a2b7e1e4127903d2","879bc69e75f34dc0a8c0216a59ff554d","c3d54760d192456f92c8be9f6b7727df","011fb99504324bd4a2828a71bd4f6cce","0629a46632754f28be6dda3198eaec2d"]},"executionInfo":{"status":"ok","timestamp":1684755246981,"user_tz":-330,"elapsed":408561,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"d1b8632d-26d9-468c-ae8a-640c15abc7b3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Processed Files::   0%|          | 0/700 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52e2e4cd70da4bf69330aaee00273083"}},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"g10Y6RH71cTN"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"fyp","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"52e2e4cd70da4bf69330aaee00273083":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b3c75199e2049f59269d85122d08468","IPY_MODEL_33639eb1d8e14ead8ab54382da4ca394","IPY_MODEL_41989dcca5ca4a1785539578f9d6b635"],"layout":"IPY_MODEL_846b4d182f8f43ebb7664e11cfdb9862"}},"9b3c75199e2049f59269d85122d08468":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_24740e58fac34bcea9d41b04f0e38b76","placeholder":"","style":"IPY_MODEL_4bd9315861934f69a2b7e1e4127903d2","value":"Processed Files:: 100%"}},"33639eb1d8e14ead8ab54382da4ca394":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_879bc69e75f34dc0a8c0216a59ff554d","max":700,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3d54760d192456f92c8be9f6b7727df","value":700}},"41989dcca5ca4a1785539578f9d6b635":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_011fb99504324bd4a2828a71bd4f6cce","placeholder":"","style":"IPY_MODEL_0629a46632754f28be6dda3198eaec2d","value":" 700/700 [06:48&lt;00:00,  2.20it/s]"}},"846b4d182f8f43ebb7664e11cfdb9862":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24740e58fac34bcea9d41b04f0e38b76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bd9315861934f69a2b7e1e4127903d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"879bc69e75f34dc0a8c0216a59ff554d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3d54760d192456f92c8be9f6b7727df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"011fb99504324bd4a2828a71bd4f6cce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0629a46632754f28be6dda3198eaec2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd53fa8b9b0d4f1ea54eb0c6f54d4a29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95bde074e5b54d22a710651df8437603","IPY_MODEL_2e13b723706e4159bc3a86ed1df6d41b","IPY_MODEL_ed5c362fcce141eeb8c97c9fed9c2e66"],"layout":"IPY_MODEL_f437048489dc4d389b90b0bc40fd61f4"}},"95bde074e5b54d22a710651df8437603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d4c65ff08b44097ab88448c551d6b58","placeholder":"","style":"IPY_MODEL_a9536b479ada416e811f48625faf8021","value":"  0%"}},"2e13b723706e4159bc3a86ed1df6d41b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_35c8f679401a4cb49ccc239d103b9ea0","max":23,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78bf97910c82402a8dc7589aaca6f891","value":0}},"ed5c362fcce141eeb8c97c9fed9c2e66":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95992d11372e48a5a957298fb0e3ed20","placeholder":"","style":"IPY_MODEL_c23bab24c132412b8d36942bec5412c2","value":" 0/23 [00:00&lt;?, ?it/s]"}},"f437048489dc4d389b90b0bc40fd61f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d4c65ff08b44097ab88448c551d6b58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9536b479ada416e811f48625faf8021":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35c8f679401a4cb49ccc239d103b9ea0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78bf97910c82402a8dc7589aaca6f891":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95992d11372e48a5a957298fb0e3ed20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23bab24c132412b8d36942bec5412c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}