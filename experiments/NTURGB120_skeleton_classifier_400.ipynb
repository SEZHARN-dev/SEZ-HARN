{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"elapsed":33852,"status":"error","timestamp":1684243529391,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"EnmESXTBv5qX"},"outputs":[{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-3-d5df0069828e\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 103\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         )\n\u001b[0;32m--\u003e 279\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed: timeout during initial read of root folder; for more info: https://research.google.com/colaboratory/faq.html#drive-timeout"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1684243486632,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"io2Hh_RUtoPq"},"outputs":[],"source":["! pip install neptune\n","! git clone https://github.com/nipdep/HAR-ZSL-XAI.git --branch AE --single-branch\n","! mv /content/HAR-ZSL-XAI/AETraining/dataset /content/"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"aborted","timestamp":1684243486632,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"BkIfhXqxWhOw"},"outputs":[],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import neptune\n","\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","\n","\"\"\"\n","Collection of functions which enable the evaluation of a classifier's performance,\n","by showing confusion matrix, accuracy, recall, precision etc.\n","\"\"\"\n","\n","import numpy as np\n","import sys\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","\n","def save_history(history, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","\n","    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n","        json.dump(history, f0)\n","\n","def get_config(file_loc):\n","    file = torch.load(file_loc)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","    \n","def save_model(model, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","    torch.save({\n","        \"n_epochs\": config[\"n_epochs\"],\n","        \"model_state_dict\": model.state_dict(),\n","        \"model_config\": config[\"model\"],\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}.pt\")\n","\n","def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n","    \"\"\"Plot confusion matrix in a separate window\"\"\"\n","    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    if label_strings:\n","        tick_marks = np.arange(len(label_strings))\n","        plt.xticks(tick_marks, label_strings, rotation=90)\n","        plt.yticks(tick_marks, label_strings)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n","    \"\"\"\n","    Returns a string of a report for given metric arrays (array length equals the number of classes).\n","    Called internally by `analyze_classification`.\n","        digits: number of digits after . for displaying results\n","        number_of_thieves: number of biggest thieves to report\n","        maxcharlength: max. number of characters to use when displaying thief names\n","    \"\"\"\n","\n","    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n","    sorted_class_indices = np.argsort(relative_freq)[\n","                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n","\n","    last_line_heading = 'avg / total'\n","\n","    width = max(len(cn) for cn in existing_class_names)\n","    width = max(width, len(last_line_heading), digits)\n","\n","    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n","    fmt = '%% %ds' % width  # first column: class name\n","    fmt += '  '\n","    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n","    fmt += '|\\t % 5s'\n","    fmt += '\\n'\n","\n","    headers = [\"\"] + headers\n","    report = fmt % tuple(headers)\n","    report += '\\n'\n","\n","    for i in sorted_class_indices:\n","        values = [existing_class_names[i]]\n","        for v in (precision[i], recall[i], f1[i],\n","                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n","            values += [\"{0:0.{1}f}\".format(v, digits)]\n","        values += [\"{}\".format(support[i])]\n","        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n","                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n","        thieves = thieves[thieves != i]  # exclude self at this point\n","        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n","        thieves_names = [\n","            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n","            in thieves]  # a little inefficient but inconsequential\n","        string_about_stealing = \"\"\n","        for j in range(len(thieves)):\n","            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n","        values += [string_about_stealing]\n","\n","        report += fmt % tuple(values)\n","\n","    report += '\\n' + 100 * '-' + '\\n'\n","\n","    # compute averages/sums\n","    values = [last_line_heading]\n","    for v in (np.average(precision, weights=relative_freq),\n","                np.average(recall, weights=relative_freq),\n","                np.average(f1, weights=relative_freq)):\n","        values += [\"{0:0.{1}f}\".format(v, digits)]\n","    values += ['{0}'.format(np.sum(relative_freq))]\n","    values += ['{0}'.format(np.sum(support))]\n","    values += ['']\n","\n","    # make last (\"Total\") line for report\n","    report += fmt % tuple(values)\n","\n","    return report\n","\n","\n","def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n","    \"\"\"\n","    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n","    Input:\n","        y_pred: 1D array of predicted labels (class indices)\n","        y_true: 1D array of true labels (class indices)\n","        class_names: 1D array or list of class names in the order of class indices.\n","            Could also be integers [0, 1, ..., num_classes-1].\n","        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n","    \"\"\"\n","\n","    # Trim class_names to include only classes existing in y_pred OR y_true\n","    in_pred_labels = set(list(y_pred))\n","    in_true_labels = set(list(y_true))\n","    # print(\"predicted labels \u003e \", in_pred_labels, \"in_true_labels \u003e \", in_true_labels)\n","\n","    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n","    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n","    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n","    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n","\n","    # Confusion matrix\n","    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n","\n","    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n","    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n","\n","    if show_plot:\n","        plt.figure()\n","        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n","                                title='Confusion matrix normalized by row')\n","        plt.show(block=False)\n","\n","    # Analyze results\n","    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n","    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n","\n","    # returns metrics for each class, in the same order as existing_class_names\n","    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n","    # Print report\n","    if print_report:\n","        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n","\n","    # Calculate average precision and recall\n","    # prec_avg, rec_avg = get_avg_prec_recall(ConfMatrix, existing_class_names, excluded_classes)\n","    # if excluded_classes:\n","    #     print(\n","    #         \"\\nAverage PRECISION: {:.2f}\\n(using class frequencies as weights, excluding classes with no predictions and predictions in '{}')\".format(\n","    #             prec_avg, ', '.join(excluded_classes)))\n","    #     print(\n","    #         \"\\nAverage RECALL (= ACCURACY): {:.2f}\\n(using class frequencies as weights, excluding classes in '{}')\".format(\n","    #             rec_avg, ', '.join(excluded_classes)))\n","\n","    # Make a histogram with the distribution of classes with respect to precision and recall\n","    # prec_rec_histogram(precision, recall)\n","\n","    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1684243486633,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"p0XIAvcQX3l0"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1684243486633,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"y7gAeWUDX7X6"},"outputs":[],"source":["model_ident = \"NTURGB120_skeleton_classifier_400\"\n","unique_iden = \"epoch50_emb400_xy\"\n","\n","main_dir = \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE\"\n","data_dir = os.path.join(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/skel\")\n","remove_files = [\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/NTU_RGBD120_samples_with_missing_skeletons.txt\",\n","                \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/NTU_RGBD_samples_with_missing_skeletons.txt\"]\n","\n","epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n","models_saves = os.path.join(main_dir,\"model_saves\")\n","embeddings_save = os.path.join(main_dir,\"embedding_save\")\n","prototypes_save = os.path.join(main_dir,\"prototypes\")\n","test_vids = os.path.join(main_dir,\"test_vids\")\n","train_ratio = 0.90\n","val_ratio = 0.1\n","batch_size = 128\n","\n","os.makedirs(epoch_vids,exist_ok=True)\n","os.makedirs(models_saves,exist_ok=True)\n","os.makedirs(embeddings_save,exist_ok=True)\n","\n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/nturgb120_label_map.json\",\"r\") as f0:\n","    full_id2cls = json.load(f0)\n","    \n","with open(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/NTURGB120/sel_cls_list - Single_person.txt\",\"r\") as f0:\n","    class_names = [full_id2cls[x] for x in f0.read().split(\" \")]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrsB7xhhaKF5"},"outputs":[],"source":["config = {\n","    \"n_epochs\":50,\n","    \"model_name\":\"BidirectionalLSTM\",\n","    \"model\":{\n","        \"seq_len\":50,\n","        \"input_size\":12*2,\n","        \"hidden_size\":1024,\n","        \"linear_filters\":[128,256,512,1024],\n","        \"embedding_size\":400,\n","        \"num_classes\":len(class_names),\n","        \"num_layers\":1,\n","        \"bidirectional\":True,\n","        \"batch_size\":batch_size,\n","        \"dev\":device\n","    },\n","    'alpha_recon': 0.95,\n","    'alpha_target': 0.05,\n","}\n","\n","id2clsname, clsname2id = classname_id(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"25RyOW-8gA46"},"outputs":[],"source":["info_dict = {\"file_names\":[]}\n","\n","for k in class_names:\n","  info_dict[\"file_names\"] += list(itertools.product([clsname2id[k]],os.listdir(os.path.join(data_files,k))))\n","\n","info_pd = pd.DataFrame(data=info_dict[\"file_names\"],columns=[\"target\",\"file_name\"])\n","train_df, val_df = train_test_split(info_pd,stratify=info_pd[\"target\"],train_size=train_ratio)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Res0UAsOjjki"},"outputs":[],"source":["def select_frames(sequence,sel_len_to_sel=50):\n","  if sequence.shape[0]\u003csel_len_to_sel:\n","    times = sel_len_to_sel//sequence.shape[0] + 1\n","\n","    sequence = sequence.repeat(times,1,1)\n","\n","\n","  sel_index = sorted(random.sample(range(sequence.shape[0]),sel_len_to_sel))\n","  return sequence[sel_index,...]\n","\n","def load_file_to_memory(df,data_dict,data_dir,id2class,idx):\n","  data_dict[idx] = np.load(\n","              os.path.join(\n","                  data_dir,\n","                  id2class[df.iloc[idx,0]],\n","                  df.iloc[idx,1]\n","                  )\n","              )\n","  \n","  return os.path.join(\n","                  data_dir,\n","                  id2class[df.iloc[idx,0]],\n","                  df.iloc[idx,1]\n","                  )\n","\n","class SkeletonDataset(Dataset):\n","    def __init__(self,\n","                 data_dir, \n","                 df,\n","                 id2class,\n","                 transform=None,\n","                 seq_len = 50,\n","                 target_transform=None,\n","                 active_locations=[11, 12, 13, 14, 15, 16, 23, 24, 25, 26, 27, 28],\n","                 file_name=False, \n","                 is_2d=False):\n","        self.data_dir = data_dir\n","        self.df = df\n","        self.transform = transform\n","        self.id2class = id2class\n","        self.target_transform = target_transform\n","        self.active_locations = active_locations\n","        self.file_name = file_name\n","        self.is_2d = is_2d\n","\n","        self.data = {}\n","        with ThreadPoolExecutor() as executor:\n","          file_loc = list(\n","              tqdm(\n","                executor.map(\n","                  partial(load_file_to_memory,self.df,self.data,self.data_dir,self.id2class),\n","                  range(self.df.shape[0])), \n","                total=self.df.shape[0],\n","                desc=\"Loaded Files\")\n","              )\n","        \n","        black_filter = []\n","        for idx in range(self.df.shape[0]):\n","          if len(self.data[idx][\"coords\"].shape)\u003c3 or self.data[idx][\"coords\"].shape[0]\u003c20:\n","            black_filter.append(idx)\n","\n","        self.indexes = [x for x in range(self.df.shape[0]) if x not in black_filter]\n","\n","    def __len__(self):\n","        return len(self.indexes)\n","\n","    def __getitem__(self, idx):\n","        idx = self.indexes[idx]\n","        target = self.df.iloc[idx,0]\n","        file_path = self.df.iloc[idx,1]\n","        a_file = self.data[idx]\n","        coords, vid_size = a_file[\"coords\"],a_file[\"video_size\"]\n","        \n","        coords = coords[:,self.active_locations,:]\n","\n","        if self.is_2d:\n","            coords = coords[...,0:2]\n","\n","        coords = torch.from_numpy(coords).float()\n","        coords = select_frames(coords,sel_len_to_sel=50)\n","\n","        shape = coords.shape\n","        coords = torch.reshape(coords, (shape[0], shape[1]*shape[2]))\n","        label = torch.clone(coords)\n","\n","        if self.transform:\n","            coords = self.transform(coords)\n","        if self.target_transform:\n","            label = self.target_transform(coords)\n","\n","        if self.file_name:\n","            return coords, label, target,a_file[\"video_size\"],file_path\n","        return coords, label, target,a_file[\"video_size\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49},"id":"o0Mlr88WEjWt","outputId":"eb9be919-05d1-4f9d-a66d-8103fc403630"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12ed8f81fb5e4988afd59d9295f04610","version_major":2,"version_minor":0},"text/plain":["Loaded Files:   0%|          | 0/8068 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data = SkeletonDataset(data_files,train_df,id2clsname,is_2d=True,file_name=False)\n","val_data = SkeletonDataset(data_files,val_df,id2clsname,is_2d=True)\n","test_data = SkeletonDataset(data_files,val_df,id2clsname,is_2d=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iThGfDYvch4Q"},"outputs":[],"source":["len(train_data),train_df.shape,len(val_data),val_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6mtux6OEmIk"},"outputs":[],"source":["train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gv4MRtlZbLYt"},"outputs":[],"source":["def gen_skeleton(frame, connections, height, width):\n","    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n","    img_3.fill(255)\n","\n","    # add circles\n","    for coord in frame:\n","        x, y = int(width*coord[0]), int(height*coord[1])\n","        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n","\n","    # add lines\n","    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n","    for line in mapping_list:\n","        i, j = line\n","        st = frame[i, :]\n","        start_point = (int(width*st[0]), int(height*st[1]))\n","\n","        en = frame[j, :]\n","        end_point = (int(width*en[0]), int(height*en[1]))\n","\n","        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n","\n","    return img_3\n","\n","def gen_video(points, save_file, frame_h, frame_w, is_3d=True):\n","    # make 3D if points are flatten\n","    if len(points.shape) == 2:\n","        if is_3d:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 3))\n","          y_cds = list(range(1, fts, 3))\n","          z_cds = list(range(2, fts, 3))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds], \n","                                          points[:, z_cds]]), (1,2,0))\n","        else:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 2))\n","          y_cds = list(range(1, fts, 2))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds]]), (1,2,0))\n","\n","    size = (frame_w, frame_h)\n","    result = cv2.VideoWriter(save_file,\n","                         cv2.VideoWriter_fourcc(*'MJPG'),\n","                         10, size)\n","\n","    # mapping_list = [(11, 12), (11, 13), (13, 15), (12, 14), (14, 16), (12, 24), (11, 23), (23, 24), (24, 26), (26, 28), (23, 25), (25, 27)]\n","    mapping_list = [(0, 1), (1, 3), (3, 5), (0, 2), (2, 4), (0, 6), (1, 7), (6, 7), (6, 8), (7, 9), (8, 10), (9, 11)]\n","    for __id,frame in enumerate(points):\n","        skel_image = gen_skeleton(frame, mapping_list, frame_h, frame_w)\n","        result.write(skel_image)\n","\n","    result.release()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OJzG7jyh7pU_"},"outputs":[],"source":["save_vids_dir = \"checking_vids/init\"\n","#for adata in tqdm(train_dl):\n","  #print(adata[0].size(), adata[1].size(), adata[2].size(),adata[4])\n","#  selected_ind = random.randint(0,adata[0].shape[0]-1)\n","#  data = adata[0][selected_ind].numpy()\n","#  file_id = adata[4][selected_ind].split(\".\")[0]\n","#  vid_size = adata[3][selected_ind].numpy()\n","\n","  #if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out.mp4\"):\n","    #os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n","    #gen_video(data, f\"{save_vids_dir}/{file_id}/dataloader_out.mp4\", vid_size[0], vid_size[1],is_3d=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZ2J16W-KBNw"},"outputs":[],"source":["#!zip -r /content/check_vids.zip /content/checking_vids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNhRUvwyFUQb"},"outputs":[],"source":["class BiLSTMEncoder(nn.Module):\n","    def __init__(self,seq_len, input_size,num_classes, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMEncoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev=dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","        self.num_classes = num_classes\n","\n","        # define LSTM layer\n","        self.layers = []\n","\n","        # add linear layers \n","        for __id,layer_out in enumerate(self.linear_filters):\n","            if __id == 0:\n","                self.layers.append(nn.Linear(self.input_size, layer_out))\n","            else:\n","                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n","\n","        # add lstm layer\n","        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n","                            batch_first=True)\n","        \n","        self.net = nn.Sequential(*self.layers)\n","\n","        self.classification_header = nn.Linear(self.embedding_size,self.num_classes)\n","\n","        #add embedding out\n","        if bidirectional:\n","            self.bn = nn.BatchNorm1d(self.hidden_size*4)\n","            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n","        else:\n","            self.bn = nn.BatchNorm1d(self.hidden_size*2)\n","            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n","\n","        \n","    def forward(self, x_input):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        x = self.net(x_input)\n","        lstm_out, self.hidden = self.lstm(x)\n","        hidden_transformed = torch.cat(self.hidden,0)\n","        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n","        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n","\n","        #hidden_transformed = self.bn(hidden_transformed)\n","        hidden_transformed = self.out_linear(hidden_transformed)\n","\n","        label = self.classification_header(hidden_transformed)\n","        \n","        return label, hidden_transformed\n","\n","    \n","class BiLSTMDecoder(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMDecoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","\n","        if bidirectional:\n","            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n","        else:\n","            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n","\n","        # define LSTM layer\n","        self.layers = []\n","        # add lstm\n","        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=True,\n","                            batch_first=bidirectional)\n","\n","                        \n","        # add linear layers \n","        if bidirectional:\n","            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n","        else:\n","            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n","\n","        for __id,layer_in in enumerate(self.linear_filters):\n","            if __id == len(linear_filters)-1:\n","                self.layers.append(nn.Linear(layer_in,self.input_size))\n","            else:\n","                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n","\n","        self.net = nn.Sequential(*self.layers)\n","\n","        \n","        \n","\n","    def forward(self,encoder_hidden):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        \n","        hidden_shape = encoder_hidden.shape\n","        encoder_hidden = self.input_linear(encoder_hidden)\n","        \n","        if self.bidirectional:\n","            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h1,h2,c1,c2 = torch.unbind(hidden,0)\n","            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n","            bs = h.size()[1]\n","        else:\n","            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h,c = torch.unbind(hidden,0)\n","            bs = h.size()[1]\n","        \n","        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n","        \n","        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n","        x = self.net(lstm_out)\n","        \n","        return x\n","\n","class BiLSTMEncDecModel(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size,num_classes, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMEncDecModel, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.batch_size = batch_size\n","        self.seq_len = seq_len\n","        self.num_classes= num_classes\n","        \n","        self.encoder = BiLSTMEncoder(seq_len, input_size, num_classes,hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n","        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n","        \n","    def forward(self,x):\n","        label,embedding = self.encoder(x)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, label\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxOl8YuW6f71"},"outputs":[],"source":["encoder = BiLSTMEncoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    num_classes = config[\"model\"][\"num_classes\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"]).to(device)\n","\n","decoder = BiLSTMDecoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"]).to(device)\n","\n","bilstm_model = BiLSTMEncDecModel(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    num_classes = config[\"model\"][\"num_classes\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RETBAwj46i3Z"},"outputs":[],"source":["bilstm_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F5Kn20ZT6m4H"},"outputs":[],"source":["label, embedding = encoder(torch.randn((32,50,24)).to(device))\n","embedding.shape,label.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8t79v6xM6wx7"},"outputs":[],"source":["label_map = [(k,v) for k,v in id2clsname.items()]\n","labelToId = {x[0]: i for i, x in enumerate(label_map)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfa8NjY57En3"},"outputs":[],"source":["def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n","    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n","    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n","    loss =  recon_loss + tar_loss\n","\n","    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n","    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n","\n","    return loss, {\n","        \"reconstruction_loss\":recon_loss.item(),\n","        \"target_loss\":tar_loss.item()\n","    }\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brTkTW9BEa3i"},"outputs":[],"source":["optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=1e-3, weight_decay=0.01)\n","std_loss = {\n","    \"reconstruction_loss\" :nn.L1Loss(),\n","    \"target_loss\" :nn.CrossEntropyLoss()\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUigg8dNcWJO"},"outputs":[],"source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=4)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DksWHRbgDgPQ"},"outputs":[],"source":["def train_step(model, dataloader, optimizer, loss_module, device, class_names):\n","    model = model.train()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    targets = []\n","    predicts = []\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=\"train\") as tepoch:\n","          for input_sequence,target_sequence,target_action,target_vid_size in tepoch:\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = target_sequence.to(device)\n","            target_action = target_action.to(device)\n","            \n","\n","            # Zero gradients, perform a backward pass, and update the weights.\n","            optimizer.zero_grad()\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(True):\n","            # with autocast():\n","                predicted_sequence, _, predicted_label  = model(input_sequence)\n","            \n","            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n","            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n","            loss =  config['alpha_recon']*recon_loss + config['alpha_target']*tar_loss\n","            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n","\n","            class_output = torch.argmax(predicted_label,dim=1)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            metrics = {\"loss\": loss.item()}\n","            with torch.no_grad():\n","                total_samples += len(target_action)\n","                epoch_loss += loss.item()  # add total loss of batch\n","\n","            # convert feature vector into action class using cosine\n","            pred_class = class_output.cpu().detach().numpy()\n","            metrics[\"accuracy\"] = accuracy_score(y_true=target_action.cpu().detach().numpy(), y_pred=pred_class)\n","            tepoch.set_postfix(metrics)\n","\n","            targets.append(target_action.cpu().detach().numpy())\n","            predicts.append(pred_class)\n","\n","    \n","    predicts = np.concatenate(predicts)\n","    targets = np.concatenate(targets)\n","    #train_metrics = action_evaluator(predicts,targets,class_names=list(clsname2id.keys()),print_report=False)\n","\n","    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmXFlJJdEN9N"},"outputs":[],"source":["def eval_step(model, dataloader,loss_module, device, class_names,  print_report=False, show_plot=False):\n","    model = model.eval()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    per_batch = {'targets': [], 'predictions': [], 'metrics': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n","\n","    with torch.no_grad():\n","      with tqdm(dataloader, unit=\"batch\", desc=\"eval\") as tepoch:\n","        for input_sequence,target_sequence,target_action,target_vid_size in tepoch:\n","\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = target_sequence.to(device)\n","            target_action = target_action.to(device)\n","\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(False):\n","            # with autocast():\n","                predicted_sequence,_,predicted_label  = model(input_sequence)\n","\n","            # loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            recon_loss = loss_module[\"reconstruction_loss\"](predicted_sequence,target_sequence)\n","            tar_loss = loss_module[\"target_loss\"](predicted_label,target_action)\n","            loss =  config['alpha_recon']*recon_loss + config['alpha_target']*tar_loss\n","            loss_detail = {\"reconstruction_loss\":recon_loss.item(),\"target_loss\":tar_loss.item()}\n","            \n","            pred_action = torch.argmax(predicted_label,dim=1)\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(target_action)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += loss_detail[\"reconstruction_loss\"]\n","                metrics['classi. loss'] += loss_detail[\"target_loss\"]\n","\n","            per_batch['targets'].append(target_action.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n","    metrics_dict.update(metrics)\n","    return metrics_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gEfzDnIP81qB"},"outputs":[],"source":["#model_params, model_config, config = get_config(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_HMDB51_skeleton_classifier/100__epoch50_emb1024_xy.pt\")\n","#bilstm_model.load_state_dict(model_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyUEFUlp7HOt"},"outputs":[],"source":["best_model_wts = copy.deepcopy(bilstm_model.state_dict())\n","best_acc = 0.0\n","show_interval = 10\n","\n","train_data = []\n","for epoch in tqdm(range(1, config[\"n_epochs\"] + 1), desc='Training Epoch', leave=False):\n","  \n","  train_metrics = train_step(bilstm_model, train_dl, optimizer, std_loss, device, class_names)\n","  train_metrics['epoch'] = epoch\n","  train_metrics['phase'] = 'train'\n","  train_data.append(train_metrics)\n","  \n","  if epoch % 10 == 0:\n","    eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=True, show_plot=True)\n","  else:\n","    eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=False, show_plot=False)\n","  eval_metrics['epoch'] = epoch \n","  eval_metrics['phase'] = 'valid'\n","  train_data.append(eval_metrics)\n","\n","  if epoch%10 == 0:\n","    save_model(\n","        bilstm_model, \n","        f\"temp_{model_ident}\", \n","        f\"{epoch}__{unique_iden}\",\n","         models_saves, \n","         config)\n","    \n","  if eval_metrics['accuracy'] \u003e best_acc:\n","    best_model = copy.deepcopy(bilstm_model.state_dict())\n","  \n","train_df = pd.DataFrame().from_records(train_data)\n","plot_curves(train_df)\n","\n","# replace by best model \n","bilstm_model.load_state_dict(best_model)\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"12ed8f81fb5e4988afd59d9295f04610":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_246a38f6ab364ffdab84183e6180ecd6","IPY_MODEL_bced63ced06f4d40aed8e54e157cfb1d","IPY_MODEL_ca6bec787d9e4d3d8c800a7f03874f9a"],"layout":"IPY_MODEL_e56ddc2b2bf1486aa164a62fd8b2f507"}},"1afc7e8726cc4af38befa158bf65d981":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"246a38f6ab364ffdab84183e6180ecd6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82712137503f4d4486ea134eda2ca07e","placeholder":"â€‹","style":"IPY_MODEL_a056b991023b487787891e26f961a18b","value":"Loaded Files:  19%"}},"3fa6e1b6b6664a51beff0803939f2499":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82712137503f4d4486ea134eda2ca07e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a056b991023b487787891e26f961a18b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af757c6f2405452a94bc2533582ff389":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b51ab8c7ddfe4e87957c18400fbe72e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bced63ced06f4d40aed8e54e157cfb1d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_af757c6f2405452a94bc2533582ff389","max":8068,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b51ab8c7ddfe4e87957c18400fbe72e4","value":1511}},"ca6bec787d9e4d3d8c800a7f03874f9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3fa6e1b6b6664a51beff0803939f2499","placeholder":"â€‹","style":"IPY_MODEL_1afc7e8726cc4af38befa158bf65d981","value":" 1511/8068 [00:36\u0026lt;05:06, 21.41it/s]"}},"e56ddc2b2bf1486aa164a62fd8b2f507":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}