{"cells":[{"cell_type":"markdown","metadata":{"id":"1RVz-hzNaDhz"},"source":["## Interpretable Model \n","- ICANN model\n","- with skeleton decoder\n","- and two new reconstruction losses\n","- 3 BiLSTM layers\n","- fixed single skeleton seq.\n","- freeze & non-freeze model \n","- two reconstruction loss [imu feat. & I3D feat.]"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21qdvhzbaF4k","outputId":"5b043e26-67b6-4eaa-e96d-4ce6ab638d6f","executionInfo":{"status":"ok","timestamp":1685324786516,"user_tz":-330,"elapsed":207325,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! pip install neptune decord pytorchvideo timm==0.4.12  transformers evaluate einops torchinfo\n","! git clone https://github.com/nipdep/HAR-ZSL-XAI.git --branch pd/PoseAE --single-branch\n","! mv /content/HAR-ZSL-XAI/src /content/"],"metadata":{"id":"r366knPcaGTL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685324826813,"user_tz":-330,"elapsed":40311,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"98b98261-84da-49c7-ba73-79f14be3c916"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting neptune\n","  Downloading neptune-1.2.0-py3-none-any.whl (448 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.1/448.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting decord\n","  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorchvideo\n","  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting timm==0.4.12\n","  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting transformers\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting einops\n","  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.4.12) (0.15.2+cu118)\n","Collecting GitPython>=2.0.8 (from neptune)\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.4.0)\n","Collecting PyJWT (from neptune)\n","  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n","Collecting boto3>=1.16.0 (from neptune)\n","  Downloading boto3-1.26.142-py3-none-any.whl (135 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n","  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.3)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n","Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (23.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.3)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n","Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.27.1)\n","Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n","Collecting swagger-spec-validator>=2.7.4 (from neptune)\n","  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.26.15)\n","Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.5.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from decord) (1.22.4)\n","Collecting fvcore (from pytorchvideo)\n","  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting av (from pytorchvideo)\n","  Downloading av-10.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.0/31.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting parameterized (from pytorchvideo)\n","  Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n","Collecting iopath (from pytorchvideo)\n","  Downloading iopath-0.1.10.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pytorchvideo) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Collecting datasets>=2.0.0 (from evaluate)\n","  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill (from evaluate)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash (from evaluate)\n","  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess (from evaluate)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting botocore<1.30.0,>=1.29.142 (from boto3>=1.16.0->neptune)\n","  Downloading botocore-1.29.142-py3-none-any.whl (10.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.16.0->neptune)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0 (from boto3>=1.16.0->neptune)\n","  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n","  Downloading bravado_core-5.17.1-py2.py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.5)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n","Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n","  Downloading simplejson-3.19.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n","  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (4.5.0)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Collecting aiohttp (from datasets>=2.0.0->evaluate)\n","  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune)\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.4)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.3.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (1.11.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.4.12) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.4->timm==0.4.12) (16.0.5)\n","Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n","  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n","Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (2.3.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->pytorchvideo) (0.8.10)\n","Collecting portalocker (from iopath->pytorchvideo)\n","  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2022.7.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n","Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune)\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.4.12) (2.1.2)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.19.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4->timm==0.4.12) (1.3.0)\n","Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n","Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Collecting rfc3987 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading rfc3987-1.3.8-py2.py3-none-any.whl (13 kB)\n","Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading uri_template-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n","  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pytorchvideo, fvcore, iopath\n","  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188694 sha256=d99867c08276a40c914c67c9b8a2a0da8f72c9c26601cdfbcddd2435da9fb5d0\n","  Stored in directory: /root/.cache/pip/wheels/ff/4e/81/0f72a543be9ed7eb737c95bfc5da4025e73226b44368074ece\n","  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61405 sha256=1dee7a6bb5907f2f5fa579a3cb503320c259fa1f35c368e4a30f3a318e7f82da\n","  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n","  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31531 sha256=4c0143160292aac51e890de32a6ff9abd4b18925c725cef9a3490feff346bd51\n","  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n","Successfully built pytorchvideo fvcore iopath\n","Installing collected packages: tokenizers, rfc3987, monotonic, av, yacs, xxhash, uri-template, torchinfo, smmap, simplejson, rfc3339-validator, PyJWT, portalocker, parameterized, multidict, jsonref, jsonpointer, jmespath, frozenlist, fqdn, einops, dill, decord, async-timeout, yarl, swagger-spec-validator, responses, multiprocess, iopath, huggingface-hub, gitdb, botocore, arrow, aiosignal, transformers, s3transfer, isoduration, GitPython, fvcore, aiohttp, pytorchvideo, boto3, datasets, bravado-core, evaluate, bravado, neptune, timm\n","Successfully installed GitPython-3.1.31 PyJWT-2.7.0 aiohttp-3.8.4 aiosignal-1.3.1 arrow-1.2.3 async-timeout-4.0.2 av-10.0.0 boto3-1.26.142 botocore-1.29.142 bravado-11.0.3 bravado-core-5.17.1 datasets-2.12.0 decord-0.6.0 dill-0.3.6 einops-0.6.1 evaluate-0.4.0 fqdn-1.5.1 frozenlist-1.3.3 fvcore-0.1.5.post20221221 gitdb-4.0.10 huggingface-hub-0.14.1 iopath-0.1.10 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.3 jsonref-1.1.0 monotonic-1.6 multidict-6.0.4 multiprocess-0.70.14 neptune-1.2.0 parameterized-0.9.0 portalocker-2.7.0 pytorchvideo-0.1.5 responses-0.18.0 rfc3339-validator-0.1.4 rfc3987-1.3.8 s3transfer-0.6.1 simplejson-3.19.1 smmap-5.0.0 swagger-spec-validator-3.0.3 timm-0.4.12 tokenizers-0.13.3 torchinfo-1.8.0 transformers-4.29.2 uri-template-1.2.0 xxhash-3.2.0 yacs-0.1.8 yarl-1.9.2\n","Cloning into 'HAR-ZSL-XAI'...\n","remote: Enumerating objects: 255, done.\u001b[K\n","remote: Counting objects: 100% (79/79), done.\u001b[K\n","remote: Compressing objects: 100% (54/54), done.\u001b[K\n","remote: Total 255 (delta 39), reused 53 (delta 25), pack-reused 176\u001b[K\n","Receiving objects: 100% (255/255), 103.34 MiB | 31.05 MiB/s, done.\n","Resolving deltas: 100% (114/114), done.\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"UdbayXMQaKDf"}},{"cell_type":"code","source":["data_root = '/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Datasets/Consolidated/PAMPA2'"],"metadata":{"id":"nL_OUR-8aJWH","executionInfo":{"status":"ok","timestamp":1685324826815,"user_tz":-330,"elapsed":24,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"eJ1ZWg_VaDiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685324835001,"user_tz":-330,"elapsed":8209,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"3ec2ad0b-15bf-4048-ae50-324fd9b9dc3a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-6bb350ee5c45>:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n"]}],"source":["from datetime import date, datetime\n","from tqdm.autonotebook import tqdm\n","from copy import deepcopy\n","from collections import defaultdict\n","import numpy as np \n","import numpy.random as random\n","import pandas as pd\n","import json\n","import pickle\n","from collections import defaultdict, OrderedDict\n","# import neptune\n","\n","import torch \n","from torch import nn, Tensor\n","from torch.nn import functional as F\n","from torch.nn.modules import MultiheadAttention, Linear, Dropout, BatchNorm1d, TransformerEncoderLayer\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","from torch.nn import MSELoss\n","\n","from src.datasets.data import PAMAP2Reader, PAMAP2ReaderV2\n","# from src.datasets.dataset import PAMAP2Dataset\n","from src.utils.analysis import action_evaluator\n","from src.datasets.utils import load_attribute\n","\n","from src.models.loss import FeatureLoss, AttributeLoss\n","from src.utils.losses import *\n","from src.utils.analysis import action_evaluator\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"98u3C7bAaDiI","executionInfo":{"status":"ok","timestamp":1685324836308,"user_tz":-330,"elapsed":1321,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["from sklearn.manifold import TSNE\n","# from umap import UMAP\n","\n","import matplotlib.pyplot as plt \n","import seaborn as sns \n","import plotly.express as px"]},{"cell_type":"markdown","metadata":{"id":"CzzYTE1XaDiK"},"source":["---"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dQqetUTlaDiN","executionInfo":{"status":"ok","timestamp":1685324836309,"user_tz":-330,"elapsed":21,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["# setup model configurations\n","config = {\n","    # general information\n","    \"experiment-name\": \"Experiment_section-1\", \n","    \"datetime\": date.today(),\n","    \"device\": \"gpu\",\n","    \"dataset\": \"PAMAP2\", # \"PAMAP2\", \"DaLiAc\", \"UTD\"\n","    \"Model\": \"BiLSTM\",\n","    \"sem-space\": 'I3D embeddings',\n","    # model training configs\n","    \"lr\": 0.0001,\n","    \"imu_alpha\": 0.0001,\n","    \"n_epochs\": 10,\n","    \"freeze_n\": 15,\n","    \"batch_size\": 64,\n","    # model configs\n","    \"d_model\": 128, \n","    \"num_heads\": 3,\n","    \"feat_size\": 400, # skel-AE hidden size and IMU-Anc output size\n","    # dataset configs\n","    \"window_size\": 5.21, \n","    \"overlap\": 4.21,\n","    \"seq_len\": 20,  # skeleton seq. length\n","    \"seen_split\": 0.1,\n","    \"unseen_split\": 0.8,\n","\n","}"]},{"cell_type":"markdown","metadata":{"id":"24Joud0LaDiO"},"source":["---"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"ey_WyRP7aDiP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685325007738,"user_tz":-330,"elapsed":171448,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"f20566bf-61a0-485b-ac2b-b7ff31a64cfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Reading file 1 of 14\n","Reading file 2 of 14\n","Reading file 3 of 14\n","Reading file 4 of 14\n","Reading file 5 of 14\n","Reading file 6 of 14\n","Reading file 7 of 14\n","Reading file 8 of 14\n","Reading file 9 of 14\n","Reading file 10 of 14\n","Reading file 11 of 14\n","Reading file 12 of 14\n","Reading file 13 of 14\n","Reading file 14 of 14\n"]},{"output_type":"stream","name":"stderr","text":["/content/src/datasets/data.py:1275: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  return np.asarray(data), np.asarray(labels, dtype=int), np.array(collection)\n"]}],"source":["IMU_data_path = data_root+'/IMU/Protocol/'\n","dataReader = PAMAP2ReaderV2(IMU_data_path)\n","actionList = dataReader.idToLabel"]},{"cell_type":"code","source":["actionList"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZcdXzUxM5FF","executionInfo":{"status":"ok","timestamp":1685325135053,"user_tz":-330,"elapsed":331,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"9d31885f-9f48-4ddb-d433-2ad48e6dd2a3"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['lying',\n"," 'sitting',\n"," 'standing',\n"," 'walking',\n"," 'running',\n"," 'cycling',\n"," 'Nordic walking',\n"," 'watching TV',\n"," 'computer work',\n"," 'car driving',\n"," 'ascending stairs',\n"," 'descending stairs',\n"," 'vacuum cleaning',\n"," 'ironing',\n"," 'folding laundry',\n"," 'house cleaning',\n"," 'playing soccer',\n"," 'rope jumping']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ILmCNMYqaDiR","executionInfo":{"status":"ok","timestamp":1685326841125,"user_tz":-330,"elapsed":543,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def read_I3D_pkl(loc,feat_size=\"400\"):\n","  if feat_size == \"400\":\n","    feat_index = 1\n","  elif feat_size == \"2048\":\n","    feat_index = 0\n","  else:\n","    raise NotImplementedError()\n","\n","  with open(loc,\"rb\") as f0:\n","    __data = pickle.load(f0)\n","\n","  label = []\n","  prototype = []\n","  for k,v in __data.items():\n","    label.append(k)\n","    all_arr = [x[feat_index] for x in v]\n","    all_arr = np.array(all_arr).mean(axis=0)\n","    prototype.append(all_arr)\n","\n","  label = np.array(label)\n","  prototype = np.array(prototype)\n","  return {\"activity\":label, \"features\":prototype}"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"QyEioe_JaDiS","colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"status":"ok","timestamp":1685326841126,"user_tz":-330,"elapsed":5,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"d519374e-7448-41f4-fd6c-20a835e70bae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"# load video dataset\\nskeleton_data = np.load(data_root+'/Skeletons/skeleton_mediapipe.npz')\\nskeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\\nnew_fts = [i for i in range(skeleton_mov.shape[-1]) if i%3 != 2]\\nseq_len = 60\\nskeleton_mov = skeleton_mov[:, :seq_len, new_fts]\\n\\nskel_mean = defaultdict(list)\\nfor i,c in enumerate(skeleton_classes):\\n    skel_mean[c] = skeleton_mov[i]\\n\\n# skel_mean = {}\\n# for k,v in skel_dict.items():\\n#     # print( np.array(v).shape)\\n#     skel_mean[k] = np.array(v).mean(axis=0)\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["\"\"\"# load video dataset\n","skeleton_data = np.load(data_root+'/Skeletons/skeleton_mediapipe.npz')\n","skeleton_classes, skeleton_mov = skeleton_data['arr_0'], skeleton_data['arr_1']\n","new_fts = [i for i in range(skeleton_mov.shape[-1]) if i%3 != 2]\n","seq_len = 60\n","skeleton_mov = skeleton_mov[:, :seq_len, new_fts]\n","\n","skel_mean = defaultdict(list)\n","for i,c in enumerate(skeleton_classes):\n","    skel_mean[c] = skeleton_mov[i]\n","\n","# skel_mean = {}\n","# for k,v in skel_dict.items():\n","#     # print( np.array(v).shape)\n","#     skel_mean[k] = np.array(v).mean(axis=0)\"\"\""]},{"cell_type":"code","source":["import os\n","\n","# load video dataset\n","video_embedding_path = \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/preprocessed/PAMPA2_embed/train\"\n","data = [np.load(os.path.join(video_embedding_path,x)) for x in os.listdir(video_embedding_path)]\n","video_data = np.stack([x[\"embedding\"] for x in data],axis=0)\n","original_data = np.stack([x[\"video\"] for x in data],axis=0)\n","video_classes = np.stack([x[\"label\"] for x in data],axis=0)\n","\n","skel_mean = defaultdict(list)\n","for i,c in enumerate(video_classes):\n","  try:\n","     skel_mean[str(c)].append(video_data[i])\n","  except KeyError:\n","     skel_mean[str(c)] = [video_data[i]]"],"metadata":{"id":"lHiQEV0tSfQJ","executionInfo":{"status":"ok","timestamp":1685326869904,"user_tz":-330,"elapsed":28307,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"30gRPHmDaDiV","executionInfo":{"status":"ok","timestamp":1685326869904,"user_tz":-330,"elapsed":14,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["# load video dataset\n","I3D_data_path  = data_root + '/I3D/video_feat.pkl'\n","video_data = read_I3D_pkl(I3D_data_path,feat_size=\"400\")\n","video_classes, video_feat = video_data['activity'], video_data['features']"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Cnenwwq4aDiY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685326869905,"user_tz":-330,"elapsed":13,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"7aabc9ae-99c2-473c-e61f-39399e40bea0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18, 400)"]},"metadata":{},"execution_count":17}],"source":["video_feat.shape"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"D0BPmMZ3aDif","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685326869906,"user_tz":-330,"elapsed":11,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"cd007185-5557-4f3d-d7b4-f5c61b4a0bcb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(18,)"]},"metadata":{},"execution_count":18}],"source":["video_classes.shape"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"p3w9W3_AaDii","executionInfo":{"status":"ok","timestamp":1685326869906,"user_tz":-330,"elapsed":8,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["label2Id = {c[1]:i for i,c in enumerate(dataReader.label_map)}\n","action_dict = defaultdict(list)\n","for i, a in enumerate(video_classes):\n","    action_dict[label2Id[a]].append(i)"]},{"cell_type":"code","source":["label2Id"],"metadata":{"id":"V6UHoqgzbSvM","executionInfo":{"status":"ok","timestamp":1685328838888,"user_tz":-330,"elapsed":337,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"ea2a2906-4333-4927-c4ca-ad224d790ff2","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'lying': 0,\n"," 'sitting': 1,\n"," 'standing': 2,\n"," 'walking': 3,\n"," 'running': 4,\n"," 'cycling': 5,\n"," 'Nordic walking': 6,\n"," 'watching TV': 7,\n"," 'computer work': 8,\n"," 'car driving': 9,\n"," 'ascending stairs': 10,\n"," 'descending stairs': 11,\n"," 'vacuum cleaning': 12,\n"," 'ironing': 13,\n"," 'folding laundry': 14,\n"," 'house cleaning': 15,\n"," 'playing soccer': 16,\n"," 'rope jumping': 17}"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"MpTGuWxhaDij","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685326869907,"user_tz":-330,"elapsed":9,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"b030f139-7d35-4c60-94ea-2829aec5fa59"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6"]},"metadata":{},"execution_count":20}],"source":["random.choice(range(len(skel_mean['Nordic walking'])))"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"gu-Eh5MdaDij","executionInfo":{"status":"ok","timestamp":1685326870631,"user_tz":-330,"elapsed":7,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["class PAMAP2Dataset(Dataset):\n","    def __init__(self, data, actions, attributes, attribute_dict, skel_dict, action_classes, all_classes, seq_len=120):\n","        super(PAMAP2Dataset, self).__init__()\n","        print(data.shape, actions,action_classes, all_classes)\n","        self.data = torch.from_numpy(data)\n","        self.actions = actions\n","        self.attribute_dict = attribute_dict\n","        self.skel_dict = skel_dict\n","        self.seq_len = seq_len\n","        self.attributes = torch.from_numpy(attributes)\n","        self.action_classes = action_classes\n","        # build action to id mapping dict\n","        self.n_action = len(self.actions)\n","        self.Id2action = dict(zip(action_classes, [all_classes[i] for i in action_classes]))\n","        self.action2Id = dict(zip(action_classes, range(self.n_action)))\n","\n","    def __getitem__(self, ind):\n","        x = self.data[ind, ...]\n","        target = self.actions[ind]\n","        target_name = self.Id2action[target]\n","        y = torch.from_numpy(np.array([self.action2Id[target]]))\n","        # extraction semantic space generation skeleton sequences\n","        vid_idx = random.choice(self.attribute_dict[target])\n","        y_feat = self.attributes[vid_idx, ...]\n","        y_skel_i = random.choice(range(len(self.skel_dict[target_name])))\n","        y_skel = self.skel_dict[target_name][y_skel_i]\n","        return x, y, y_feat, y_skel\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def getClassAttrs(self):\n","        sampling_idx = [random.choice(self.attribute_dict[i]) for i in self.action_classes]\n","        ft_mat = self.attributes[sampling_idx, ...]\n","        return ft_mat\n","\n","    def getClassFeatures(self):\n","        cls_feat = []\n","        for c in self.action_classes:\n","            idx = self.attribute_dict[c]\n","            cls_feat.append(torch.mean(self.attributes[idx, ...], dim=0))\n","\n","        cls_feat = torch.vstack(cls_feat)\n","        # print(cls_feat.size())\n","        return cls_feat"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"Uwe7aPROaDik","executionInfo":{"status":"ok","timestamp":1685326870632,"user_tz":-330,"elapsed":7,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["class IMUEncoder(nn.Module):\n","    def __init__(self, in_ft, d_model, ft_size, num_heads=1, dropout=0.1):\n","        super(IMUEncoder, self).__init__()\n","        self.in_ft = in_ft\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.ft_size = ft_size \n","\n","        self.lstm = nn.LSTM(input_size=self.in_ft,\n","                            hidden_size=self.d_model,\n","                            num_layers=self.num_heads,\n","                            batch_first=True,\n","                            bidirectional=True,\n","                            dropout=dropout)\n","        self.drop = nn.Dropout(p=0.1)\n","        self.act = nn.ReLU()\n","        self.fcLayer1 = nn.Linear(2*self.d_model, self.ft_size)\n","        # self.fcLayer2 = nn.Linear(self.ft_size, self.ft_size)\n","\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        out_forward = out[:,-1, :self.d_model]\n","        out_reverse = out[:, 0, self.d_model:]\n","        out_reduced = torch.cat((out_forward, out_reverse), 1)\n","        out = self.drop(out_reduced)\n","        out = self.act(out)\n","        out = self.fcLayer1(out)\n","        # out = self.fcLayer2(out)\n","        return out"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"7J8oW3L-aDil","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685326870633,"user_tz":-330,"elapsed":8,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"29f984ba-6185-40aa-9165-2cafc7298118"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 400])"]},"metadata":{},"execution_count":23}],"source":["sample_imu = IMUEncoder(in_ft=54, d_model=128, num_heads=3, ft_size=400)\n","\n","imu_input = torch.randn((32, 60, 54))\n","imu_output = sample_imu(imu_input)\n","imu_output.shape"]},{"cell_type":"markdown","source":["---\n","Video AE pre-training"],"metadata":{"id":"PBGB3eM7SpnZ"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"In_Eq4jzaDim","executionInfo":{"status":"ok","timestamp":1685326872399,"user_tz":-330,"elapsed":1772,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["from functools import partial\n","import numpy as np\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n","from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n","from timm.models.registry import register_model\n","import torch.utils.checkpoint as checkpoint\n","import math\n","\n","def pretrain_trunc_normal_(tensor, mean=0., std=1.):\n","    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n","        **kwargs\n","    }\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","    \n","    def extra_repr(self) -> str:\n","        return 'p={}'.format(self.drop_prob)\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        # x = self.drop(x)\n","        # commit this for the orignal BERT implement \n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","                 attn_head_dim=None):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if init_values > 0:\n","            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        else:\n","            self.gamma_1, self.gamma_2 = None, None\n","\n","    def forward(self, x):\n","        if self.gamma_1 is None:\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        else:\n","            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        self.tubelet_size = int(tubelet_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim, \n","                            kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]), \n","                            stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n","\n","    def forward(self, x, **kwargs):\n","        B, C, T, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","    \n","# sin-cos position encoding\n","# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n","def get_sinusoid_encoding_table(n_position, d_hid): \n","    ''' Sinusoid position encoding table ''' \n","    # TODO: make it with torch instead of numpy \n","    def get_position_angle_vec(position): \n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n","\n","    return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0) \n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, \n","                 img_size=224, \n","                 patch_size=16, \n","                 in_chans=3, \n","                 num_classes=1000, \n","                 embed_dim=768, \n","                 depth=12,\n","                 num_heads=12, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 fc_drop_rate=0., \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False, \n","                 init_scale=0.,\n","                 all_frames=16,\n","                 tubelet_size=2,\n","                 use_checkpoint=False,\n","                 use_mean_pooling=True):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.tubelet_size = tubelet_size\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=all_frames, tubelet_size=self.tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings is on the way\n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n","        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n","        self.fc_dropout = nn.Dropout(p=fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        pretrain_trunc_normal_(self.head.weight, std=.02)\n","        self.apply(self._init_weights)\n","\n","        self.head.weight.data.mul_(init_scale)\n","        self.head.bias.data.mul_(init_scale)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            pretrain_trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        x = self.patch_embed(x)\n","        B, _, _ = x.size()\n","\n","        if self.pos_embed is not None:\n","            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        x = self.pos_drop(x)\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        x = self.norm(x)\n","        if self.fc_norm is not None:\n","            return self.fc_norm(x.mean(1))\n","        else:\n","            return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(self.fc_dropout(x))\n","        return x\n","\n","\n","@register_model\n","def vit_small_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_512(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_huge_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","class PretrainVisionTransformerEncoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n","                 use_learnable_pos_emb=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,tubelet_size=tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        # TODO: Add the cls token\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings \n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        _, _, T, _, _ = x.shape\n","        x = self.patch_embed(x)\n","        \n","        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n","\n","        B, _, C = x.shape\n","        x_vis = x.reshape(B, -1, C) # ~mask means visible\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x_vis = checkpoint.checkpoint(blk, x_vis)\n","        else:   \n","            for blk in self.blocks:\n","                x_vis = blk(x_vis)\n","\n","        x_vis = self.norm(x_vis)\n","        return x_vis\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","class PretrainVisionTransformerDecoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n","                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n","                 norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n","                 ):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        assert num_classes == 3 * tubelet_size * patch_size ** 2 \n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_size = patch_size\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x, return_token_num):\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        if return_token_num > 0:\n","            x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n","        else:\n","            x = self.head(self.norm(x))\n","\n","        return x\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","\n","class PretrainVisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self,\n","                 img_size=224, \n","                 patch_size=16, \n","                 encoder_in_chans=3, \n","                 encoder_num_classes=0, \n","                 encoder_embed_dim=768, \n","                 encoder_depth=12,\n","                 encoder_num_heads=12, \n","                 decoder_num_classes=1536, #  decoder_num_classes=768, \n","                 decoder_embed_dim=512, \n","                 decoder_depth=8,\n","                 decoder_num_heads=8, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False,\n","                 use_checkpoint=False,\n","                 tubelet_size=2,\n","                 num_classes=0, # avoid the error from create_fn in timm\n","                 in_chans=0, # avoid the error from create_fn in timm\n","                 ):\n","        super().__init__()\n","        self.encoder = PretrainVisionTransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=encoder_in_chans, \n","            num_classes=encoder_num_classes, \n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint,\n","            use_learnable_pos_emb=use_learnable_pos_emb)\n","\n","        self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=patch_size, \n","            num_patches=self.encoder.patch_embed.num_patches,\n","            num_classes=decoder_num_classes, \n","            embed_dim=decoder_embed_dim, \n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint)\n","\n","        self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n","\n","        trunc_normal_(self.mask_token, std=.02)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'mask_token'}\n","\n","    def forward(self, x):\n","        _, _, T, _, _ = x.shape\n","        x_vis = self.encoder(x) # [B, N_vis, C_e]\n","        x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","        B, N, C = x_vis.shape\n","        # we don't unshuffle the correct visible token order, \n","        # but shuffle the pos embedding accorddingly.\n","        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","        pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","        x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","        x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","\n","        return x\n","\n","@register_model\n","def pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=768, \n","        encoder_depth=12, \n","        encoder_num_heads=12,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536,\n","        decoder_embed_dim=384,\n","        decoder_num_heads=6,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n"," \n","@register_model\n","def pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1024, \n","        encoder_depth=24, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=512,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1280, \n","        encoder_depth=32, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=640,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","\n","\n","class EncoderMappingV3(nn.Module):\n","  def __init__(self,embedding_dim, *args, **kwargs) -> None:\n","      super(EncoderMappingV3,self).__init__(*args, **kwargs)\n","      self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=2,dilation=2)\n","      self.conv2 = nn.Conv2d(16,32,kernel_size=3,stride=2,dilation=2)\n","      self.conv3 = nn.Conv2d(32,32,kernel_size=3,stride=2,dilation=2)\n","      self.pool1 = nn.AvgPool2d(3)\n","      self.pool2 = nn.AvgPool2d(3)\n","      \n","      self.embed = nn.Linear(32*19*3,embedding_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1):\n","    lout = (lin+2*padding-dilation*(kernal_size-1)-1)//stride+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = x.unsqueeze(1)\n","    x = self.conv1(x)\n","    x = self.pool1(x)\n","    x = self.conv2(x)\n","    x = self.pool2(x)\n","    x = self.conv3(x)\n","    x = x.flatten(1)\n","    x = self.embed(x)\n","    return x\n","\n","\n","class DecoderMappingV3(nn.Module):\n","  def __init__(self,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMappingV3,self).__init__(*args, **kwargs)\n","      \n","      self.conv1 = nn.ConvTranspose2d(8,1,kernel_size=3,stride=2,dilation=2)\n","      self.conv2 = nn.ConvTranspose2d(8,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv3 = nn.ConvTranspose2d(16,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv4 = nn.ConvTranspose2d(32,16,kernel_size=4,stride=2,dilation=2)\n","      self.conv5 = nn.ConvTranspose2d(32,32,kernel_size=3,stride=1,dilation=2)\n","      \n","      self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      \n","      self.linear1 = nn.Linear(549,384)\n","      self.linear2 = nn.Linear(1573,1568)\n","      \n","      self.embed = nn.Linear(embedding_dim,32*19*3)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = x.reshape(-1,32,19,3)\n","    x = self.conv5(x)\n","    x = self.upsample2(x)\n","    x = self.conv4(x)\n","    x = self.upsample1(x)\n","    x = self.conv3(x)\n","    x = self.conv2(x)\n","    \n","    x = self.conv1(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear2(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear1(x).squeeze()\n","    return x\n","\n","from functools import partial\n","import numpy as np\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from timm.models.layers import drop_path, to_2tuple, trunc_normal_\n","from timm.models.layers import trunc_normal_ as __call_trunc_normal_\n","from timm.models.registry import register_model\n","import torch.utils.checkpoint as checkpoint\n","import math\n","\n","def pretrain_trunc_normal_(tensor, mean=0., std=1.):\n","    __call_trunc_normal_(tensor, mean=mean, std=std, a=-std, b=std)\n","\n","\n","def _cfg(url='', **kwargs):\n","    return {\n","        'url': url,\n","        'num_classes': 400, 'input_size': (3, 224, 224), 'pool_size': None,\n","        'crop_pct': .9, 'interpolation': 'bicubic',\n","        'mean': (0.5, 0.5, 0.5), 'std': (0.5, 0.5, 0.5),\n","        **kwargs\n","    }\n","\n","\n","class DropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n","    \"\"\"\n","    def __init__(self, drop_prob=None):\n","        super(DropPath, self).__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, x):\n","        return drop_path(x, self.drop_prob, self.training)\n","    \n","    def extra_repr(self) -> str:\n","        return 'p={}'.format(self.drop_prob)\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        # x = self.drop(x)\n","        # commit this for the orignal BERT implement \n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","class Attention(nn.Module):\n","    def __init__(\n","            self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.,\n","            proj_drop=0., attn_head_dim=None):\n","        super().__init__()\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        if attn_head_dim is not None:\n","            head_dim = attn_head_dim\n","        all_head_dim = head_dim * self.num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        self.qkv = nn.Linear(dim, all_head_dim * 3, bias=False)\n","        if qkv_bias:\n","            self.q_bias = nn.Parameter(torch.zeros(all_head_dim))\n","            self.v_bias = nn.Parameter(torch.zeros(all_head_dim))\n","        else:\n","            self.q_bias = None\n","            self.v_bias = None\n","\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(all_head_dim, dim)\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","    def forward(self, x):\n","        B, N, C = x.shape\n","        qkv_bias = None\n","        if self.q_bias is not None:\n","            qkv_bias = torch.cat((self.q_bias, torch.zeros_like(self.v_bias, requires_grad=False), self.v_bias))\n","        # qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        qkv = F.linear(input=x, weight=self.qkv.weight, bias=qkv_bias)\n","        qkv = qkv.reshape(B, N, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        \n","        attn = attn.softmax(dim=-1)\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B, N, -1)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","\n","    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., init_values=None, act_layer=nn.GELU, norm_layer=nn.LayerNorm,\n","                 attn_head_dim=None):\n","        super().__init__()\n","        self.norm1 = norm_layer(dim)\n","        self.attn = Attention(\n","            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","            attn_drop=attn_drop, proj_drop=drop, attn_head_dim=attn_head_dim)\n","        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if init_values > 0:\n","            self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","            self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n","        else:\n","            self.gamma_1, self.gamma_2 = None, None\n","\n","    def forward(self, x):\n","        if self.gamma_1 is None:\n","            x = x + self.drop_path(self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.mlp(self.norm2(x)))\n","        else:\n","            x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x)))\n","            x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n","        return x\n","\n","\n","class PatchEmbed(nn.Module):\n","    \"\"\" Image to Patch Embedding\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, num_frames=16, tubelet_size=2):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        self.tubelet_size = int(tubelet_size)\n","        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]) * (num_frames // self.tubelet_size)\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","        self.proj = nn.Conv3d(in_channels=in_chans, out_channels=embed_dim, \n","                            kernel_size = (self.tubelet_size,  patch_size[0],patch_size[1]), \n","                            stride=(self.tubelet_size,  patch_size[0],  patch_size[1]))\n","\n","    def forward(self, x, **kwargs):\n","        B, C, T, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x).flatten(2).transpose(1, 2)\n","        return x\n","    \n","# sin-cos position encoding\n","# https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Models.py#L31\n","def get_sinusoid_encoding_table(n_position, d_hid): \n","    ''' Sinusoid position encoding table ''' \n","    # TODO: make it with torch instead of numpy \n","    def get_position_angle_vec(position): \n","        return [position / np.power(10000, 2 * (hid_j // 2) / d_hid) for hid_j in range(d_hid)] \n","\n","    sinusoid_table = np.array([get_position_angle_vec(pos_i) for pos_i in range(n_position)]) \n","    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2]) # dim 2i \n","    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2]) # dim 2i+1 \n","\n","    return  torch.tensor(sinusoid_table,dtype=torch.float, requires_grad=False).unsqueeze(0) \n","\n","\n","class VisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, \n","                 img_size=224, \n","                 patch_size=16, \n","                 in_chans=3, \n","                 num_classes=1000, \n","                 embed_dim=768, \n","                 depth=12,\n","                 num_heads=12, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 fc_drop_rate=0., \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False, \n","                 init_scale=0.,\n","                 all_frames=16,\n","                 tubelet_size=2,\n","                 use_checkpoint=False,\n","                 use_mean_pooling=True):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.tubelet_size = tubelet_size\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, num_frames=all_frames, tubelet_size=self.tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings is on the way\n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm = nn.Identity() if use_mean_pooling else norm_layer(embed_dim)\n","        self.fc_norm = norm_layer(embed_dim) if use_mean_pooling else None\n","        self.fc_dropout = nn.Dropout(p=fc_drop_rate) if fc_drop_rate > 0 else nn.Identity()\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        pretrain_trunc_normal_(self.head.weight, std=.02)\n","        self.apply(self._init_weights)\n","\n","        self.head.weight.data.mul_(init_scale)\n","        self.head.bias.data.mul_(init_scale)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            pretrain_trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        x = self.patch_embed(x)\n","        B, _, _ = x.size()\n","\n","        if self.pos_embed is not None:\n","            x = x + self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        x = self.pos_drop(x)\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        x = self.norm(x)\n","        if self.fc_norm is not None:\n","            return self.fc_norm(x.mean(1))\n","        else:\n","            return x[:, 0]\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(self.fc_dropout(x))\n","        return x\n","\n","\n","@register_model\n","def vit_small_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=384, depth=12, num_heads=6, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_base_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_384(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=384, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_large_patch16_512(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        img_size=512, patch_size=16, embed_dim=1024, depth=24, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","\n","@register_model\n","def vit_huge_patch16_224(pretrained=False, **kwargs):\n","    model = VisionTransformer(\n","        patch_size=16, embed_dim=1280, depth=32, num_heads=16, mlp_ratio=4, qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n","    model.default_cfg = _cfg()\n","    return model\n","\n","class PretrainVisionTransformerEncoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=0, embed_dim=768, depth=12,\n","                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n","                 drop_path_rate=0., norm_layer=nn.LayerNorm, init_values=None, tubelet_size=2, use_checkpoint=False,\n","                 use_learnable_pos_emb=False):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,tubelet_size=tubelet_size)\n","        num_patches = self.patch_embed.num_patches\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        # TODO: Add the cls token\n","        if use_learnable_pos_emb:\n","            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n","        else:\n","            # sine-cosine positional embeddings \n","            self.pos_embed = get_sinusoid_encoding_table(num_patches, embed_dim)\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        if use_learnable_pos_emb:\n","            pretrain_trunc_normal_(self.pos_embed, std=.02)\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward_features(self, x):\n","        _, _, T, _, _ = x.shape\n","        x = self.patch_embed(x)\n","        \n","        x = x + self.pos_embed.type_as(x).to(x.device).clone().detach()\n","\n","        B, _, C = x.shape\n","        x_vis = x.reshape(B, -1, C) # ~mask means visible\n","\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x_vis = checkpoint.checkpoint(blk, x_vis)\n","        else:   \n","            for blk in self.blocks:\n","                x_vis = blk(x_vis)\n","\n","        x_vis = self.norm(x_vis)\n","        return x_vis\n","\n","    def forward(self, x):\n","        x = self.forward_features(x)\n","        x = self.head(x)\n","        return x\n","\n","class PretrainVisionTransformerDecoder(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self, patch_size=16, num_classes=768, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n","                 qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.,\n","                 norm_layer=nn.LayerNorm, init_values=None, num_patches=196, tubelet_size=2, use_checkpoint=False\n","                 ):\n","        super().__init__()\n","        self.num_classes = num_classes\n","        assert num_classes == 3 * tubelet_size * patch_size ** 2 \n","        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n","        self.patch_size = patch_size\n","        self.use_checkpoint = use_checkpoint\n","\n","\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n","        self.blocks = nn.ModuleList([\n","            Block(\n","                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n","                init_values=init_values)\n","            for i in range(depth)])\n","        self.norm =  norm_layer(embed_dim)\n","        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","        self.apply(self._init_weights)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token'}\n","\n","    def get_classifier(self):\n","        return self.head\n","\n","    def reset_classifier(self, num_classes, global_pool=''):\n","        self.num_classes = num_classes\n","        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n","\n","    def forward(self, x, return_token_num):\n","        if self.use_checkpoint:\n","            for blk in self.blocks:\n","                x = checkpoint.checkpoint(blk, x)\n","        else:   \n","            for blk in self.blocks:\n","                x = blk(x)\n","\n","        if return_token_num > 0:\n","            x = self.head(self.norm(x[:, -return_token_num:])) # only return the mask tokens predict pixels\n","        else:\n","            x = self.head(self.norm(x))\n","\n","        return x\n","\n","class PretrainVisionTransformer(nn.Module):\n","    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n","    \"\"\"\n","    def __init__(self,\n","                 img_size=224, \n","                 patch_size=16, \n","                 encoder_in_chans=3, \n","                 encoder_num_classes=0, \n","                 encoder_embed_dim=768, \n","                 encoder_depth=12,\n","                 encoder_num_heads=12, \n","                 decoder_num_classes=1536, #  decoder_num_classes=768, \n","                 decoder_embed_dim=512, \n","                 decoder_depth=8,\n","                 decoder_num_heads=8, \n","                 mlp_ratio=4., \n","                 qkv_bias=False, \n","                 qk_scale=None, \n","                 drop_rate=0., \n","                 attn_drop_rate=0.,\n","                 drop_path_rate=0., \n","                 norm_layer=nn.LayerNorm, \n","                 init_values=0.,\n","                 use_learnable_pos_emb=False,\n","                 use_checkpoint=False,\n","                 tubelet_size=2,\n","                 num_classes=0, # avoid the error from create_fn in timm\n","                 in_chans=0, # avoid the error from create_fn in timm\n","                 ):\n","        super().__init__()\n","        self.encoder = PretrainVisionTransformerEncoder(\n","            img_size=img_size, \n","            patch_size=patch_size, \n","            in_chans=encoder_in_chans, \n","            num_classes=encoder_num_classes, \n","            embed_dim=encoder_embed_dim, \n","            depth=encoder_depth,\n","            num_heads=encoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint,\n","            use_learnable_pos_emb=use_learnable_pos_emb)\n","\n","        self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=patch_size, \n","            num_patches=self.encoder.patch_embed.num_patches,\n","            num_classes=decoder_num_classes, \n","            embed_dim=decoder_embed_dim, \n","            depth=decoder_depth,\n","            num_heads=decoder_num_heads, \n","            mlp_ratio=mlp_ratio, \n","            qkv_bias=qkv_bias, \n","            qk_scale=qk_scale, \n","            drop_rate=drop_rate, \n","            attn_drop_rate=attn_drop_rate,\n","            drop_path_rate=drop_path_rate, \n","            norm_layer=norm_layer, \n","            init_values=init_values,\n","            tubelet_size=tubelet_size,\n","            use_checkpoint=use_checkpoint)\n","\n","        self.encoder_to_decoder = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=False)\n","\n","        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))\n","\n","        self.pos_embed = get_sinusoid_encoding_table(self.encoder.patch_embed.num_patches, decoder_embed_dim)\n","\n","        trunc_normal_(self.mask_token, std=.02)\n","\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            nn.init.xavier_uniform_(m.weight)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    def get_num_layers(self):\n","        return len(self.blocks)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'pos_embed', 'cls_token', 'mask_token'}\n","\n","    def forward(self, x):\n","        _, _, T, _, _ = x.shape\n","        x_vis = self.encoder(x) # [B, N_vis, C_e]\n","        x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","        B, N, C = x_vis.shape\n","        # we don't unshuffle the correct visible token order, \n","        # but shuffle the pos embedding accorddingly.\n","        expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","        pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","        pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","        x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","        x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","\n","        return x\n","\n","@register_model\n","def pretrain_videomae_small_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16,\n","        encoder_embed_dim=384,\n","        encoder_depth=12,\n","        encoder_num_heads=6,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=192, \n","        decoder_num_heads=3,\n","        mlp_ratio=4,\n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6),\n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_base_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=768, \n","        encoder_depth=12, \n","        encoder_num_heads=12,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536,\n","        decoder_embed_dim=384,\n","        decoder_num_heads=6,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n"," \n","@register_model\n","def pretrain_videomae_large_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1024, \n","        encoder_depth=24, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=512,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","@register_model\n","def pretrain_videomae_huge_patch16_224(pretrained=False, **kwargs):\n","    model = PretrainVisionTransformer(\n","        img_size=224,\n","        patch_size=16, \n","        encoder_embed_dim=1280, \n","        encoder_depth=32, \n","        encoder_num_heads=16,\n","        encoder_num_classes=0,\n","        decoder_num_classes=1536, \n","        decoder_embed_dim=640,\n","        decoder_num_heads=8,\n","        mlp_ratio=4, \n","        qkv_bias=True,\n","        norm_layer=partial(nn.LayerNorm, eps=1e-6), \n","        **kwargs)\n","    model.default_cfg = _cfg()\n","    if pretrained:\n","        checkpoint = torch.load(\n","            kwargs[\"init_ckpt\"], map_location=\"cpu\"\n","        )\n","        model.load_state_dict(checkpoint[\"model\"])\n","    return model\n","\n","class DecoderMappingV3(nn.Module):\n","  def __init__(self,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMappingV3,self).__init__(*args, **kwargs)\n","      \n","      self.conv1 = nn.ConvTranspose2d(8,1,kernel_size=3,stride=2,dilation=2)\n","      self.conv2 = nn.ConvTranspose2d(8,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv3 = nn.ConvTranspose2d(16,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv4 = nn.ConvTranspose2d(32,16,kernel_size=4,stride=2,dilation=2)\n","      self.conv5 = nn.ConvTranspose2d(32,32,kernel_size=3,stride=1,dilation=2)\n","      \n","      self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      \n","      self.linear1 = nn.Linear(549,384)\n","      self.linear2 = nn.Linear(1573,1568)\n","      \n","      self.embed = nn.Linear(embedding_dim,32*19*3)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = x.reshape(-1,32,19,3)\n","    x = self.conv5(x)\n","    x = self.upsample2(x)\n","    x = self.conv4(x)\n","    x = self.upsample1(x)\n","    x = self.conv3(x)\n","    x = self.conv2(x)\n","    \n","    x = self.conv1(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear2(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear1(x).squeeze()\n","    return x\n","  \n","\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","\n","class EncoderMappingV1(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(EncoderMappingV1,self).__init__(*args, **kwargs)\n","      input_dim = encoder_embed_dim\n","\n","      lout1 = EncoderMappingV1.calc_lout(num_patches,512,3,input_dim,stride=2)\n","      lout2 = EncoderMappingV1.calc_lout(512,128,3,lout1,stride=1)\n","\n","      self.attn1 = Attention(\n","            lout2, num_heads=12)\n","      self.linear1 = nn.Linear(lout2,128)\n","      \n","      self.attn2 = Attention(\n","            128, num_heads=12)\n","      self.linear2 = nn.Linear(128,32)\n","      \n","      self.conv1 = nn.Conv1d(num_patches,512,3, stride=2)\n","      self.conv2 = nn.Conv1d(512,128,3, stride=1)\n","\n","      self.linear3 = nn.Linear(32*128,1024)\n","      self.embed = nn.Linear(1024,embedding_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1):\n","    lout = (lin+2*padding-dilation*(kernal_size-1)-1)//stride+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    \n","    x = self.attn1(x)\n","    x = self.linear1(x)\n","    x = self.attn2(x)\n","    x = self.linear2(x)\n","\n","    x = x.flatten(1)\n","    \n","    x = self.linear3(x)\n","    x = self.embed(x)\n","    return x\n","\n","\n","\n","class DecoderMappingV1(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMappingV1,self).__init__(*args, **kwargs)\n","      input_dim = encoder_embed_dim\n","\n","      lout2 = DecoderMappingV1.calc_lout(128,512,3,input_dim,stride=1)\n","      lout1 = DecoderMappingV1.calc_lout(512,num_patches,3,lout2,stride=2)\n","\n","      self.attn1 = Attention(\n","            input_dim, num_heads=12)\n","      self.linear1 = nn.Linear(128,input_dim)\n","      \n","      self.attn2 = Attention(\n","            128, num_heads=12)\n","      self.linear2 = nn.Linear(32,128)\n","      \n","      self.conv1 = nn.ConvTranspose1d(512,num_patches,3, stride=2)\n","      self.conv2 = nn.ConvTranspose1d(128,512,3, stride=1)\n","\n","      self.linear3 = nn.Linear(1024,32*128)\n","      self.embed = nn.Linear(embedding_dim,1024)\n","      self.linear4 = nn.Linear(lout1,input_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = self.linear3(x)\n","    x = x.reshape(-1,128,32)\n","\n","    x = self.linear2(x)\n","    x = self.attn2(x)\n","    x = self.linear1(x)\n","    x = self.attn1(x)\n","\n","\n","    x = self.conv2(x)\n","    x = self.conv1(x)\n","    x = self.linear4(x)\n","    \n","    return x\n","  \n","  \n","  \n","class EncoderMappingV2(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(EncoderMappingV2,self).__init__(*args, **kwargs)\n","      self.pool1 = nn.AveragePool1d\n","      self.embed = nn.Linear(num_patches*encoder_embed_dim,embedding_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1):\n","    lout = (lin+2*padding-dilation*(kernal_size-1)-1)//stride+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = x.flatten(1)\n","    x = self.embed(x)\n","    return x\n","\n","\n","\n","class DecoderMappingV2(nn.Module):\n","  def __init__(self,encoder_embed_dim,num_patches,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMappingV2,self).__init__(*args, **kwargs)\n","      \n","      self.num_patches = num_patches\n","      self.encoder_embed_dim = encoder_embed_dim\n","\n","      self.linear1 = nn.Linear(embedding_dim,num_patches*encoder_embed_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = x.reshape(-1,self.num_patches,self.encoder_embed_dim)\n","    return x\n","  \n","  \n","\n","class EncoderMappingV3(nn.Module):\n","  def __init__(self,embedding_dim, *args, **kwargs) -> None:\n","      super(EncoderMappingV3,self).__init__(*args, **kwargs)\n","      self.conv1 = nn.Conv2d(1,16,kernel_size=3,stride=2,dilation=2)\n","      self.conv2 = nn.Conv2d(16,32,kernel_size=3,stride=2,dilation=2)\n","      self.conv3 = nn.Conv2d(32,32,kernel_size=3,stride=2,dilation=2)\n","      self.pool1 = nn.AvgPool2d(3)\n","      self.pool2 = nn.AvgPool2d(3)\n","      \n","      self.embed = nn.Linear(32*19*3,embedding_dim)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1):\n","    lout = (lin+2*padding-dilation*(kernal_size-1)-1)//stride+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = x.unsqueeze(1)\n","    x = self.conv1(x)\n","    x = self.pool1(x)\n","    x = self.conv2(x)\n","    x = self.pool2(x)\n","    x = self.conv3(x)\n","    x = x.flatten(1)\n","    x = self.embed(x)\n","    return x\n","\n","\n","\n","class DecoderMappingV3(nn.Module):\n","  def __init__(self,embedding_dim, *args, **kwargs) -> None:\n","      super(DecoderMappingV3,self).__init__(*args, **kwargs)\n","      \n","      self.conv1 = nn.ConvTranspose2d(8,1,kernel_size=3,stride=2,dilation=2)\n","      self.conv2 = nn.ConvTranspose2d(8,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv3 = nn.ConvTranspose2d(16,8,kernel_size=3,stride=2,dilation=2)\n","      self.conv4 = nn.ConvTranspose2d(32,16,kernel_size=4,stride=2,dilation=2)\n","      self.conv5 = nn.ConvTranspose2d(32,32,kernel_size=3,stride=1,dilation=2)\n","      \n","      self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear')\n","      \n","      self.linear1 = nn.Linear(549,384)\n","      self.linear2 = nn.Linear(1573,1568)\n","      \n","      self.embed = nn.Linear(embedding_dim,32*19*3)\n","\n","  @staticmethod\n","  def calc_lout(cin,cout,kernal_size,lin,stride=1,padding=0,dilation=1,output_padding=0):\n","    lout = (lin-1)*stride-2*padding+dilation*(kernal_size - 1) + output_padding+1\n","    return lout\n","\n","  def forward(self,x):\n","    x = self.embed(x)\n","    x = x.reshape(-1,32,19,3)\n","    x = self.conv5(x)\n","    x = self.upsample2(x)\n","    x = self.conv4(x)\n","    x = self.upsample1(x)\n","    x = self.conv3(x)\n","    x = self.conv2(x)\n","    \n","    x = self.conv1(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear2(x)\n","    x = torch.transpose(x,-2,-1)\n","    x = self.linear1(x).squeeze()\n","    return x\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","\n","class VideoDecoder(nn.Module):\n","  def __init__(self,embedding_size, *args, **kwargs) -> None:\n","      super(VideoDecoder,self).__init__(*args, **kwargs)\n","      self.decoder = PretrainVisionTransformerDecoder(\n","            patch_size=16, \n","            num_patches=1568,\n","            num_classes=1536, \n","            embed_dim=192, \n","            depth=4,\n","            num_heads=3, \n","            mlp_ratio=4, \n","            qkv_bias=True, \n","            qk_scale=None, \n","            drop_rate=0, \n","            attn_drop_rate=0,\n","            drop_path_rate=0, \n","            norm_layer=nn.LayerNorm, \n","            init_values=0,\n","            tubelet_size=2,\n","            use_checkpoint=False)\n","      self.encoder_to_decoder = nn.Linear(384, 192, bias=False)\n","      self.map_unit = DecoderMappingV3(embedding_dim=embedding_size)\n","\n","      self.mask_token = nn.Parameter(torch.zeros(1, 1, self.decoder.embed_dim))\n","\n","      self.pos_embed = get_sinusoid_encoding_table(1568, self.decoder.embed_dim)\n","\n","      trunc_normal_(self.mask_token, std=.02)\n","\n","  def forward(self,x):\n","    x_vis = self.map_unit(x)\n","    x_vis = self.encoder_to_decoder(x_vis) # [B, N_vis, C_d]\n","    B, N, C = x_vis.shape\n","    # we don't unshuffle the correct visible token order, \n","    # but shuffle the pos embedding accorddingly.\n","    expand_pos_embed = self.pos_embed.expand(B, -1, -1).type_as(x).to(x.device).clone().detach()\n","    pos_emd_vis = expand_pos_embed.reshape(B, -1, C)\n","    pos_emd_mask = expand_pos_embed.reshape(B, -1, C)\n","    x_full = torch.cat([x_vis + pos_emd_vis, self.mask_token + pos_emd_mask], dim=1) # [B, N, C_d]\n","    x = self.decoder(x_full, pos_emd_mask.shape[1]) # [B, N_mask, 3 * 16 * 16]\n","    \n","    return x\n","  "]},{"cell_type":"code","execution_count":25,"metadata":{"id":"VT62VI5laDio","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1685326872399,"user_tz":-330,"elapsed":8,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"d06ccf56-1bfa-4903-8d42-f171b76547b5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'video_decoder.to(device);'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":25}],"source":["def get_config(file_loc,device):\n","    file = torch.load(file_loc,map_location=device)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\"\"\"video_decoder = EncDecModel(\n","    encoder = EncoderMappingV3(400),\n","    decoder = DecoderMappingV3(400),\n","    classifier = SGNClassifier(51,400)\n",")\"\"\"\n","\n","\n","\"\"\"video_decoder.load_state_dict(\n","    torch.load(\n","        \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/pretrained_weights/temp_HMDB51_Mapping_unitv2_classifier_1024_emb1d/34__epoch50_emb400.pt\",\n","               map_location=device)[\"model_state_dict\"])\"\"\"\n","\n","\n","\"\"\"video_decoder.to(device);\"\"\""]},{"cell_type":"code","execution_count":26,"metadata":{"id":"NGazdIxBaDir","executionInfo":{"status":"ok","timestamp":1685326872400,"user_tz":-330,"elapsed":7,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["class CompoundModel(nn.Module):\n","    def __init__(self, config, freeze=False,decoder_pretrain=None):\n","        super(CompoundModel, self).__init__()\n","        self.imu_encoder = IMUEncoder(**config['imu_config'])\n","        self.skel_decoder = DecoderMappingV3(400)\n","\n","        if decoder_pretrain:\n","          self.skel_decoder.load_state_dict(\n","              torch.load(decoder_pretrain)\n","          )\n","          \n","        self.device = config['device']\n","\n","        if freeze:\n","            self.__freeze_model()\n","\n","    def __freeze_model(self):\n","        for params in self.skel_decoder.parameters():\n","                params.requies_grad = False\n","    \n","    def unfreeze(self):\n","        for params in self.skel_decoder.parameters():\n","                params.requies_grad = True\n","\n","    def forward(self, x, feat):\n","        imu_feat = self.imu_encoder(x)\n","        skel_recon = self.skel_decoder(imu_feat)\n","        i3d_recon = self.skel_decoder(feat)\n","        return imu_feat, skel_recon, i3d_recon"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"i-yIbGHEaDis","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685326886146,"user_tz":-330,"elapsed":13753,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"873e28a0-8f63-4414-e757-820067b0a5b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 400]) torch.Size([32, 1568, 384]) torch.Size([1568, 384])\n"]}],"source":["imu_config = {\n","    'in_ft': 54,\n","    'd_model': 256,\n","    'ft_size': 400,\n","    'num_heads': 2,\n","    'dropout': 0.1\n","}\n","\n","decoder_config = {\n","    'input_size': 24,\n","    'seq_len': 60,\n","    'hidden_size': 128,\n","    'linear_filters': [64, 128],\n","    'embedding_size': 400,\n","    'num_layers': 1,\n","    'bidirectional': True,\n","    # 'batch_size': 32,\n","    'device': 'cpu'\n","}\n","\n","model_config = {\n","    'imu_config': imu_config,\n","    'skel_config': decoder_config,\n","    'device': 'cpu'\n","}\n","\n","sample_model = CompoundModel(model_config,decoder_pretrain=\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/VideoAE/model_saves/decoder_58__epoch50_emb400.pt\").to(device)\n","sample_input = torch.randn((32, 78, 54)).to(device)\n","sample_feat = torch.randn((1, 400)).to(device)\n","output_y, output_recon1, output_recon2 = sample_model(sample_input, sample_feat)\n","print(output_y.shape, output_recon1.shape, output_recon2.shape)"]},{"cell_type":"markdown","metadata":{"id":"w9dxgQYGaDit"},"source":["---"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"vCsf1yhraDit","executionInfo":{"status":"ok","timestamp":1685326886148,"user_tz":-330,"elapsed":25,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["if config['device'] == 'cpu':\n","    device = \"cpu\"\n","else:\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"nrxZVqOIaDiu","executionInfo":{"status":"ok","timestamp":1685326886150,"user_tz":-330,"elapsed":26,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["# run 5-fold running\n","fold_classes = [['watching TV', 'house cleaning', 'standing', 'ascending stairs'], ['walking', 'rope jumping', 'sitting', 'descending stairs'], ['playing soccer', 'lying', 'vacuum cleaning', 'computer work'], ['cycling', 'running', 'Nordic walking'], ['ironing', 'car driving', 'folding laundry']]\n","\n","fold_cls_ids = [[actionList.index(i) for i in j] for j in fold_classes]"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"lvoG38wEaDiu","executionInfo":{"status":"ok","timestamp":1685326886154,"user_tz":-330,"elapsed":27,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def loss_cross_entropy(y_pred, y, feat, loss_fn):\n","    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n","    feat_norm = torch.norm(feat, p=2, dim=1)\n","    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n","    softmax_vec = torch.softmax(norm_vec, dim=1)\n","    output = loss_fn(softmax_vec, y)\n","    pred = torch.argmax(softmax_vec, dim=-1)\n","    return output, pred\n","\n","def loss_reconstruction_calc(y_pred, y_feat, loss_fn=nn.L1Loss(reduction=\"sum\")):\n","    loss = loss_fn(y_pred,y_feat)\n","    return loss\n","\n","def predict_class(y_pred, feat):\n","    mm_vec = torch.mm(y_pred, torch.transpose(feat, 0, 1))\n","    feat_norm = torch.norm(feat, p=2, dim=1)\n","    norm_vec = mm_vec/torch.unsqueeze(feat_norm, 0)\n","    softmax_vec = torch.softmax(norm_vec, dim=1)\n","    pred = torch.argmax(softmax_vec, dim=-1)\n","    return pred"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"FOEiPEZ2aDiv","executionInfo":{"status":"ok","timestamp":1685326886155,"user_tz":-330,"elapsed":26,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def train_step(model, dataloader, dataset, optimizer, loss_module, device, class_names, phase='train', l2_reg=False, loss_alpha=0.7, loss_beta=0.5):\n","    model = model.train()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    random_selected_feat = dataset.getClassFeatures().to(device)\n","    # print(random_selected_feat.shape)\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n","        for batch in tepoch:\n","            X, targets, target_feat, skel = batch\n","            # print(X.shape, targets.shape, target_feat.shape, skel.shape)\n","            X = X.float().to(device)\n","            skel = skel.float().to(device)\n","            target_feat = target_feat.float().to(device)\n","            targets = targets.long().to(device)\n","\n","            # Zero gradients, perform a backward pass, and update the weights.\n","            optimizer.zero_grad()\n","            # forward\n","            # track history if only in train\n","            with torch.set_grad_enabled(phase == 'train'):\n","            # with autocast():\n","                feat_output, recon_output, i3d_recon_output = model(X, target_feat)\n","                class_loss, class_output = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn=loss_module['class'] )\n","                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n","                recon_loss = loss_module['recon_loss'](i3d_recon_output, skel) + loss_module['recon_loss'](recon_output, skel)\n","\n","            #loss = cross_entropy_loss\n","            loss = feat_loss + loss_alpha*class_loss + loss_beta*recon_loss\n","            # class_output = predict_class(feat_output,random_selected_feat)\n","\n","            if phase == 'train':\n","                loss.backward()\n","                optimizer.step()\n","\n","            metrics = {\"loss\": loss.item()}\n","            with torch.no_grad():\n","                total_samples += len(targets)\n","                epoch_loss += loss.item()  # add total loss of batch\n","\n","            # convert feature vector into action class\n","            # using cosine\n","            pred_class = class_output.cpu().detach().numpy()\n","            metrics[\"accuracy\"] = accuracy_score(y_true=targets.cpu().detach().numpy(), y_pred=pred_class)\n","            tepoch.set_postfix(metrics)\n","\n","    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n","    return metrics"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"y24W3XvcaDiv","executionInfo":{"status":"ok","timestamp":1685326886156,"user_tz":-330,"elapsed":26,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def eval_step(model, \n","              dataloader,\n","              dataset, \n","              loss_module, \n","              device, \n","              class_names,  \n","              phase='seen', \n","              l2_reg=False, \n","              print_report=False, \n","              show_plot=False, \n","              loss_alpha=0.7, \n","              loss_beta=0.5,\n","              ):\n","    model = model.eval()\n","    random_selected_feat = dataset.getClassFeatures().to(device)\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    per_batch = {'target_masks': [], 'targets': [], 'predictions': [], 'metrics': [], 'IDs': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0, 'recon. loss': 0}\n","    pred_videos = {\n","        \"true\":[],\n","        \"pred\":[],\n","        \"action_true\":[],\n","        \"action_pred\":[]\n","    }\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=phase) as tepoch:\n","        for batch in tepoch:\n","            X, targets, target_feat, skel = batch\n","            X = X.float().to(device)\n","            skel = skel.float().to(device)\n","            target_feat = target_feat.float().to(device)\n","            targets = targets.long().to(device)\n","\n","            # forward\n","            # track history if only in train\n","            with torch.set_grad_enabled(phase == 'train'):\n","            # with autocast():\n","                feat_output, recon_output, i3d_recon_output = model(X, target_feat)\n","                class_loss, class_output = loss_cross_entropy(feat_output,targets.squeeze(),random_selected_feat,loss_fn =loss_module['class'] )\n","                feat_loss = loss_reconstruction_calc(feat_output,target_feat,loss_fn=loss_module[\"feature\"])\n","                recon_loss = loss_module['recon_loss'](i3d_recon_output, skel) + loss_module['recon_loss'](recon_output, skel)\n","                pred_videos[\"pred\"].append(recon_output[0].detach().cpu().numpy())\n","                pred_videos[\"true\"].append(skel[0].detach().cpu().numpy())\n","                pred_videos[\"action_pred\"].append(class_output[0].detach().cpu().numpy())\n","                pred_videos[\"action_true\"].append(targets.squeeze()[0].detach().cpu().numpy())\n","\n","            #loss = cross_entropy_loss\n","            loss = feat_loss + loss_alpha*class_loss + loss_beta*recon_loss\n","            # class_output = predict_class(feat_output,random_selected_feat)\n","\n","            pred_action = class_output\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(targets)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += feat_loss.item()\n","                metrics['classi. loss'] += class_loss.item()\n","                metrics['recon. loss'] += recon_loss.item()\n","\n","            per_batch['targets'].append(targets.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets[:, 0], class_names=class_names, print_report=print_report, show_plot=show_plot)\n","    metrics_dict.update(metrics)\n","    return metrics_dict,pred_videos"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"XOuHfHWiaDiv","executionInfo":{"status":"ok","timestamp":1685326886168,"user_tz":-330,"elapsed":37,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    df['recon. loss'] = df['recon. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=5)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='recon. loss', hue='phase', marker='o', ax=axs[3]).set(title=\"Reconstruction Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[4]).set(title=\"Accuracy\")"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"ATkZnwBtaDiw","executionInfo":{"status":"ok","timestamp":1685326886175,"user_tz":-330,"elapsed":43,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[],"source":["def log(fold, phase, metrics):\n","    for m, v in metrics.items():\n","        if fold == 'global':\n","            run[f'global/{m}'].log(v)\n","        else:\n","            run[f\"Fold-{fold}/{phase}/{m}\"].log(v) "]},{"cell_type":"code","source":["def get_config(file_loc,device):\n","    file = torch.load(file_loc,map_location=device)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","\n","\"\"\"model_params, model_config, saved_config = get_config(\n","    f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_NTURGB120_skeleton_SGN_classifier/10__epoch50_emb400_xy.pt\",\n","    device\n","    )\"\"\""],"metadata":{"id":"suv9_KWqKY6T","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1685326886176,"user_tz":-330,"elapsed":43,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"fa94902e-21e2-4d77-d24a-cfe32156a751"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'model_params, model_config, saved_config = get_config(\\n    f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_NTURGB120_skeleton_SGN_classifier/10__epoch50_emb400_xy.pt\",\\n    device\\n    )'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["dataReader.idToLabel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D4sAjt6TbFUD","executionInfo":{"status":"ok","timestamp":1685328782492,"user_tz":-330,"elapsed":10,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}},"outputId":"af11c58a-e4ee-4776-ec42-4b6c5c98b70d"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['lying',\n"," 'sitting',\n"," 'standing',\n"," 'walking',\n"," 'running',\n"," 'cycling',\n"," 'Nordic walking',\n"," 'watching TV',\n"," 'computer work',\n"," 'car driving',\n"," 'ascending stairs',\n"," 'descending stairs',\n"," 'vacuum cleaning',\n"," 'ironing',\n"," 'folding laundry',\n"," 'house cleaning',\n"," 'playing soccer',\n"," 'rope jumping']"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","execution_count":36,"metadata":{"id":"2KUWHAYQaDiw","colab":{"base_uri":"https://localhost:8080/","height":927,"referenced_widgets":["0b9abd59cf27431c965a8af74f0f44f6","2a0e679146e14cc9a82f9529ee5ffe56","5d4a4c40655c423691b3074d867cb7c5","19f0e6e5767040dbb210c7c65af734f3","20eac718befe4e0fb66971c76a42c031","26c802c289844c01a522dd062a48c39a","63e0761c99754b16a2988680f1c8b62f","3a4d5d0f4c5a40708bc7dfe549f252b0","f40766ede7084f0d8a0ba4bfeb614e00","ca8610d457984ccd9ab8710633de1222","4a6d41e783bb4cd8b73b54691ea663c3","8ce12051aa4c4a64bec127b7ea61f6b1","63432c37c1e644f69795ae3709679c59","aba25d9a53d9483a81b43b04c793ffd2","ea6d670333df49d8845b128a6f2ecfa6","24d78c8df3d242ff933710bdab9e36af","e94278253f1548b7997da123ffadfc2f","64ba068d9c8341018d3d540e6bbbbf9d","12c8143057b84b5ca81f71b4fbb8acce","77844d236a284ab380aad51ce6b7f850","0fbb3c2696b94d0989f3a87cfe56cea5","222eb50f99994d8d979c3c441794956d","36394d758b434ea6b5c32c6e38b81ea2","3ab207a2ca224868b705aa4e238816f7","c4f58caca1f141e2b1390ecf249503d1","ba309eaf29f64a22b0620999a550307e","20709f57f4454e0698ca190cb6a57d39","a1d1517343584d1ea7385da6af67a8f5","c9f3906b433a4edda0a1d3fc57bfb705","a477e463586f4236b01981b656bb24cf","dd3cb6a1e6264230a41150cd7d3d3196","c3bed3a501b54788814c614dc971358c","bdd419d16e6746dc84bc7c18ff11fcb1","0350ee2ac9034d0eab225301813fe858","5b74f95675054fedac11ca39cf58acc1","9bb7000e272948d4bfc4ca0d6c8a4e53","62c2125070f840b3b1ae516244d3a590","181c0138ef904d11a735262d69f7c25c","2334903fa01e4bc1b5af51b8a64f16de","babfd16017f84a28af68b2595ace315d","fff910629ebe4abf8782237df459da5e","82b7b1e871f1448588a79fbc240a2e37","de21029203114fddbfc2caeafa336029","22f167803df6474eaaea82d18ff03c57","d8655ad56b9d43b6a9cb8bdf24d2e7c0","4086a0a3a1e7426e907a94a1d432b33b","c6f7ce12e37d495e99952cd916e7c770","f43681bc5eec46be9483e0fae4062d48","6e8fb73f5039411dbb4afdcba4bd3ecc","17cb74ca87284ad9adf1fa9c8b3bee9c","88a01713fa3047aba63d5e2e1a4ee5f9","a96ea57f8af84d09b04387afd7e44dc2","6401868128ac47b2b3b2a42f4e146e13","8a6b557731c24cbd97ce2cafb99e739a","35eafba9153a498e91dd4b3635fedf23","b0a9ce04bae44d28bdb6f3ec0d46b7a0","513bdbc39efc4a49b08434f965ba5b56","fb53979e4bf645d3ba7433992c3cf02b","b5d59c5a12404554af4890d523d3f960","139773311a0c461f98c532cc8956938a","a3762fedd4904883b0e636d196d057f2","f8d7819a838e488f878cceef2710d2bb","07693230734d4c55b5fefb3f1fcdbeef","f237515594284e33a0475056971f39a8","867c28ff577f43a69a926c0e357d9927","76bff80fe3654568b1bc3e9dc6393352","506ad937704641348aaa47aba041c592","3a525a7f2a3f4155a92eaf0d4e453808","398363798f294e38b8d3a051bead3f2a","cd75587d75b149df9eb72c7fea552c6d","d14f370264ce45069f687e1d83f7ea0c","6258f4bd4a024c31932b051e0bedcd1f","74c3b513b19d40a2b416744f36a286c6","db9ca45dd95c4a56abdbf5a9fba196e0","076de881f1844da185a98e8617705621","3c158b31ce274409bef1ee48cffd8920","84a5315b0b3f411db6b8b0bf699a9ec9","e6b09d78489e490f96ec867196ee6531","4ccdfe0bda29491fbe513adc3d14aaa0","11630a60f95c4d84aba14c8a2c64a9ee","c051d5711bf74df0a1d20051cb10e9f4","80976f0119424d9486ee9eb8c5616727","d0b44b22481a4553821fd0efe9601301","40aaf9b02f9d4c518898fe873330f647","16a82123cf01467599d964ee885ff753","96e4fd9d46fd4482aac77f29af0b044f","6bc5722d2bba49488dc626e7dd734ed4","d5a0630464bf4863ac0b2f6d33fbc4be"]},"outputId":"6a3e5a63-721e-4ddd-bb11-089d5d06db43","executionInfo":{"status":"error","timestamp":1685327739850,"user_tz":-330,"elapsed":853715,"user":{"displayName":"That RandomGamer","userId":"02453500423869449578"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["================ Fold-0 ================\n","Unseen Classes : ['watching TV', 'house cleaning', 'standing', 'ascending stairs']\n","seen classes >  [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17]\n","unseen classes >  [7, 15, 2, 10]\n","Initiate IMU datasets ...\n","(18846, 26, 54) [ 1  8  3 ... 13  0  8] [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17] ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling', 'Nordic walking', 'watching TV', 'computer work', 'car driving', 'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n","(2095, 26, 54) [ 8 12 11 ... 16  6  9] [0, 1, 3, 4, 5, 6, 8, 9, 11, 12, 13, 14, 16, 17] ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling', 'Nordic walking', 'watching TV', 'computer work', 'car driving', 'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n","(5608, 26, 54) [ 2  2  2 ... 15 15 15] [7, 15, 2, 10] ['lying', 'sitting', 'standing', 'walking', 'running', 'cycling', 'Nordic walking', 'watching TV', 'computer work', 'car driving', 'ascending stairs', 'descending stairs', 'vacuum cleaning', 'ironing', 'folding laundry', 'house cleaning', 'playing soccer', 'rope jumping']\n","{7: 'watching TV', 15: 'house cleaning', 2: 'standing', 10: 'ascending stairs'}\n","{7: 0, 15: 1, 2: 2, 10: 3}\n"]},{"output_type":"display_data","data":{"text/plain":["Training Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9abd59cf27431c965a8af74f0f44f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["train:   0%|          | 0/294 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce12051aa4c4a64bec127b7ea61f6b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["seen:   0%|          | 0/32 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36394d758b434ea6b5c32c6e38b81ea2"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Overall accuracy: 0.313\n","\n"]},{"output_type":"display_data","data":{"text/plain":["train:   0%|          | 0/294 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0350ee2ac9034d0eab225301813fe858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["seen:   0%|          | 0/32 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8655ad56b9d43b6a9cb8bdf24d2e7c0"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Overall accuracy: 0.410\n","\n"]},{"output_type":"display_data","data":{"text/plain":["train:   0%|          | 0/294 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a9ce04bae44d28bdb6f3ec0d46b7a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["seen:   0%|          | 0/32 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"506ad937704641348aaa47aba041c592"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Overall accuracy: 0.682\n","\n"]},{"output_type":"display_data","data":{"text/plain":["train:   0%|          | 0/294 [00:00<?, ?batch/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b09d78489e490f96ec867196ee6531"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-d8e315d94ec2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_classes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'phase'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-31-c48b9b7d2805>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, dataset, optimizer, loss_module, device, class_names, phase, l2_reg, loss_alpha, loss_beta)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mtotal_samples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import pickle\n","\n","# run['parameters'] = config\n","fold_metric_scores = []\n","for i, cs in enumerate(fold_cls_ids):\n","    print(\"=\"*16, f'Fold-{i}', \"=\"*16)\n","    print(f'Unseen Classes : {fold_classes[i]}')\n","\n","    data_dict = dataReader.generate(unseen_classes=cs, seen_ratio=config['seen_split'], unseen_ratio=config['unseen_split'], window_size=config['window_size'], window_overlap=config['overlap'], resample_freq=config['seq_len'])\n","    all_classes = dataReader.idToLabel\n","    seen_classes = data_dict['seen_classes']\n","    unseen_classes = data_dict['unseen_classes']\n","    print(\"seen classes > \", seen_classes)\n","    print(\"unseen classes > \", unseen_classes)\n","    train_n, seq_len, in_ft = data_dict['train']['X'].shape\n","\n","    print(\"Initiate IMU datasets ...\")\n","    # build IMU datasets\n","    train_dt = PAMAP2Dataset(data=data_dict['train']['X'], actions=data_dict['train']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=seen_classes, all_classes=all_classes, seq_len=100)\n","    train_dl = DataLoader(train_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    # build seen eval_dt\n","    eval_dt = PAMAP2Dataset(data=data_dict['eval-seen']['X'], actions=data_dict['eval-seen']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=seen_classes, all_classes=all_classes, seq_len=100)\n","    eval_dl = DataLoader(eval_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    # build unseen test_dt\n","    test_dt = PAMAP2Dataset(data=data_dict['test']['X'], actions=data_dict['test']['y'], attributes=video_feat, attribute_dict=action_dict, skel_dict=skel_mean, action_classes=unseen_classes, all_classes=all_classes, seq_len=100)\n","    print(test_dt.Id2action)\n","    print(test_dt.action2Id)\n","    test_dl = DataLoader(test_dt, batch_size=config['batch_size'], shuffle=True, pin_memory=True, drop_last=True)\n","    \n","    # build model\n","    imu_config = {\n","        'in_ft': in_ft,\n","        'd_model': config['d_model'],\n","        'ft_size': config['feat_size'],\n","        'num_heads': config['num_heads'],\n","        'dropout': 0.1\n","    }\n","\n","    \"\"\"decoder_config = {\n","        'seq_len': saved_config[\"model\"][\"seq_len\"],\n","        'input_size': saved_config[\"model\"][\"input_size\"],\n","        'hidden_size': saved_config[\"model\"][\"decoder_hidden_size\"],\n","        'linear_filters': saved_config[\"model\"][\"linear_filters\"],\n","        'embedding_size': saved_config[\"model\"][\"embedding_size\"],\n","        'num_layers': saved_config[\"model\"][\"num_layers\"],\n","        'bidirectional': saved_config[\"model\"][\"bidirectional\"],\n","        'device': device\n","    }\"\"\"\n","\n","    model_config = {\n","        'imu_config': imu_config,\n","        'skel_config': decoder_config,\n","        'device': 'cpu'\n","    }\n","    model = CompoundModel(model_config,\n","                          decoder_pretrain=None,\n","                          freeze=True)\n","    \"\"\"_ = model.load_state_dict(model_params, strict=False)\"\"\"\n","    model.to(device)\n","\n","    # define run parameters \n","    optimizer = Adam(model.parameters(), lr=config['lr'], weight_decay=1e-6)\n","    loss_module = {'class': nn.CrossEntropyLoss(), 'feature': nn.L1Loss(), 'recon_loss': nn.MSELoss()}\n","    best_acc = 0.0\n","\n","    # train the model \n","    train_data = []\n","    for epoch in tqdm(range(config['n_epochs']), desc='Training Epoch', leave=False):\n","        if epoch == config['freeze_n']:\n","            model.unfreeze()\n","    \n","        train_metrics = train_step(model, train_dl, train_dt,optimizer, loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='train', loss_alpha=0.0001, loss_beta=0.9)\n","        train_metrics['epoch'] = epoch\n","        train_metrics['phase'] = 'train'\n","        train_data.append(train_metrics)\n","        # log(i, 'train', train_metrics)\n","\n","        eval_metrics,_ = eval_step(model, eval_dl, eval_dt,loss_module, device, class_names=[all_classes[i] for i in seen_classes], phase='seen', loss_alpha=0.0001, loss_beta=0.9, print_report=False, show_plot=False)\n","        eval_metrics['epoch'] = epoch \n","        eval_metrics['phase'] = 'valid'\n","        train_data.append(eval_metrics)\n","        # log(i, 'eval', eval_metrics)\n","        # print(f\"EPOCH [{epoch}] TRAINING : {train_metrics}\")\n","        # print(f\"EPOCH [{epoch}] EVAL : {eval_metrics}\")\n","        if eval_metrics['accuracy'] > best_acc:\n","            best_model = deepcopy(model.state_dict())\n","    \n","    train_df = pd.DataFrame().from_records(train_data)\n","    plot_curves(train_df)\n","\n","    # replace by best model \n","    model.load_state_dict(best_model)\n","    # save_model(model,notebook_iden,model_iden,i)\n","\n","    # run evaluation on unseen classes\n","    test_metrics,recon_vids = eval_step(model, test_dl,test_dt, loss_module, device, class_names=[all_classes[i] for i in unseen_classes], phase='unseen', loss_alpha=0.0001, print_report=True, show_plot=True)\n","    \n","    with open(f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/epoch_vids/recon_videos/{i}_2_recon_videos.pkl\",\"wb\") as f0:\n","      pickle.dump(recon_vids,f0)\n","\n","    test_metrics['N'] = len(unseen_classes)\n","    fold_metric_scores.append(test_metrics)\n","    # log('test', i, test_metrics)\n","    print(test_metrics)\n","    print(\"=\"*40)\n","\n","print(\"=\"*14, \"Overall Unseen Classes Performance\", \"=\"*14)\n","seen_score_df = pd.DataFrame.from_records(fold_metric_scores)\n","weighted_score_df = seen_score_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].multiply(seen_score_df[\"N\"], axis=\"index\")\n","final_results = weighted_score_df.sum()/seen_score_df['N'].sum()\n","print(final_results)\n","# log('global', '',final_results.to_dict())\n","# run.stop()\n"]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"vH-a7-2Ug_BQ"}}],"metadata":{"kernelspec":{"display_name":"fyp_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"0b9abd59cf27431c965a8af74f0f44f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a0e679146e14cc9a82f9529ee5ffe56","IPY_MODEL_5d4a4c40655c423691b3074d867cb7c5","IPY_MODEL_19f0e6e5767040dbb210c7c65af734f3"],"layout":"IPY_MODEL_20eac718befe4e0fb66971c76a42c031"}},"2a0e679146e14cc9a82f9529ee5ffe56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26c802c289844c01a522dd062a48c39a","placeholder":"​","style":"IPY_MODEL_63e0761c99754b16a2988680f1c8b62f","value":"Training Epoch:  30%"}},"5d4a4c40655c423691b3074d867cb7c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a4d5d0f4c5a40708bc7dfe549f252b0","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f40766ede7084f0d8a0ba4bfeb614e00","value":3}},"19f0e6e5767040dbb210c7c65af734f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca8610d457984ccd9ab8710633de1222","placeholder":"​","style":"IPY_MODEL_4a6d41e783bb4cd8b73b54691ea663c3","value":" 3/10 [14:04&lt;29:24, 252.09s/it]"}},"20eac718befe4e0fb66971c76a42c031":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26c802c289844c01a522dd062a48c39a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e0761c99754b16a2988680f1c8b62f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a4d5d0f4c5a40708bc7dfe549f252b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f40766ede7084f0d8a0ba4bfeb614e00":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca8610d457984ccd9ab8710633de1222":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a6d41e783bb4cd8b73b54691ea663c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ce12051aa4c4a64bec127b7ea61f6b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_63432c37c1e644f69795ae3709679c59","IPY_MODEL_aba25d9a53d9483a81b43b04c793ffd2","IPY_MODEL_ea6d670333df49d8845b128a6f2ecfa6"],"layout":"IPY_MODEL_24d78c8df3d242ff933710bdab9e36af"}},"63432c37c1e644f69795ae3709679c59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e94278253f1548b7997da123ffadfc2f","placeholder":"​","style":"IPY_MODEL_64ba068d9c8341018d3d540e6bbbbf9d","value":"train: 100%"}},"aba25d9a53d9483a81b43b04c793ffd2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12c8143057b84b5ca81f71b4fbb8acce","max":294,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77844d236a284ab380aad51ce6b7f850","value":294}},"ea6d670333df49d8845b128a6f2ecfa6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fbb3c2696b94d0989f3a87cfe56cea5","placeholder":"​","style":"IPY_MODEL_222eb50f99994d8d979c3c441794956d","value":" 294/294 [03:56&lt;00:00,  1.21batch/s, loss=1.27, accuracy=0.281]"}},"24d78c8df3d242ff933710bdab9e36af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e94278253f1548b7997da123ffadfc2f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"64ba068d9c8341018d3d540e6bbbbf9d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12c8143057b84b5ca81f71b4fbb8acce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77844d236a284ab380aad51ce6b7f850":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fbb3c2696b94d0989f3a87cfe56cea5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"222eb50f99994d8d979c3c441794956d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"36394d758b434ea6b5c32c6e38b81ea2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3ab207a2ca224868b705aa4e238816f7","IPY_MODEL_c4f58caca1f141e2b1390ecf249503d1","IPY_MODEL_ba309eaf29f64a22b0620999a550307e"],"layout":"IPY_MODEL_20709f57f4454e0698ca190cb6a57d39"}},"3ab207a2ca224868b705aa4e238816f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a1d1517343584d1ea7385da6af67a8f5","placeholder":"​","style":"IPY_MODEL_c9f3906b433a4edda0a1d3fc57bfb705","value":"seen: 100%"}},"c4f58caca1f141e2b1390ecf249503d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a477e463586f4236b01981b656bb24cf","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd3cb6a1e6264230a41150cd7d3d3196","value":32}},"ba309eaf29f64a22b0620999a550307e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3bed3a501b54788814c614dc971358c","placeholder":"​","style":"IPY_MODEL_bdd419d16e6746dc84bc7c18ff11fcb1","value":" 32/32 [00:12&lt;00:00,  2.53batch/s, loss=1.28]"}},"20709f57f4454e0698ca190cb6a57d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d1517343584d1ea7385da6af67a8f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f3906b433a4edda0a1d3fc57bfb705":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a477e463586f4236b01981b656bb24cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd3cb6a1e6264230a41150cd7d3d3196":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3bed3a501b54788814c614dc971358c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdd419d16e6746dc84bc7c18ff11fcb1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0350ee2ac9034d0eab225301813fe858":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b74f95675054fedac11ca39cf58acc1","IPY_MODEL_9bb7000e272948d4bfc4ca0d6c8a4e53","IPY_MODEL_62c2125070f840b3b1ae516244d3a590"],"layout":"IPY_MODEL_181c0138ef904d11a735262d69f7c25c"}},"5b74f95675054fedac11ca39cf58acc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2334903fa01e4bc1b5af51b8a64f16de","placeholder":"​","style":"IPY_MODEL_babfd16017f84a28af68b2595ace315d","value":"train: 100%"}},"9bb7000e272948d4bfc4ca0d6c8a4e53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fff910629ebe4abf8782237df459da5e","max":294,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82b7b1e871f1448588a79fbc240a2e37","value":294}},"62c2125070f840b3b1ae516244d3a590":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de21029203114fddbfc2caeafa336029","placeholder":"​","style":"IPY_MODEL_22f167803df6474eaaea82d18ff03c57","value":" 294/294 [04:00&lt;00:00,  1.22batch/s, loss=1.11, accuracy=0.406]"}},"181c0138ef904d11a735262d69f7c25c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2334903fa01e4bc1b5af51b8a64f16de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"babfd16017f84a28af68b2595ace315d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fff910629ebe4abf8782237df459da5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82b7b1e871f1448588a79fbc240a2e37":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de21029203114fddbfc2caeafa336029":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f167803df6474eaaea82d18ff03c57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8655ad56b9d43b6a9cb8bdf24d2e7c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4086a0a3a1e7426e907a94a1d432b33b","IPY_MODEL_c6f7ce12e37d495e99952cd916e7c770","IPY_MODEL_f43681bc5eec46be9483e0fae4062d48"],"layout":"IPY_MODEL_6e8fb73f5039411dbb4afdcba4bd3ecc"}},"4086a0a3a1e7426e907a94a1d432b33b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_17cb74ca87284ad9adf1fa9c8b3bee9c","placeholder":"​","style":"IPY_MODEL_88a01713fa3047aba63d5e2e1a4ee5f9","value":"seen: 100%"}},"c6f7ce12e37d495e99952cd916e7c770":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a96ea57f8af84d09b04387afd7e44dc2","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6401868128ac47b2b3b2a42f4e146e13","value":32}},"f43681bc5eec46be9483e0fae4062d48":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a6b557731c24cbd97ce2cafb99e739a","placeholder":"​","style":"IPY_MODEL_35eafba9153a498e91dd4b3635fedf23","value":" 32/32 [00:12&lt;00:00,  2.61batch/s, loss=1.06]"}},"6e8fb73f5039411dbb4afdcba4bd3ecc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17cb74ca87284ad9adf1fa9c8b3bee9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88a01713fa3047aba63d5e2e1a4ee5f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a96ea57f8af84d09b04387afd7e44dc2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6401868128ac47b2b3b2a42f4e146e13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8a6b557731c24cbd97ce2cafb99e739a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35eafba9153a498e91dd4b3635fedf23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b0a9ce04bae44d28bdb6f3ec0d46b7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_513bdbc39efc4a49b08434f965ba5b56","IPY_MODEL_fb53979e4bf645d3ba7433992c3cf02b","IPY_MODEL_b5d59c5a12404554af4890d523d3f960"],"layout":"IPY_MODEL_139773311a0c461f98c532cc8956938a"}},"513bdbc39efc4a49b08434f965ba5b56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3762fedd4904883b0e636d196d057f2","placeholder":"​","style":"IPY_MODEL_f8d7819a838e488f878cceef2710d2bb","value":"train: 100%"}},"fb53979e4bf645d3ba7433992c3cf02b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07693230734d4c55b5fefb3f1fcdbeef","max":294,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f237515594284e33a0475056971f39a8","value":294}},"b5d59c5a12404554af4890d523d3f960":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_867c28ff577f43a69a926c0e357d9927","placeholder":"​","style":"IPY_MODEL_76bff80fe3654568b1bc3e9dc6393352","value":" 294/294 [04:00&lt;00:00,  1.24batch/s, loss=0.939, accuracy=0.656]"}},"139773311a0c461f98c532cc8956938a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3762fedd4904883b0e636d196d057f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d7819a838e488f878cceef2710d2bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07693230734d4c55b5fefb3f1fcdbeef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f237515594284e33a0475056971f39a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"867c28ff577f43a69a926c0e357d9927":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76bff80fe3654568b1bc3e9dc6393352":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"506ad937704641348aaa47aba041c592":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a525a7f2a3f4155a92eaf0d4e453808","IPY_MODEL_398363798f294e38b8d3a051bead3f2a","IPY_MODEL_cd75587d75b149df9eb72c7fea552c6d"],"layout":"IPY_MODEL_d14f370264ce45069f687e1d83f7ea0c"}},"3a525a7f2a3f4155a92eaf0d4e453808":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6258f4bd4a024c31932b051e0bedcd1f","placeholder":"​","style":"IPY_MODEL_74c3b513b19d40a2b416744f36a286c6","value":"seen: 100%"}},"398363798f294e38b8d3a051bead3f2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_db9ca45dd95c4a56abdbf5a9fba196e0","max":32,"min":0,"orientation":"horizontal","style":"IPY_MODEL_076de881f1844da185a98e8617705621","value":32}},"cd75587d75b149df9eb72c7fea552c6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c158b31ce274409bef1ee48cffd8920","placeholder":"​","style":"IPY_MODEL_84a5315b0b3f411db6b8b0bf699a9ec9","value":" 32/32 [00:12&lt;00:00,  2.61batch/s, loss=0.85]"}},"d14f370264ce45069f687e1d83f7ea0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6258f4bd4a024c31932b051e0bedcd1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74c3b513b19d40a2b416744f36a286c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"db9ca45dd95c4a56abdbf5a9fba196e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"076de881f1844da185a98e8617705621":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3c158b31ce274409bef1ee48cffd8920":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84a5315b0b3f411db6b8b0bf699a9ec9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e6b09d78489e490f96ec867196ee6531":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ccdfe0bda29491fbe513adc3d14aaa0","IPY_MODEL_11630a60f95c4d84aba14c8a2c64a9ee","IPY_MODEL_c051d5711bf74df0a1d20051cb10e9f4"],"layout":"IPY_MODEL_80976f0119424d9486ee9eb8c5616727"}},"4ccdfe0bda29491fbe513adc3d14aaa0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0b44b22481a4553821fd0efe9601301","placeholder":"​","style":"IPY_MODEL_40aaf9b02f9d4c518898fe873330f647","value":"train:  37%"}},"11630a60f95c4d84aba14c8a2c64a9ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_16a82123cf01467599d964ee885ff753","max":294,"min":0,"orientation":"horizontal","style":"IPY_MODEL_96e4fd9d46fd4482aac77f29af0b044f","value":109}},"c051d5711bf74df0a1d20051cb10e9f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bc5722d2bba49488dc626e7dd734ed4","placeholder":"​","style":"IPY_MODEL_d5a0630464bf4863ac0b2f6d33fbc4be","value":" 109/294 [01:29&lt;02:33,  1.21batch/s, loss=0.888, accuracy=0.672]"}},"80976f0119424d9486ee9eb8c5616727":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0b44b22481a4553821fd0efe9601301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40aaf9b02f9d4c518898fe873330f647":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16a82123cf01467599d964ee885ff753":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"96e4fd9d46fd4482aac77f29af0b044f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6bc5722d2bba49488dc626e7dd734ed4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5a0630464bf4863ac0b2f6d33fbc4be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}