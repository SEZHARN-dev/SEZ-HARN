{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3juA8-pbZXvL","executionInfo":{"status":"ok","timestamp":1683080221674,"user_tz":-330,"elapsed":27468,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"0152385f-d60f-4063-b286-0156d7adc950"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B37O-QG2ZSWW","executionInfo":{"status":"ok","timestamp":1683080228114,"user_tz":-330,"elapsed":6443,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"1b336187-987b-46df-a095-f2005d03de08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkIfhXqxWhOw"},"outputs":[],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import math\n","\n","import torchinfo\n","from itertools import product\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8"]},{"cell_type":"code","source":["import torch \n","import torch.nn as nn\n","from torch.nn import functional as F\n","import numpy as np\n","\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"\n","    Basic block is composed of 2 CNN layers with residual connection.\n","    Each CNN layer is followed by batchnorm layer and swish activation \n","    function. \n","    Args:\n","        in_channel: number of input channels\n","        out_channel: number of output channels\n","        k: (default = 1) kernel size\n","    \"\"\"\n","    def __init__(self, in_channel, out_channel, k=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_channel,\n","            out_channel,\n","            kernel_size=k,\n","            padding=(0, 0),\n","            stride=(1, 1))\n","        self.bn1 = nn.BatchNorm2d(out_channel)\n","\n","        self.conv2 = nn.Conv2d(\n","            out_channel,\n","            out_channel,\n","            kernel_size=1,\n","            padding=(0, 0),\n","            stride=(1, 1))\n","        self.bn2 = nn.BatchNorm2d(out_channel)\n","\n","        self.shortcut = nn.Sequential()\n","        # if in_channel != out_channel:\n","        self.shortcut.add_module(\n","            'conv',\n","            nn.Conv2d(\n","                in_channel,\n","                out_channel,\n","                kernel_size=k,\n","                padding=(0,0),\n","                stride=(1,1)))\n","        self.shortcut.add_module('bn', nn.BatchNorm2d(out_channel))\n","\n","    def swish(self,x):\n","        \"\"\"\n","        We use swish in spatio-temporal encoding/decoding. We tried with \n","        other activation functions such as ReLU and LeakyReLU. But we \n","        achieved the best performance with swish activation function.\n","        Args:\n","            X: tensor: (batch_size, ...)\n","        Return:\n","            _: tensor: (batch, ...): applies swish \n","            activation to input tensor and returns  \n","        \"\"\"\n","        return x*torch.sigmoid(x)\n","\n","    def forward(self, x):\n","        y = self.swish(self.conv1(x))\n","        y = self.swish(self.conv2(y))\n","        y = y + self.shortcut(x)\n","        y = self.swish(y)\n","        return y\n","\n","\n","class BasicBlockTranspose(nn.Module):\n","    \"\"\"\n","    Basic block is composed of 2 CNN layers with residual connection.\n","    Each CNN layer is followed by batchnorm layer and swish activation \n","    function. \n","    Args:\n","        in_channel: number of input channels\n","        out_channel: number of output channels\n","        k: (default = 1) kernel size\n","    \"\"\"\n","    def __init__(self, in_channel, out_channel, k=(1,1)):\n","        super(BasicBlockTranspose, self).__init__()\n","        self.stride = (1, 1)\n","        self.padding = (0, 0)\n","        self.k = k\n","        self.conv1 = nn.ConvTranspose2d(\n","            in_channel,\n","            out_channel,\n","            kernel_size=k,\n","            padding=self.padding,\n","            stride=self.stride)\n","        self.bn1 = nn.BatchNorm2d(out_channel)\n","\n","        self.conv2 = nn.ConvTranspose2d(\n","            out_channel,\n","            out_channel,\n","            kernel_size=1,\n","            padding=self.padding,\n","            stride=self.stride)\n","        self.bn2 = nn.BatchNorm2d(out_channel)\n","\n","        self.shortcut = nn.Sequential()\n","        # if in_channel != out_channel:\n","        self.shortcut.add_module(\n","            'conv',\n","            nn.ConvTranspose2d(\n","                in_channel,\n","                out_channel,\n","                kernel_size=k,\n","                padding=self.padding,\n","                stride=self.stride))\n","        self.shortcut.add_module('bn', nn.BatchNorm2d(out_channel))\n","\n","    def get_h_out(self,h_in):\n","        return (h_in - 1)*self.stride[0]-2*self.padding[0]+(self.k[0]-1)+1\n","    def get_w_out(self,w_in):\n","        return (w_in - 1)*self.stride[1]-2*self.padding[1]+(self.k[1]-1)+1\n","\n","    def swish(self,x):\n","        \"\"\"\n","        We use swish in spatio-temporal encoding/decoding. We tried with \n","        other activation functions such as ReLU and LeakyReLU. But we \n","        achieved the best performance with swish activation function.\n","        Args:\n","            X: tensor: (batch_size, ...)\n","        Return:\n","            _: tensor: (batch, ...): applies swish \n","            activation to input tensor and returns  \n","        \"\"\"\n","        return x*torch.sigmoid(x)\n","\n","    def forward(self, x):\n","        y = self.swish(self.bn1(self.conv1(x)))\n","        y = self.swish(self.bn2(self.conv2(y)))\n","        y = y + self.shortcut(x)\n","        y = self.swish(y)\n","        return y\n","\n","\n","\n","class Self_Attn_Seq(nn.Module):\n","    def __init__(self,in_dim, n_head=3):\n","        super(Self_Attn_Seq,self).__init__()\n","        input_dim = in_dim\n","        self.n_head = n_head # number of attenn head\n","        self.hidden_size_attention = input_dim // self.n_head\n","        self.w_q = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        self.w_k = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        self.w_v = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        nn.init.normal_(self.w_q.weight, mean=0, std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        nn.init.normal_(self.w_k.weight, mean=0,\n","                        std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        nn.init.normal_(self.w_v.weight, mean=0,\n","                        std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        self.temperature = np.power(self.hidden_size_attention, 0.5)\n","\n","        self.softmax = nn.Softmax(dim=2)\n","        self.linear2 = nn.Linear(self.n_head * self.hidden_size_attention, input_dim)\n","        self.layer_norm = nn.LayerNorm(input_dim)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    \n","\n","    def forward(self, q):\n","        n_head = self.n_head\n","        residual = q\n","        k, v = q, q\n","        bs, len, _ = q.size()\n","        q = self.w_q(q).view(bs, len, n_head, self.hidden_size_attention)\n","        k = self.w_k(k).view(bs, len, n_head, self.hidden_size_attention)\n","        v = self.w_v(v).view(bs, len, n_head, self.hidden_size_attention)\n","\n","        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","\n","        # generate mask\n","        subsequent_mask = torch.triu(\n","            torch.ones((len, len), device=q.device, dtype=torch.uint8), diagonal=1)\n","        subsequent_mask = subsequent_mask.unsqueeze(0).expand(bs, -1, -1).gt(0)\n","        mask = subsequent_mask.repeat(n_head, 1, 1)\n","\n","        # self attention\n","        attn = torch.bmm(q, k.transpose(1, 2)) / self.temperature\n","        attn = attn.masked_fill(mask, -np.inf)\n","        attn = self.softmax(attn)\n","\n","        output = torch.bmm(attn, v)\n","        output = output.view(n_head, bs, len, self.hidden_size_attention)\n","        output = output.permute(1, 2, 0, 3).contiguous().view(bs, len, -1)\n","        output = self.gamma * self.linear2(output) + residual\n","\n","\n","        attn = attn.view(n_head,bs,len,len)\n","        attn_avg = torch.mean(attn,0)\n","        return output, attn_avg\n","\n","\n","\n","\n","\n","class DSAGGenerator(nn.Module):\n","    def __init__(self,  \n","                 seq_len, \n","                 input_size, \n","                 embedding_size:int,\n","                 temporal_decoder_filters=[4,8,14,16],\n","                 feat_size = [2,4],\n","                 internal_attention = [168]\n","                 ):\n","        super(DSAGGenerator, self).__init__()\n","        self.embedding_size = embedding_size\n","\n","        temporal_decoder_filters.append(seq_len-2)\n","        self.temporal_decoder_filters = temporal_decoder_filters\n","\n","        self.latent_dim_inner = self.embedding_size//self.temporal_decoder_filters[0]\n","\n","        self.input_size = input_size\n","        self.internal_attention = internal_attention\n","        self.feat_sizes = feat_size\n","        self.seq_len = seq_len\n","\n","        #transpose blocks\n","        self.decode_s1 = BasicBlockTranspose(self.latent_dim_inner//self.feat_sizes[0], self.latent_dim_inner//self.feat_sizes[0], k=(3,1))\n","        self.decode_s2 = BasicBlockTranspose(self.internal_attention[0]//self.feat_sizes[1], self.internal_attention[0]//self.feat_sizes[1], k=(3,1))\n","\n","        # decoder \n","        self.conv1 = BasicBlock(1,1)\n","        self.conv2 = BasicBlock(1,1)\n","        self.conv3 = BasicBlock(1,1)\n","        self.conv4 = BasicBlock(1,1)\n","        self.decode_t = BasicBlock(self.temporal_decoder_filters[0],self.temporal_decoder_filters[1])\n","        self.decode_t1 = BasicBlock(self.temporal_decoder_filters[1],self.temporal_decoder_filters[2])\n","        self.decode_t2 = BasicBlock(self.decode_s1.get_h_out(self.temporal_decoder_filters[2]),\n","                                    self.temporal_decoder_filters[3])\n","        self.decode_t3 = BasicBlock(self.temporal_decoder_filters[3],self.temporal_decoder_filters[4])\n","        self.decode_t4 = BasicBlock(self.decode_s2.get_h_out(self.temporal_decoder_filters[4]),\n","                                    self.seq_len)\n","\n","        # self attention layer\n","        self.decoder_attn1 = Self_Attn_Seq(self.latent_dim_inner)\n","        self.decoder_attn2 = Self_Attn_Seq(self.internal_attention[0])\n","        self.decoder = nn.Linear(self.latent_dim_inner, self.internal_attention[0])\n","        self.decoder1 = nn.Linear(self.internal_attention[0],self.input_size)\n","\n","        \n","        # self.decode_s3 = BasicBlockTranspose(22, 22, k=(3,1))\n","\n","\n","\n","    def forward(self, X):\n","        \"\"\"0\n","        The deocder is opposit of the encoder. It takes the vector sampled\n","        from a mixture of gaussian parameter conditioned by class label on-\n","        hot vector and viewpoint vector, upsamples it in the temporal dimension \n","        first and then upsamples it in the spatial dimension.\n","        Args:\n","            X: tensor: (batch_size, 4, ...): sampled vector conditionied on class \n","            label and viewpoint\n","        Return:\n","            x: tensor: (batch_size, 32, 48, 6): generated human motion\n","        \"\"\"\n","\n","        N = X.shape[0]\n","        X = X.reshape((N,self.temporal_decoder_filters[0],-1))\n","        N,T,J = X.shape\n","        x, attn = self.decoder_attn1(X)\n","\n","        # temporal decoding\n","        x = x.reshape((N,T,J//self.feat_sizes[0],self.feat_sizes[0]))\n","        x = self.decode_t(x)\n","        x = self.decode_t1(x)\n","\n","        # ----------------------------------------------------------------\n","        # ------------------------- newly added --------------------------\n","        # ----------------------------------------------------------------\n","\n","        x = x.transpose(2,1)\n","        x = self.decode_s1(x)\n","        x = x.transpose(2,1)\n","\n","        # ----------------------------------------------------------------\n","        # pose decoding\n","        x = x.reshape((N*self.decode_s1.get_h_out(self.temporal_decoder_filters[2]),1,J//self.feat_sizes[0],self.feat_sizes[0]))\n","        x = self.conv1(x)\n","        x = x.reshape((N,self.decode_s1.get_h_out(self.temporal_decoder_filters[2]), -1))\n","\n","        x = self.decoder(x)\n","        x, attn = self.decoder_attn2(x)\n","        # ------------------------ End of block one ---------------------\n","\n","        \n","        N,T,J = x.shape\n","        # temporal decoding\n","        x = x.reshape((N,T,J//self.feat_sizes[1], self.feat_sizes[1]))\n","        x = self.decode_t2(x)\n","        x = self.decode_t3(x)\n","\n","        # ----------------------------------------------------------------\n","        # ------------------------- Transpose block --------------------------\n","        # ----------------------------------------------------------------\n","\n","        x = x.transpose(2,1)\n","        x = self.decode_s2(x)\n","        x = x.transpose(2,1)\n","\n","        # ----------------------------------------------------------------\n","        # pose decoding\n","        x = x.reshape((N*self.seq_len,1,J//self.feat_sizes[1],self.feat_sizes[1]))\n","        x = self.conv2(x)\n","        x = x.reshape((N,self.seq_len, -1))\n","        x = self.decoder1(x)\n","        # ------------------------ End of block two ---------------------\n","\n","        return x\n","\n","class norm_data(nn.Module):\n","    def __init__(self, dim=3, joints=20):\n","        super(norm_data, self).__init__()\n","\n","        self.bn = nn.BatchNorm1d(dim*joints)\n","\n","    def forward(self, x):\n","        bs, c, num_joints, step = x.size()\n","        x = x.view(bs, -1, step)\n","        x = self.bn(x)\n","        x = x.view(bs, -1, num_joints, step).contiguous()\n","        return x\n","\n","class embed(nn.Module):\n","    def __init__(self, dim=3, joint=20, hidden_dim=128, norm=True, bias=False):\n","        super(embed, self).__init__()\n","\n","        if norm:\n","            self.cnn = nn.Sequential(\n","                norm_data(dim, joint),\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","        else:\n","            self.cnn = nn.Sequential(\n","                cnn1x1(dim, 64, bias=bias),\n","                nn.ReLU(),\n","                cnn1x1(64, hidden_dim, bias=bias),\n","                nn.ReLU(),\n","            )\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class cnn1x1(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 =3, bias = True):\n","        super(cnn1x1, self).__init__()\n","        self.cnn = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","\n","    def forward(self, x):\n","        x = self.cnn(x)\n","        return x\n","\n","class local(nn.Module):\n","    def __init__(self, dim1 = 3, dim2 = 3, bias = False):\n","        super(local, self).__init__()\n","        self.maxpool = nn.AdaptiveMaxPool2d((1, None))\n","        self.cnn1 = nn.Conv2d(dim1, dim1, kernel_size=(1, 3), padding=(0, 1), bias=bias)\n","        self.bn1 = nn.BatchNorm2d(dim1)\n","        self.relu = nn.ReLU()\n","        self.cnn2 = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n","        self.bn2 = nn.BatchNorm2d(dim2)\n","        self.dropout = nn.Dropout2d(0.2)\n","\n","    def forward(self, x1):\n","        x1 = self.maxpool(x1)\n","        x = self.cnn1(x1)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.cnn2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        return x\n","\n","class gcn_spa(nn.Module):\n","    def __init__(self, in_feature, out_feature, bias = False):\n","        super(gcn_spa, self).__init__()\n","        self.bn = nn.BatchNorm2d(out_feature)\n","        self.relu = nn.ReLU()\n","        self.w = cnn1x1(in_feature, out_feature, bias=False)\n","        self.w1 = cnn1x1(in_feature, out_feature, bias=bias)\n","\n","\n","    def forward(self, x1, g):\n","        x = x1.permute(0, 3, 2, 1).contiguous()\n","        x = g.matmul(x)\n","        x = x.permute(0, 3, 2, 1).contiguous()\n","        x = self.w(x) + self.w1(x1)\n","        x = self.relu(self.bn(x))\n","        return x\n","\n","class compute_g_spa(nn.Module):\n","    def __init__(self, dim1 = 64 *3, dim2 = 64*3, bias = False):\n","        super(compute_g_spa, self).__init__()\n","        self.dim1 = dim1\n","        self.dim2 = dim2\n","        self.g1 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.g2 = cnn1x1(self.dim1, self.dim2, bias=bias)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x1):\n","\n","        g1 = self.g1(x1).permute(0, 3, 2, 1).contiguous()\n","        g2 = self.g2(x1).permute(0, 3, 1, 2).contiguous()\n","        g3 = g1.matmul(g2)\n","        g = self.softmax(g3)\n","        return g\n","    \n","\n","class SGN(nn.Module):\n","    def __init__(self, num_joint, seg, hidden_size=128, bs=32, is_3d=True, train=True, bias=True, device='cpu'):\n","        super(SGN, self).__init__()\n","\n","        self.dim1 = hidden_size\n","        self.dim_unit = hidden_size // 4 \n","        self.seg = seg\n","        self.num_joint = num_joint\n","        self.bs = bs\n","\n","        if is_3d:\n","          self.spatial_dim = 3\n","        else:\n","          self.spatial_dim = 2\n","\n","        if train:\n","            self.spa = self.one_hot(bs, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(bs, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","        else:\n","            self.spa = self.one_hot(32 * 5, num_joint, self.seg)\n","            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n","            self.tem = self.one_hot(32 * 5, self.seg, num_joint)\n","            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n","\n","        self.tem_embed = embed(self.seg, joint=self.num_joint, hidden_dim=self.dim_unit*4, norm=False, bias=bias)\n","        self.spa_embed = embed(num_joint, joint=self.num_joint, hidden_dim=self.dim_unit, norm=False, bias=bias)\n","        self.joint_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.dif_embed = embed(self.spatial_dim, joint=self.num_joint, hidden_dim=self.dim_unit, norm=True, bias=bias)\n","        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n","        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n","        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n","        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n","        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n","        \n","        self.embed_maxpool = nn.AdaptiveMaxPool2d([self.dim1, 2])\n","\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","\n","        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n","        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n","\n","\n","    def forward(self, input):\n","        \n","        # Dynamic Representation\n","        input = input.view((self.bs, self.seg, self.num_joint, self.spatial_dim))\n","        input = input.permute(0, 3, 2, 1).contiguous()\n","        dif = input[:, :, :, 1:] - input[:, :, :, 0:-1]\n","        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n","        # print(input.shape)\n","        pos = self.joint_embed(input)\n","        tem1 = self.tem_embed(self.tem)\n","        spa1 = self.spa_embed(self.spa)\n","        dif = self.dif_embed(dif)\n","        dy = pos + dif\n","        # Joint-level Module\n","        input= torch.cat([dy, spa1], 1)\n","        g = self.compute_g1(input)\n","        input = self.gcn1(input, g)\n","        input = self.gcn2(input, g)\n","        input = self.gcn3(input, g)\n","        # Frame-level Module\n","        input = input + tem1\n","        input = self.cnn(input)\n","        output_feat = torch.squeeze(input)\n","        output_feat = self.embed_maxpool(output_feat)\n","        output_feat = torch.flatten(output_feat, 1)\n","\n","        return output_feat\n","\n","    def one_hot(self, bs, spa, tem):\n","\n","        y = torch.arange(spa).unsqueeze(-1)\n","        y_onehot = torch.FloatTensor(spa, spa)\n","\n","        y_onehot.zero_()\n","        y_onehot.scatter_(1, y, 1)\n","\n","        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n","        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n","\n","        return y_onehot\n","\n","class SGNClassifier(nn.Module):\n","  def __init__(self,num_classes,embedding_size, *args, **kwargs) -> None:\n","      super().__init__(*args, **kwargs)\n","      self.num_classes = num_classes\n","      self.embedding_size = embedding_size\n","      self.fc = nn.Linear(self.embedding_size, self.num_classes)\n","\n","  def forward(self, input):\n","      output = self.fc(input)\n","      return output\n","    \n","\n","\n","class EncDecModel(nn.Module):\n","    def __init__(self,encoder,decoder,classifier):\n","        super(EncDecModel, self).__init__()\n","        \n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.classifier = classifier\n","        \n","    def forward(self,x):\n","        embedding = self.encoder(x)\n","        classifier_out = self.classifier(embedding)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, classifier_out\n","        "],"metadata":{"id":"QNhRUvwyFUQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=128\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def get_config(file_loc,device):\n","    file = torch.load(file_loc,map_location=device)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","\n","model_params, model_config, config = get_config(\n","    f\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_NTURGB120_skeleton_SGN_DSAG_classifier_1024_emb1d/20__epoch50_emb1024_xy.pt\",\n","    device\n","    )"],"metadata":{"id":"LnVVDWf8YulU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = SGN( \n","    num_joint=config[\"model\"][\"num_joint\"], \n","    seg=config[\"model\"][\"seq_len\"], \n","    hidden_size=config[\"model\"][\"encoder_hidden_size\"], \n","    bs=batch_size, \n","    is_3d=config[\"model\"][\"is_3d\"],\n","    device = device,\n","    train=True).to(device)\n","\n","classifier = SGNClassifier(\n","    num_classes=82,\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n",").to(device)\n","\n","decoder = DSAGGenerator(\n","    seq_len = config[\"model\"][\"seq_len\"], \n","    input_size = config[\"model\"][\"input_size\"], \n","    embedding_size=config[\"model\"][\"embedding_size\"]\n",").to(device)\n","\n","bilstm_model = EncDecModel(\n","    encoder = encoder,\n","    decoder = decoder,\n","    classifier = classifier\n",").to(device)"],"metadata":{"id":"SxOl8YuW6f71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bilstm_model.load_state_dict(model_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J2XFouQeZDFW","executionInfo":{"status":"ok","timestamp":1683080921848,"user_tz":-330,"elapsed":5,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"86d18325-87b1-4a36-a8d2-4523ce8c44c0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["encoder = bilstm_model.encoder\n","decoder = bilstm_model.decoder"],"metadata":{"id":"2mfeX5hXdGZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder(torch.rand(128,60,24).to(device)).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZT-IBb4dMIX","executionInfo":{"status":"ok","timestamp":1683081478679,"user_tz":-330,"elapsed":112384,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"2473b952-eccf-4877-fda4-c3ec5ece75bb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([128, 1024])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["decoder(torch.rand(128,1024).to(device)).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O71wDyTrdfRY","executionInfo":{"status":"ok","timestamp":1683081480634,"user_tz":-330,"elapsed":1967,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"1d5a45d6-24bb-4348-9252-2ab54ead0b4a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([128, 60, 24])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["torchinfo.summary(bilstm_model, input_size=(batch_size, config[\"model\"][\"seq_len\"], config[\"model\"][\"input_size\"]), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xBdLYSVvL0f3","executionInfo":{"status":"ok","timestamp":1683081052051,"user_tz":-330,"elapsed":120191,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"1844cf00-0317-4a09-c2cb-db5886dbc41a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  action_fn=lambda data: sys.getsizeof(data.storage()),\n","/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return super().__sizeof__() + self.nbytes()\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================================================================================================\n","Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n","==========================================================================================================================================================================\n","EncDecModel                                   [128, 60, 24]             [128, 60, 24]             --                        --                        --\n","├─SGN: 1-1                                    [128, 60, 24]             [128, 1024]               --                        --                        --\n","│    └─embed: 2-1                             [128, 2, 12, 60]          [128, 128, 12, 60]        --                        --                        --\n","│    │    └─Sequential: 3-1                   [128, 2, 12, 60]          [128, 128, 12, 60]        8,560                     --                        784,472,064\n","│    └─embed: 2-2                             [128, 60, 12, 60]         [128, 512, 12, 60]        --                        --                        --\n","│    │    └─Sequential: 3-2                   [128, 60, 12, 60]         [128, 512, 12, 60]        37,184                    --                        3,426,877,440\n","│    └─embed: 2-3                             [128, 12, 12, 60]         [128, 128, 12, 60]        --                        --                        --\n","│    │    └─Sequential: 3-3                   [128, 12, 12, 60]         [128, 128, 12, 60]        9,152                     --                        843,448,320\n","│    └─embed: 2-4                             [128, 2, 12, 60]          [128, 128, 12, 60]        --                        --                        --\n","│    │    └─Sequential: 3-4                   [128, 2, 12, 60]          [128, 128, 12, 60]        8,560                     --                        784,472,064\n","│    └─compute_g_spa: 2-5                     [128, 256, 12, 60]        [128, 60, 12, 12]         --                        --                        --\n","│    │    └─cnn1x1: 3-5                       [128, 256, 12, 60]        [128, 512, 12, 60]        131,584                   --                        12,126,781,440\n","│    │    └─cnn1x1: 3-6                       [128, 256, 12, 60]        [128, 512, 12, 60]        131,584                   --                        12,126,781,440\n","│    │    └─Softmax: 3-7                      [128, 60, 12, 12]         [128, 60, 12, 12]         --                        --                        --\n","│    └─gcn_spa: 2-6                           [128, 256, 12, 60]        [128, 256, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-8                       [128, 256, 12, 60]        [128, 256, 12, 60]        65,536                    --                        6,039,797,760\n","│    │    └─cnn1x1: 3-9                       [128, 256, 12, 60]        [128, 256, 12, 60]        65,792                    --                        6,063,390,720\n","│    │    └─BatchNorm2d: 3-10                 [128, 256, 12, 60]        [128, 256, 12, 60]        512                       --                        65,536\n","│    │    └─ReLU: 3-11                        [128, 256, 12, 60]        [128, 256, 12, 60]        --                        --                        --\n","│    └─gcn_spa: 2-7                           [128, 256, 12, 60]        [128, 512, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-12                      [128, 256, 12, 60]        [128, 512, 12, 60]        131,072                   --                        12,079,595,520\n","│    │    └─cnn1x1: 3-13                      [128, 256, 12, 60]        [128, 512, 12, 60]        131,584                   --                        12,126,781,440\n","│    │    └─BatchNorm2d: 3-14                 [128, 512, 12, 60]        [128, 512, 12, 60]        1,024                     --                        131,072\n","│    │    └─ReLU: 3-15                        [128, 512, 12, 60]        [128, 512, 12, 60]        --                        --                        --\n","│    └─gcn_spa: 2-8                           [128, 512, 12, 60]        [128, 512, 12, 60]        --                        --                        --\n","│    │    └─cnn1x1: 3-16                      [128, 512, 12, 60]        [128, 512, 12, 60]        262,144                   --                        24,159,191,040\n","│    │    └─cnn1x1: 3-17                      [128, 512, 12, 60]        [128, 512, 12, 60]        262,656                   --                        24,206,376,960\n","│    │    └─BatchNorm2d: 3-18                 [128, 512, 12, 60]        [128, 512, 12, 60]        1,024                     --                        131,072\n","│    │    └─ReLU: 3-19                        [128, 512, 12, 60]        [128, 512, 12, 60]        --                        --                        --\n","│    └─local: 2-9                             [128, 512, 12, 60]        [128, 1024, 1, 60]        --                        --                        --\n","│    │    └─AdaptiveMaxPool2d: 3-20           [128, 512, 12, 60]        [128, 512, 1, 60]         --                        --                        --\n","│    │    └─Conv2d: 3-21                      [128, 512, 1, 60]         [128, 512, 1, 60]         786,944                   [1, 3]                    6,043,729,920\n","│    │    └─BatchNorm2d: 3-22                 [128, 512, 1, 60]         [128, 512, 1, 60]         1,024                     --                        131,072\n","│    │    └─ReLU: 3-23                        [128, 512, 1, 60]         [128, 512, 1, 60]         --                        --                        --\n","│    │    └─Dropout2d: 3-24                   [128, 512, 1, 60]         [128, 512, 1, 60]         --                        --                        --\n","│    │    └─Conv2d: 3-25                      [128, 512, 1, 60]         [128, 1024, 1, 60]        525,312                   [1, 1]                    4,034,396,160\n","│    │    └─BatchNorm2d: 3-26                 [128, 1024, 1, 60]        [128, 1024, 1, 60]        2,048                     --                        262,144\n","│    │    └─ReLU: 3-27                        [128, 1024, 1, 60]        [128, 1024, 1, 60]        --                        --                        --\n","│    └─AdaptiveMaxPool2d: 2-10                [128, 1024, 60]           [128, 512, 2]             --                        --                        --\n","├─SGNClassifier: 1-2                          [128, 1024]               [128, 82]                 --                        --                        --\n","│    └─Linear: 2-11                           [128, 1024]               [128, 82]                 84,050                    --                        10,758,400\n","├─DSAGGenerator: 1-3                          [128, 1024]               [128, 60, 24]             11,364                    --                        --\n","│    └─Self_Attn_Seq: 2-12                    [128, 4, 256]             [128, 4, 256]             513                       --                        --\n","│    │    └─Linear: 3-28                      [128, 4, 256]             [128, 4, 255]             65,535                    --                        8,388,480\n","│    │    └─Linear: 3-29                      [128, 4, 256]             [128, 4, 255]             65,535                    --                        8,388,480\n","│    │    └─Linear: 3-30                      [128, 4, 256]             [128, 4, 255]             65,535                    --                        8,388,480\n","│    │    └─Softmax: 3-31                     [384, 4, 4]               [384, 4, 4]               --                        --                        --\n","│    │    └─Linear: 3-32                      [128, 4, 255]             [128, 4, 256]             65,536                    --                        8,388,608\n","│    └─BasicBlock: 2-13                       [128, 4, 128, 2]          [128, 8, 128, 2]          32                        --                        --\n","│    │    └─Conv2d: 3-33                      [128, 4, 128, 2]          [128, 8, 128, 2]          40                        [1, 1]                    1,310,720\n","│    │    └─Conv2d: 3-34                      [128, 8, 128, 2]          [128, 8, 128, 2]          72                        [1, 1]                    2,359,296\n","│    │    └─Sequential: 3-35                  [128, 4, 128, 2]          [128, 8, 128, 2]          56                        --                        1,312,768\n","│    └─BasicBlock: 2-14                       [128, 8, 128, 2]          [128, 14, 128, 2]         56                        --                        --\n","│    │    └─Conv2d: 3-36                      [128, 8, 128, 2]          [128, 14, 128, 2]         126                       [1, 1]                    4,128,768\n","│    │    └─Conv2d: 3-37                      [128, 14, 128, 2]         [128, 14, 128, 2]         210                       [1, 1]                    6,881,280\n","│    │    └─Sequential: 3-38                  [128, 8, 128, 2]          [128, 14, 128, 2]         154                       --                        4,132,352\n","│    └─BasicBlockTranspose: 2-15              [128, 128, 14, 2]         [128, 128, 16, 2]         --                        --                        --\n","│    │    └─ConvTranspose2d: 3-39             [128, 128, 14, 2]         [128, 128, 16, 2]         49,280                    [3, 1]                    201,850,880\n","│    │    └─BatchNorm2d: 3-40                 [128, 128, 16, 2]         [128, 128, 16, 2]         256                       --                        32,768\n","│    │    └─ConvTranspose2d: 3-41             [128, 128, 16, 2]         [128, 128, 16, 2]         16,512                    [1, 1]                    67,633,152\n","│    │    └─BatchNorm2d: 3-42                 [128, 128, 16, 2]         [128, 128, 16, 2]         256                       --                        32,768\n","│    │    └─Sequential: 3-43                  [128, 128, 14, 2]         [128, 128, 16, 2]         49,536                    --                        201,883,648\n","│    └─BasicBlock: 2-16                       [2048, 1, 128, 2]         [2048, 1, 128, 2]         4                         --                        --\n","│    │    └─Conv2d: 3-44                      [2048, 1, 128, 2]         [2048, 1, 128, 2]         2                         [1, 1]                    1,048,576\n","│    │    └─Conv2d: 3-45                      [2048, 1, 128, 2]         [2048, 1, 128, 2]         2                         [1, 1]                    1,048,576\n","│    │    └─Sequential: 3-46                  [2048, 1, 128, 2]         [2048, 1, 128, 2]         4                         --                        1,052,672\n","│    └─Linear: 2-17                           [128, 16, 256]            [128, 16, 168]            43,176                    --                        5,526,528\n","│    └─Self_Attn_Seq: 2-18                    [128, 16, 168]            [128, 16, 168]            337                       --                        --\n","│    │    └─Linear: 3-47                      [128, 16, 168]            [128, 16, 168]            28,392                    --                        3,634,176\n","│    │    └─Linear: 3-48                      [128, 16, 168]            [128, 16, 168]            28,392                    --                        3,634,176\n","│    │    └─Linear: 3-49                      [128, 16, 168]            [128, 16, 168]            28,392                    --                        3,634,176\n","│    │    └─Softmax: 3-50                     [384, 16, 16]             [384, 16, 16]             --                        --                        --\n","│    │    └─Linear: 3-51                      [128, 16, 168]            [128, 16, 168]            28,392                    --                        3,634,176\n","│    └─BasicBlock: 2-19                       [128, 16, 42, 4]          [128, 16, 42, 4]          64                        --                        --\n","│    │    └─Conv2d: 3-52                      [128, 16, 42, 4]          [128, 16, 42, 4]          272                       [1, 1]                    5,849,088\n","│    │    └─Conv2d: 3-53                      [128, 16, 42, 4]          [128, 16, 42, 4]          272                       [1, 1]                    5,849,088\n","│    │    └─Sequential: 3-54                  [128, 16, 42, 4]          [128, 16, 42, 4]          304                       --                        5,853,184\n","│    └─BasicBlock: 2-20                       [128, 16, 42, 4]          [128, 58, 42, 4]          232                       --                        --\n","│    │    └─Conv2d: 3-55                      [128, 16, 42, 4]          [128, 58, 42, 4]          986                       [1, 1]                    21,202,944\n","│    │    └─Conv2d: 3-56                      [128, 58, 42, 4]          [128, 58, 42, 4]          3,422                     [1, 1]                    73,586,688\n","│    │    └─Sequential: 3-57                  [128, 16, 42, 4]          [128, 58, 42, 4]          1,102                     --                        21,217,792\n","│    └─BasicBlockTranspose: 2-21              [128, 42, 58, 4]          [128, 42, 60, 4]          --                        --                        --\n","│    │    └─ConvTranspose2d: 3-58             [128, 42, 58, 4]          [128, 42, 60, 4]          5,334                     [3, 1]                    163,860,480\n","│    │    └─BatchNorm2d: 3-59                 [128, 42, 60, 4]          [128, 42, 60, 4]          84                        --                        10,752\n","│    │    └─ConvTranspose2d: 3-60             [128, 42, 60, 4]          [128, 42, 60, 4]          1,806                     [1, 1]                    55,480,320\n","│    │    └─BatchNorm2d: 3-61                 [128, 42, 60, 4]          [128, 42, 60, 4]          84                        --                        10,752\n","│    │    └─Sequential: 3-62                  [128, 42, 58, 4]          [128, 42, 60, 4]          5,418                     --                        163,871,232\n","│    └─BasicBlock: 2-22                       [7680, 1, 42, 4]          [7680, 1, 42, 4]          4                         --                        --\n","│    │    └─Conv2d: 3-63                      [7680, 1, 42, 4]          [7680, 1, 42, 4]          2                         [1, 1]                    2,580,480\n","│    │    └─Conv2d: 3-64                      [7680, 1, 42, 4]          [7680, 1, 42, 4]          2                         [1, 1]                    2,580,480\n","│    │    └─Sequential: 3-65                  [7680, 1, 42, 4]          [7680, 1, 42, 4]          4                         --                        2,595,840\n","│    └─Linear: 2-23                           [128, 60, 168]            [128, 60, 24]             4,056                     --                        519,168\n","==========================================================================================================================================================================\n","Total params: 3,218,491\n","Trainable params: 3,218,491\n","Non-trainable params: 0\n","Total mult-adds (G): 125.93\n","==========================================================================================================================================================================\n","Input size (MB): 0.74\n","Forward/backward pass size (MB): 4865.82\n","Params size (MB): 12.82\n","Estimated Total Size (MB): 4879.39\n","=========================================================================================================================================================================="]},"metadata":{},"execution_count":24}]}]}