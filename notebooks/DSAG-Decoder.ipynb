{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4rwsSw5ggk1k","executionInfo":{"status":"ok","timestamp":1683034561032,"user_tz":-330,"elapsed":4545,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"f2a38fa1-40c3-49e9-8808-2d6353e2d608"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2FuMXB_0KeyH","executionInfo":{"status":"ok","timestamp":1683034569753,"user_tz":-330,"elapsed":8724,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"3aa4c12b-99d4-4dac-db1e-d32667ac7b2e"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.7.2)\n"]}]},{"cell_type":"code","execution_count":20,"metadata":{"id":"BkIfhXqxWhOw","executionInfo":{"status":"ok","timestamp":1683034569755,"user_tz":-330,"elapsed":9,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"outputs":[],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import sys\n","import torchinfo\n","\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.notebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","from itertools import product\n","import matplotlib.pyplot as plt\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","import argparse"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"p0XIAvcQX3l0","executionInfo":{"status":"ok","timestamp":1683034569756,"user_tz":-330,"elapsed":9,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id\n","\n","def trunc(latent, mean_size, truncation):  # Truncation trick on Z\n","    t = Variable(FloatTensor(np.random.normal(0, 1, (mean_size, *latent.shape[1:]))))\n","    m = t.mean(0, keepdim=True)\n","\n","    for i,_ in enumerate(latent):\n","        latent[i] = m + truncation*(latent[i] - m)\n","\n","    return latent"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"BzKv-uPwf8hA","executionInfo":{"status":"ok","timestamp":1683034569756,"user_tz":-330,"elapsed":9,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","source":["config = {\n","    \"batch_size\":10,\n","    \"latent_dim\":512,\n","    \"mlp_dim\":8,\n","    \"n_classes\":120,\n","    \"label\":-1,\n","    \"t_size\":64,\n","    \"v_size\":25,\n","    \"channels\":3,\n","    \"dataset\":\"ntu\",\n","    \"model\":\"/content/model_saves/generator_ntu120_xsub_mlp8_2150000.pth\",\n","    \"stochastic\":False,\n","    \"stochastic_file\":\"-\",\n","    \"stochastic_index\":0,\n","    \"gen_qtd\":10,\n","    \"trunc\":0.95,\n","    \"trunc_mode\":'w',\n","    \"mean_size\":1000\n","}\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"LhLJ-nlnekJz","executionInfo":{"status":"ok","timestamp":1683034569757,"user_tz":-330,"elapsed":10,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import torch \n","import torch.nn as nn\n","from torch.nn import functional as F\n","import numpy as np\n","\n","\n","\n","\n","class BasicBlock(nn.Module):\n","    \"\"\"\n","    Basic block is composed of 2 CNN layers with residual connection.\n","    Each CNN layer is followed by batchnorm layer and swish activation \n","    function. \n","    Args:\n","        in_channel: number of input channels\n","        out_channel: number of output channels\n","        k: (default = 1) kernel size\n","    \"\"\"\n","    def __init__(self, in_channel, out_channel, k=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_channel,\n","            out_channel,\n","            kernel_size=k,\n","            padding=(0, 0),\n","            stride=(1, 1))\n","        self.bn1 = nn.BatchNorm2d(out_channel)\n","\n","        self.conv2 = nn.Conv2d(\n","            out_channel,\n","            out_channel,\n","            kernel_size=1,\n","            padding=(0, 0),\n","            stride=(1, 1))\n","        self.bn2 = nn.BatchNorm2d(out_channel)\n","\n","        self.shortcut = nn.Sequential()\n","        # if in_channel != out_channel:\n","        self.shortcut.add_module(\n","            'conv',\n","            nn.Conv2d(\n","                in_channel,\n","                out_channel,\n","                kernel_size=k,\n","                padding=(0,0),\n","                stride=(1,1)))\n","        self.shortcut.add_module('bn', nn.BatchNorm2d(out_channel))\n","\n","    def swish(self,x):\n","        \"\"\"\n","        We use swish in spatio-temporal encoding/decoding. We tried with \n","        other activation functions such as ReLU and LeakyReLU. But we \n","        achieved the best performance with swish activation function.\n","        Args:\n","            X: tensor: (batch_size, ...)\n","        Return:\n","            _: tensor: (batch, ...): applies swish \n","            activation to input tensor and returns  \n","        \"\"\"\n","        return x*torch.sigmoid(x)\n","\n","    def forward(self, x):\n","        y = self.swish(self.conv1(x))\n","        y = self.swish(self.conv2(y))\n","        y = y + self.shortcut(x)\n","        y = self.swish(y)\n","        return y\n","\n","\n","class BasicBlockTranspose(nn.Module):\n","    \"\"\"\n","    Basic block is composed of 2 CNN layers with residual connection.\n","    Each CNN layer is followed by batchnorm layer and swish activation \n","    function. \n","    Args:\n","        in_channel: number of input channels\n","        out_channel: number of output channels\n","        k: (default = 1) kernel size\n","    \"\"\"\n","    def __init__(self, in_channel, out_channel, k=(1,1)):\n","        super(BasicBlockTranspose, self).__init__()\n","        self.stride = (1, 1)\n","        self.padding = (0, 0)\n","        self.k = k\n","        self.conv1 = nn.ConvTranspose2d(\n","            in_channel,\n","            out_channel,\n","            kernel_size=k,\n","            padding=self.padding,\n","            stride=self.stride)\n","        self.bn1 = nn.BatchNorm2d(out_channel)\n","\n","        self.conv2 = nn.ConvTranspose2d(\n","            out_channel,\n","            out_channel,\n","            kernel_size=1,\n","            padding=self.padding,\n","            stride=self.stride)\n","        self.bn2 = nn.BatchNorm2d(out_channel)\n","\n","        self.shortcut = nn.Sequential()\n","        # if in_channel != out_channel:\n","        self.shortcut.add_module(\n","            'conv',\n","            nn.ConvTranspose2d(\n","                in_channel,\n","                out_channel,\n","                kernel_size=k,\n","                padding=self.padding,\n","                stride=self.stride))\n","        self.shortcut.add_module('bn', nn.BatchNorm2d(out_channel))\n","\n","    def get_h_out(self,h_in):\n","        return (h_in - 1)*self.stride[0]-2*self.padding[0]+(self.k[0]-1)+1\n","    def get_w_out(self,w_in):\n","        return (w_in - 1)*self.stride[1]-2*self.padding[1]+(self.k[1]-1)+1\n","\n","    def swish(self,x):\n","        \"\"\"\n","        We use swish in spatio-temporal encoding/decoding. We tried with \n","        other activation functions such as ReLU and LeakyReLU. But we \n","        achieved the best performance with swish activation function.\n","        Args:\n","            X: tensor: (batch_size, ...)\n","        Return:\n","            _: tensor: (batch, ...): applies swish \n","            activation to input tensor and returns  \n","        \"\"\"\n","        return x*torch.sigmoid(x)\n","\n","    def forward(self, x):\n","        y = self.swish(self.bn1(self.conv1(x)))\n","        y = self.swish(self.bn2(self.conv2(y)))\n","        y = y + self.shortcut(x)\n","        y = self.swish(y)\n","        return y\n","\n","\n","\n","class Self_Attn_Seq(nn.Module):\n","    def __init__(self,in_dim, n_head=3):\n","        super(Self_Attn_Seq,self).__init__()\n","        input_dim = in_dim\n","        self.n_head = n_head # number of attenn head\n","        self.hidden_size_attention = input_dim // self.n_head\n","        self.w_q = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        self.w_k = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        self.w_v = nn.Linear(input_dim, self.n_head * self.hidden_size_attention)\n","        nn.init.normal_(self.w_q.weight, mean=0, std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        nn.init.normal_(self.w_k.weight, mean=0,\n","                        std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        nn.init.normal_(self.w_v.weight, mean=0,\n","                        std=np.sqrt(2.0 / (input_dim + self.hidden_size_attention)))\n","        self.temperature = np.power(self.hidden_size_attention, 0.5)\n","\n","        self.softmax = nn.Softmax(dim=2)\n","        self.linear2 = nn.Linear(self.n_head * self.hidden_size_attention, input_dim)\n","        self.layer_norm = nn.LayerNorm(input_dim)\n","        self.gamma = nn.Parameter(torch.zeros(1))\n","    \n","\n","    def forward(self, q):\n","        n_head = self.n_head\n","        residual = q\n","        k, v = q, q\n","        bs, len, _ = q.size()\n","        q = self.w_q(q).view(bs, len, n_head, self.hidden_size_attention)\n","        k = self.w_k(k).view(bs, len, n_head, self.hidden_size_attention)\n","        v = self.w_v(v).view(bs, len, n_head, self.hidden_size_attention)\n","\n","        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len, self.hidden_size_attention)\n","\n","        # generate mask\n","        subsequent_mask = torch.triu(\n","            torch.ones((len, len), device=q.device, dtype=torch.uint8), diagonal=1)\n","        subsequent_mask = subsequent_mask.unsqueeze(0).expand(bs, -1, -1).gt(0)\n","        mask = subsequent_mask.repeat(n_head, 1, 1)\n","\n","        # self attention\n","        attn = torch.bmm(q, k.transpose(1, 2)) / self.temperature\n","        attn = attn.masked_fill(mask, -np.inf)\n","        attn = self.softmax(attn)\n","\n","        output = torch.bmm(attn, v)\n","        output = output.view(n_head, bs, len, self.hidden_size_attention)\n","        output = output.permute(1, 2, 0, 3).contiguous().view(bs, len, -1)\n","        output = self.gamma * self.linear2(output) + residual\n","\n","\n","        attn = attn.view(n_head,bs,len,len)\n","        attn_avg = torch.mean(attn,0)\n","        return output, attn_avg\n","\n","\n","\n","\n","\n","class Generator(nn.Module):\n","    def __init__(self,  \n","                 seq_len, \n","                 input_size, \n","                 embedding_size:int,\n","                 temporal_decoder_filters=[4,8,14,16],\n","                 feat_size = [2,4],\n","                 internal_attention = [168]\n","                 ):\n","        super(Generator, self).__init__()\n","        self.embedding_size = embedding_size\n","\n","        temporal_decoder_filters.append(seq_len-2)\n","        self.temporal_decoder_filters = temporal_decoder_filters\n","\n","        self.latent_dim_inner = self.embedding_size//self.temporal_decoder_filters[0]\n","\n","        self.input_size = input_size\n","        self.internal_attention = internal_attention\n","        self.feat_sizes = feat_size\n","        self.seq_len = seq_len\n","\n","        #transpose blocks\n","        self.decode_s1 = BasicBlockTranspose(self.latent_dim_inner//self.feat_sizes[0], self.latent_dim_inner//self.feat_sizes[0], k=(3,1))\n","        self.decode_s2 = BasicBlockTranspose(self.internal_attention[0]//self.feat_sizes[1], self.internal_attention[0]//self.feat_sizes[1], k=(3,1))\n","\n","        # decoder \n","        self.conv1 = BasicBlock(1,1)\n","        self.conv2 = BasicBlock(1,1)\n","        self.conv3 = BasicBlock(1,1)\n","        self.conv4 = BasicBlock(1,1)\n","        self.decode_t = BasicBlock(self.temporal_decoder_filters[0],self.temporal_decoder_filters[1])\n","        self.decode_t1 = BasicBlock(self.temporal_decoder_filters[1],self.temporal_decoder_filters[2])\n","        self.decode_t2 = BasicBlock(self.decode_s1.get_h_out(self.temporal_decoder_filters[2]),\n","                                    self.temporal_decoder_filters[3])\n","        self.decode_t3 = BasicBlock(self.temporal_decoder_filters[3],self.temporal_decoder_filters[4])\n","        self.decode_t4 = BasicBlock(self.decode_s2.get_h_out(self.temporal_decoder_filters[4]),\n","                                    self.seq_len)\n","\n","        # self attention layer\n","        self.decoder_attn1 = Self_Attn_Seq(self.latent_dim_inner)\n","        self.decoder_attn2 = Self_Attn_Seq(self.internal_attention[0])\n","        self.decoder = nn.Linear(self.latent_dim_inner, self.internal_attention[0])\n","        self.decoder1 = nn.Linear(self.internal_attention[0],self.input_size)\n","\n","        \n","        # self.decode_s3 = BasicBlockTranspose(22, 22, k=(3,1))\n","\n","\n","\n","    def forward(self, X):\n","        \"\"\"0\n","        The deocder is opposit of the encoder. It takes the vector sampled\n","        from a mixture of gaussian parameter conditioned by class label on-\n","        hot vector and viewpoint vector, upsamples it in the temporal dimension \n","        first and then upsamples it in the spatial dimension.\n","        Args:\n","            X: tensor: (batch_size, 4, ...): sampled vector conditionied on class \n","            label and viewpoint\n","        Return:\n","            x: tensor: (batch_size, 32, 48, 6): generated human motion\n","        \"\"\"\n","\n","        N = X.shape[0]\n","        X = X.reshape((N,self.temporal_decoder_filters[0],-1))\n","        N,T,J = X.shape\n","        x, attn = self.decoder_attn1(X)\n","\n","        # temporal decoding\n","        x = x.reshape((N,T,J//self.feat_sizes[0],self.feat_sizes[0]))\n","        x = self.decode_t(x)\n","        x = self.decode_t1(x)\n","\n","        # ----------------------------------------------------------------\n","        # ------------------------- newly added --------------------------\n","        # ----------------------------------------------------------------\n","\n","        x = x.transpose(2,1)\n","        x = self.decode_s1(x)\n","        x = x.transpose(2,1)\n","\n","        # ----------------------------------------------------------------\n","        # pose decoding\n","        x = x.reshape((N*self.decode_s1.get_h_out(self.temporal_decoder_filters[2]),1,J//self.feat_sizes[0],self.feat_sizes[0]))\n","        x = self.conv1(x)\n","        x = x.reshape((N,self.decode_s1.get_h_out(self.temporal_decoder_filters[2]), -1))\n","\n","        x = self.decoder(x)\n","        x, attn = self.decoder_attn2(x)\n","        # ------------------------ End of block one ---------------------\n","\n","        \n","        N,T,J = x.shape\n","        # temporal decoding\n","        x = x.reshape((N,T,J//self.feat_sizes[1], self.feat_sizes[1]))\n","        x = self.decode_t2(x)\n","        x = self.decode_t3(x)\n","\n","        # ----------------------------------------------------------------\n","        # ------------------------- Transpose block --------------------------\n","        # ----------------------------------------------------------------\n","\n","        x = x.transpose(2,1)\n","        x = self.decode_s2(x)\n","        x = x.transpose(2,1)\n","\n","        # ----------------------------------------------------------------\n","        # pose decoding\n","        x = x.reshape((N*self.seq_len,1,J//self.feat_sizes[1],self.feat_sizes[1]))\n","        x = self.conv2(x)\n","        x = x.reshape((N,self.seq_len, -1))\n","        x = self.decoder1(x)\n","        # ------------------------ End of block two ---------------------\n","\n","        return x"],"metadata":{"id":"jDnvVOZehevQ","executionInfo":{"status":"ok","timestamp":1683042908054,"user_tz":-330,"elapsed":495,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"execution_count":244,"outputs":[]},{"cell_type":"code","source":["generator     = Generator(  \n","                 seq_len = 65, \n","                 input_size = 24, \n","                 embedding_size=1024\n","                  ).to(device)"],"metadata":{"id":"2x0GsUW-IpY9","executionInfo":{"status":"ok","timestamp":1683042929971,"user_tz":-330,"elapsed":348,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}}},"execution_count":245,"outputs":[]},{"cell_type":"code","source":["generator(torch.rand((16,1024)).to(device)).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4126lwfItAm","executionInfo":{"status":"ok","timestamp":1683042931382,"user_tz":-330,"elapsed":2,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"9c4209f9-7df2-4f0f-9869-1a127d3bf02c"},"execution_count":246,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([16, 65, 24])"]},"metadata":{},"execution_count":246}]},{"cell_type":"code","source":["torchinfo.summary(generator, input_size=(32,1024), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLu4BFHlIyZD","executionInfo":{"status":"ok","timestamp":1683042936343,"user_tz":-330,"elapsed":384,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"}},"outputId":"9e8f40dc-abe1-49f2-a129-dd91b9f3986a"},"execution_count":247,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  action_fn=lambda data: sys.getsizeof(data.storage()),\n","/usr/local/lib/python3.10/dist-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return super().__sizeof__() + self.nbytes()\n"]},{"output_type":"execute_result","data":{"text/plain":["=====================================================================================================================================================================\n","Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n","=====================================================================================================================================================================\n","Generator                                [32, 1024]                [32, 65, 24]              13,284                    --                        --\n","├─Self_Attn_Seq: 1-1                     [32, 4, 256]              [32, 4, 256]              513                       --                        --\n","│    └─Linear: 2-1                       [32, 4, 256]              [32, 4, 255]              65,535                    --                        2,097,120\n","│    └─Linear: 2-2                       [32, 4, 256]              [32, 4, 255]              65,535                    --                        2,097,120\n","│    └─Linear: 2-3                       [32, 4, 256]              [32, 4, 255]              65,535                    --                        2,097,120\n","│    └─Softmax: 2-4                      [96, 4, 4]                [96, 4, 4]                --                        --                        --\n","│    └─Linear: 2-5                       [32, 4, 255]              [32, 4, 256]              65,536                    --                        2,097,152\n","├─BasicBlock: 1-2                        [32, 4, 128, 2]           [32, 8, 128, 2]           32                        --                        --\n","│    └─Conv2d: 2-6                       [32, 4, 128, 2]           [32, 8, 128, 2]           40                        [1, 1]                    327,680\n","│    └─Conv2d: 2-7                       [32, 8, 128, 2]           [32, 8, 128, 2]           72                        [1, 1]                    589,824\n","│    └─Sequential: 2-8                   [32, 4, 128, 2]           [32, 8, 128, 2]           --                        --                        --\n","│    │    └─Conv2d: 3-1                  [32, 4, 128, 2]           [32, 8, 128, 2]           40                        [1, 1]                    327,680\n","│    │    └─BatchNorm2d: 3-2             [32, 8, 128, 2]           [32, 8, 128, 2]           16                        --                        512\n","├─BasicBlock: 1-3                        [32, 8, 128, 2]           [32, 14, 128, 2]          56                        --                        --\n","│    └─Conv2d: 2-9                       [32, 8, 128, 2]           [32, 14, 128, 2]          126                       [1, 1]                    1,032,192\n","│    └─Conv2d: 2-10                      [32, 14, 128, 2]          [32, 14, 128, 2]          210                       [1, 1]                    1,720,320\n","│    └─Sequential: 2-11                  [32, 8, 128, 2]           [32, 14, 128, 2]          --                        --                        --\n","│    │    └─Conv2d: 3-3                  [32, 8, 128, 2]           [32, 14, 128, 2]          126                       [1, 1]                    1,032,192\n","│    │    └─BatchNorm2d: 3-4             [32, 14, 128, 2]          [32, 14, 128, 2]          28                        --                        896\n","├─BasicBlockTranspose: 1-4               [32, 128, 14, 2]          [32, 128, 16, 2]          --                        --                        --\n","│    └─ConvTranspose2d: 2-12             [32, 128, 14, 2]          [32, 128, 16, 2]          49,280                    [3, 1]                    50,462,720\n","│    └─BatchNorm2d: 2-13                 [32, 128, 16, 2]          [32, 128, 16, 2]          256                       --                        8,192\n","│    └─ConvTranspose2d: 2-14             [32, 128, 16, 2]          [32, 128, 16, 2]          16,512                    [1, 1]                    16,908,288\n","│    └─BatchNorm2d: 2-15                 [32, 128, 16, 2]          [32, 128, 16, 2]          256                       --                        8,192\n","│    └─Sequential: 2-16                  [32, 128, 14, 2]          [32, 128, 16, 2]          --                        --                        --\n","│    │    └─ConvTranspose2d: 3-5         [32, 128, 14, 2]          [32, 128, 16, 2]          49,280                    [3, 1]                    50,462,720\n","│    │    └─BatchNorm2d: 3-6             [32, 128, 16, 2]          [32, 128, 16, 2]          256                       --                        8,192\n","├─BasicBlock: 1-5                        [512, 1, 128, 2]          [512, 1, 128, 2]          4                         --                        --\n","│    └─Conv2d: 2-17                      [512, 1, 128, 2]          [512, 1, 128, 2]          2                         [1, 1]                    262,144\n","│    └─Conv2d: 2-18                      [512, 1, 128, 2]          [512, 1, 128, 2]          2                         [1, 1]                    262,144\n","│    └─Sequential: 2-19                  [512, 1, 128, 2]          [512, 1, 128, 2]          --                        --                        --\n","│    │    └─Conv2d: 3-7                  [512, 1, 128, 2]          [512, 1, 128, 2]          2                         [1, 1]                    262,144\n","│    │    └─BatchNorm2d: 3-8             [512, 1, 128, 2]          [512, 1, 128, 2]          2                         --                        1,024\n","├─Linear: 1-6                            [32, 16, 256]             [32, 16, 168]             43,176                    --                        1,381,632\n","├─Self_Attn_Seq: 1-7                     [32, 16, 168]             [32, 16, 168]             337                       --                        --\n","│    └─Linear: 2-20                      [32, 16, 168]             [32, 16, 168]             28,392                    --                        908,544\n","│    └─Linear: 2-21                      [32, 16, 168]             [32, 16, 168]             28,392                    --                        908,544\n","│    └─Linear: 2-22                      [32, 16, 168]             [32, 16, 168]             28,392                    --                        908,544\n","│    └─Softmax: 2-23                     [96, 16, 16]              [96, 16, 16]              --                        --                        --\n","│    └─Linear: 2-24                      [32, 16, 168]             [32, 16, 168]             28,392                    --                        908,544\n","├─BasicBlock: 1-8                        [32, 16, 42, 4]           [32, 16, 42, 4]           64                        --                        --\n","│    └─Conv2d: 2-25                      [32, 16, 42, 4]           [32, 16, 42, 4]           272                       [1, 1]                    1,462,272\n","│    └─Conv2d: 2-26                      [32, 16, 42, 4]           [32, 16, 42, 4]           272                       [1, 1]                    1,462,272\n","│    └─Sequential: 2-27                  [32, 16, 42, 4]           [32, 16, 42, 4]           --                        --                        --\n","│    │    └─Conv2d: 3-9                  [32, 16, 42, 4]           [32, 16, 42, 4]           272                       [1, 1]                    1,462,272\n","│    │    └─BatchNorm2d: 3-10            [32, 16, 42, 4]           [32, 16, 42, 4]           32                        --                        1,024\n","├─BasicBlock: 1-9                        [32, 16, 42, 4]           [32, 63, 42, 4]           252                       --                        --\n","│    └─Conv2d: 2-28                      [32, 16, 42, 4]           [32, 63, 42, 4]           1,071                     [1, 1]                    5,757,696\n","│    └─Conv2d: 2-29                      [32, 63, 42, 4]           [32, 63, 42, 4]           4,032                     [1, 1]                    21,676,032\n","│    └─Sequential: 2-30                  [32, 16, 42, 4]           [32, 63, 42, 4]           --                        --                        --\n","│    │    └─Conv2d: 3-11                 [32, 16, 42, 4]           [32, 63, 42, 4]           1,071                     [1, 1]                    5,757,696\n","│    │    └─BatchNorm2d: 3-12            [32, 63, 42, 4]           [32, 63, 42, 4]           126                       --                        4,032\n","├─BasicBlockTranspose: 1-10              [32, 42, 63, 4]           [32, 42, 65, 4]           --                        --                        --\n","│    └─ConvTranspose2d: 2-31             [32, 42, 63, 4]           [32, 42, 65, 4]           5,334                     [3, 1]                    44,378,880\n","│    └─BatchNorm2d: 2-32                 [32, 42, 65, 4]           [32, 42, 65, 4]           84                        --                        2,688\n","│    └─ConvTranspose2d: 2-33             [32, 42, 65, 4]           [32, 42, 65, 4]           1,806                     [1, 1]                    15,025,920\n","│    └─BatchNorm2d: 2-34                 [32, 42, 65, 4]           [32, 42, 65, 4]           84                        --                        2,688\n","│    └─Sequential: 2-35                  [32, 42, 63, 4]           [32, 42, 65, 4]           --                        --                        --\n","│    │    └─ConvTranspose2d: 3-13        [32, 42, 63, 4]           [32, 42, 65, 4]           5,334                     [3, 1]                    44,378,880\n","│    │    └─BatchNorm2d: 3-14            [32, 42, 65, 4]           [32, 42, 65, 4]           84                        --                        2,688\n","├─BasicBlock: 1-11                       [2080, 1, 42, 4]          [2080, 1, 42, 4]          4                         --                        --\n","│    └─Conv2d: 2-36                      [2080, 1, 42, 4]          [2080, 1, 42, 4]          2                         [1, 1]                    698,880\n","│    └─Conv2d: 2-37                      [2080, 1, 42, 4]          [2080, 1, 42, 4]          2                         [1, 1]                    698,880\n","│    └─Sequential: 2-38                  [2080, 1, 42, 4]          [2080, 1, 42, 4]          --                        --                        --\n","│    │    └─Conv2d: 3-15                 [2080, 1, 42, 4]          [2080, 1, 42, 4]          2                         [1, 1]                    698,880\n","│    │    └─BatchNorm2d: 3-16            [2080, 1, 42, 4]          [2080, 1, 42, 4]          2                         --                        4,160\n","├─Linear: 1-12                           [32, 65, 168]             [32, 65, 24]              4,056                     --                        129,792\n","=====================================================================================================================================================================\n","Total params: 573,875\n","Trainable params: 573,875\n","Non-trainable params: 0\n","Total mult-adds (M): 280.69\n","=====================================================================================================================================================================\n","Input size (MB): 0.13\n","Forward/backward pass size (MB): 62.68\n","Params size (MB): 2.24\n","Estimated Total Size (MB): 65.05\n","====================================================================================================================================================================="]},"metadata":{},"execution_count":247}]},{"cell_type":"markdown","source":["# *Below Code WorthLess Don't Check* "],"metadata":{"id":"G76KGoZkItum"}},{"cell_type":"code","source":["#out = general.check_runs('kinetic-gan', id=-1)\n","out = \"/content/runs/kinetic-gan\"\n","actions_out = os.path.join(out, 'actions')\n","if not os.path.exists(actions_out): os.makedirs(actions_out)\n","\n","config_file = open(os.path.join(out,\"gen_config.txt\"),\"w\")\n","config_file.write(\"Kinetic-GAN.ipynb\" + '|' + str(config))\n","config_file.close()\n","\n","cuda = True if torch.cuda.is_available() else False\n","print(cuda)\n","\n","# Initialize generator \n","generator     = Generator(\n","    config[\"latent_dim\"], \n","    config[\"channels\"], \n","    config[\"n_classes\"], \n","    config[\"t_size\"], \n","    mlp_dim=config[\"mlp_dim\"], \n","    dataset=config[\"dataset\"])\n","\n","if cuda:\n","    generator.cuda()\n","\n","FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n","LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n","\n","# Load Models\n","generator.load_state_dict(torch.load(config[\"model\"], map_location=device),strict=False)\n","generator.eval()\n","\n","new_imgs   = []\n","new_labels = []\n","z_s        = []\n","\n","classes = np.arange(config[\"n_classes\"]) if config[\"label\"] == -1 else [config[\"label\"]]\n","qtd = config[\"batch_size\"]\n","\n","if config[\"stochastic_file\"]!='-':\n","    stoch = np.load(config[\"stochastic_file\"]) \n","    stoch = np.expand_dims(stoch[config[\"stochastic_index\"]], 0)\n","    print(stoch.shape)\n","\n","if config[\"stochastic\"]:  # Generate one latent point \n","    z   = Variable(FloatTensor(np.random.normal(0, 1, (1, config[\"latent_dim\"]))  if config[\"stochastic_file\"] == '-' else stoch ))\n","    z   = z.repeat(qtd*len(classes),1)\n","\n","while(len(classes)>0):\n","\n","    if not config[\"stochastic\"]: # Generate Samples if not in mode stochastic\n","        z         = Variable(FloatTensor(np.random.normal(0, 1, (qtd*len(classes), config[\"latent_dim\"])))) \n","\n","    z         = trunc(z, config[\"mean_size\"], config[\"trunc\"]) if config[\"trunc_mode\"]=='z' else z\n","    labels_np = np.array([num for _ in range(qtd) for num in classes])  # Generate labels\n","    labels    = Variable(LongTensor(labels_np))\n","    gen_imgs  = generator(z, labels, config[\"trunc\"]) if config[\"trunc_mode\"] == 'w' else generator(z, labels)\n","\n","    new_imgs   = gen_imgs.data.cpu()  if len(new_imgs)==0 else np.concatenate((new_imgs, gen_imgs.data.cpu()), axis=0)\n","    new_labels = labels_np if len(new_labels)==0 else np.concatenate((new_labels, labels_np), axis=0)\n","    z_s        = z.cpu()  if len(z_s)==0 else np.concatenate((z_s, z.cpu()), axis=0)   \n","    \n","\n","    tmp     = Counter(new_labels)\n","    classes = [i for i in classes if tmp[i]<config[\"gen_qtd\"]]\n","\n","    print('---------------------------------------------------')\n","    print(tmp)\n","    print(len(new_labels), classes)\n","\n","\n","if config[\"dataset\"] == 'ntu':\n","    new_imgs = np.expand_dims(new_imgs, axis=-1)\n","    \n","\n","new_labels = np.concatenate((np.expand_dims(new_labels, 0), np.expand_dims(new_labels, 0)), axis=0)  # Remove if not needed\n","\n","with open(os.path.join(actions_out, str(config[\"n_classes\"] if config[\"label\"] == -1 else config[\"label\"])+'_'+str(config[\"gen_qtd\"])+('_trunc' + str(config[\"trunc\"]) if config[\"trunc_mode\"]!='-' else '')+('_stochastic' if config[\"stochastic\"] else '')+'_gen_data.npy'), 'wb') as npf:\n","    np.save(npf, new_imgs)\n","\n","\n","with open(os.path.join(actions_out, str(config[\"n_classes\"] if config[\"label\"] == -1 else config[\"label\"])+'_'+str(config[\"gen_qtd\"])+('_trunc' + str(config[\"trunc\"]) if config[\"trunc_mode\"]!='-' else '')+('_stochastic' if config[\"stochastic\"] else '')+'_gen_z.npy'), 'wb') as npf:\n","    np.save(npf, z_s)\n","\n","\n","with open(os.path.join(actions_out, str(config[\"n_classes\"] if config[\"label\"] == -1 else config[\"label\"])+'_'+str(config[\"gen_qtd\"])+('_trunc' + str(config[\"trunc\"]) if config[\"trunc_mode\"]!='-' else '')+('_stochastic' if config[\"stochastic\"] else '')+'_gen_label.pkl'), 'wb') as f:\n","    pickle.dump(new_labels, f)"],"metadata":{"id":"ZP-EresIah4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = np.load(\"/content/runs/kinetic-gan/actions/120_10_trunc0.95_gen_data.npy\")\n","\n","with open(\"/content/runs/kinetic-gan/actions/120_10_trunc0.95_gen_label.pkl\", 'rb') as f:\n","    labels = pickle.load(f)"],"metadata":{"id":"X7tKkTS7nBtt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = np.squeeze(data)\n","labels = labels[0].squeeze()"],"metadata":{"id":"5NNifJOiw-Is"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.shape,labels.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"clRAHm7rxAkX","executionInfo":{"status":"ok","timestamp":1682903604554,"user_tz":-330,"elapsed":2,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"40aedb7d-3734-4755-83a8-fb8fd52254ba"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1200, 3, 64, 25), (1200,))"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["#!rm -r runs/synthetic"],"metadata":{"id":"zDsEtML80O1b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#!python visualization/action_ntu.py --path \"/content/runs/kinetic-gan/actions/120_10_trunc0.95_gen_data.npy\" --indexes 26 86 146 "],"metadata":{"id":"n11EI5JbzPd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gen_skeleton(frame, \n","                 height,\n","                 width,\n","                 mapping_list = [(0, 1), (1, 3), (3, 5), \n","                                 (0, 2), (2, 4), (0, 6), \n","                                 (1, 7), (6, 7), (6, 8), \n","                                 (7, 9), (8, 10), (9, 11)]):\n","    img_3 = np.zeros([height, width,3],dtype=np.uint8)\n","    img_3.fill(255)\n","\n","    # add circles\n","    for coord in frame:\n","        x, y = int(width*coord[0]), int(height*coord[1])\n","        img_3 = cv2.circle(img_3, center=(x,y), radius=1, color=(255, 0, 0), thickness=6)\n","\n","    # add lines\n","    for line in mapping_list:\n","        i, j = line\n","        st = frame[i, :]\n","        start_point = (int(width*st[0]), int(height*st[1]))\n","\n","        en = frame[j, :]\n","        end_point = (int(width*en[0]), int(height*en[1]))\n","\n","        img3_ = cv2.line(img_3, start_point, end_point, color=(0, 0, 0), thickness=3)\n","\n","    return img_3\n","\n","def gen_video(points, \n","              save_file, \n","              frame_h, \n","              frame_w, \n","              is_3d=True,\n","              mapping_list = [(0, 1), (1, 3), (3, 5), \n","                                 (0, 2), (2, 4), (0, 6), \n","                                 (1, 7), (6, 7), (6, 8), \n","                                 (7, 9), (8, 10), (9, 11)]):\n","    # make 3D if points are flatten\n","    if len(points.shape) == 2:\n","        if is_3d:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 3))\n","          y_cds = list(range(1, fts, 3))\n","          z_cds = list(range(2, fts, 3))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds], \n","                                          points[:, z_cds]]), (1,2,0))\n","        else:\n","          fts = points.shape[1]\n","          x_cds = list(range(0, fts, 2))\n","          y_cds = list(range(1, fts, 2))\n","          points = np.transpose(np.array([points[:, x_cds], \n","                                          points[:, y_cds]]), (1,2,0))\n","\n","    size = (frame_w, frame_h)\n","    result = cv2.VideoWriter(save_file,\n","                         cv2.VideoWriter_fourcc(*'MJPG'),\n","                         10, size)\n","\n","    for __id,frame in enumerate(points):\n","        skel_image = gen_skeleton(frame, frame_h, frame_w,mapping_list=mapping_list)\n","        result.write(skel_image)\n","\n","    result.release()"],"metadata":{"id":"W7DRWrS6zWlR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["joint_map = [(3,2),(2,20),(20,4),(4,5),(5,6),(6,7),(7,21),(7,22),(20,8),(8,9),(9,10),(10,11),(11,23),(11,24),\n","            (20,1),(1,0),(0,12),(12,13),(13,14),(14,15),(0,16),(16,17),(17,18),(18,19),(8,16),(4,12),(8,4),(16,12)]"],"metadata":{"id":"VH8CyTRX1-Xz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_vids_dir = \"checking_vids/init\"\n","for index,adata,alabel in enumerate(tqdm(zip(data,labels))):\n","  data = adata\n","  file_id = index\n","  target = alabel\n","  vid_size = [int(adata[3][0][selected_ind]),int(adata[3][1][selected_ind])]\n","\n","  try:\n","    if not os.path.exists(f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\"):\n","      os.makedirs(f\"{save_vids_dir}/{file_id}\",exist_ok=True)\n","      gen_video(data, \n","                f\"{save_vids_dir}/{file_id}/dataloader_out_cls_{target}.mp4\",\n","                vid_size[0], \n","                vid_size[1],\n","                is_3d=False,\n","                mapping_list=joint_map\n","                )\n","  except ValueError:\n","    continue"],"metadata":{"id":"9romojmQ1kMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QyyZB8cIAWuY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WnV-4CoWAti5","executionInfo":{"status":"ok","timestamp":1682907617744,"user_tz":-330,"elapsed":766,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"2d6516d2-12fa-4b7f-db8e-e33eceeae85d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([32, 2, 60, 25])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[],"metadata":{"id":"x7wALGrFA5pS"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"vscode":{"interpreter":{"hash":"544d855d7b0d57add784f15e62ffa31fd790b767434a09f782574915bd2ed2d8"}}},"nbformat":4,"nbformat_minor":0}