{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37586,"status":"ok","timestamp":1683290946742,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"},"user_tz":-330},"id":"iql7zP0D8hcz","outputId":"0b9d7059-3a35-4db2-f7db-412bce1c8d38"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"QZcrUomQjffp","executionInfo":{"status":"ok","timestamp":1683291442191,"user_tz":-330,"elapsed":495457,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}}},"outputs":[],"source":["!unzip -q \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/kinetics-skeleton.zip\" "]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkIfhXqxWhOw","executionInfo":{"status":"ok","timestamp":1683291451234,"user_tz":-330,"elapsed":9055,"user":{"displayName":"Devin De Silva","userId":"07966168235963805458"}},"outputId":"4ad53b88-9a84-45e3-c4c5-bb2c81717b4a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-3-c3265ca6a44d>:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  from tqdm.autonotebook import tqdm\n","<ipython-input-3-c3265ca6a44d>:80: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n","  def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n"]}],"source":["import os\n","import json\n","import random\n","import numpy as np\n","import pandas as pd\n","\n","import torch \n","from torch import nn \n","from torch import optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from tqdm.autonotebook import tqdm\n","import itertools\n","import random\n","import copy\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","import cv2\n","import json\n","from sklearn.model_selection import train_test_split\n","from functools import partial\n","from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","\n","sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n","\n","HAPPY_COLORS_PALETTE = [\"#01BEFE\",\n","                        \"#FFDD00\",\n","                        \"#FF7D00\",\n","                        \"#FF006D\",\n","                        \"#ADFF02\",\n","                        \"#8F00FF\"]\n","\n","sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n","rcParams['figure.figsize'] = 12, 8\n","\n","\n","\"\"\"\n","Collection of functions which enable the evaluation of a classifier's performance,\n","by showing confusion matrix, accuracy, recall, precision etc.\n","\"\"\"\n","\n","import numpy as np\n","import sys\n","\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from tabulate import tabulate\n","import math\n","import logging\n","from datetime import datetime\n","from sklearn.metrics import accuracy_score\n","\n","def save_history(history, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","\n","    with open(f\"{PATH}/{unique_name}.json\", \"w+\") as f0:\n","        json.dump(history, f0)\n","\n","def get_config(file_loc):\n","    file = torch.load(file_loc)\n","    return file[\"model_state_dict\"], file[\"model_config\"], file[\"config\"]\n","    \n","def save_model(model, model_name, unique_name, models_saves, config):\n","    PATH = f\"{models_saves}/{model_name}\"\n","    os.makedirs(PATH, exist_ok=True)\n","    torch.save({\n","        \"n_epochs\": config[\"n_epochs\"],\n","        \"model_state_dict\": model.state_dict(),\n","        \"model_config\": config[\"model\"],\n","        \"config\": config\n","    }, f\"{PATH}/{unique_name}.pt\")\n","\n","def plot_confusion_matrix(ConfMat, label_strings=None, title='Confusion matrix', cmap=plt.cm.get_cmap('Blues')):\n","    \"\"\"Plot confusion matrix in a separate window\"\"\"\n","    plt.imshow(ConfMat, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    if label_strings:\n","        tick_marks = np.arange(len(label_strings))\n","        plt.xticks(tick_marks, label_strings, rotation=90)\n","        plt.yticks(tick_marks, label_strings)\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","\n","def generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row, digits=3, number_of_thieves=2, maxcharlength=35):\n","    \"\"\"\n","    Returns a string of a report for given metric arrays (array length equals the number of classes).\n","    Called internally by `analyze_classification`.\n","        digits: number of digits after . for displaying results\n","        number_of_thieves: number of biggest thieves to report\n","        maxcharlength: max. number of characters to use when displaying thief names\n","    \"\"\"\n","\n","    relative_freq = support / np.sum(support)  # relative frequencies of each class in the true lables\n","    sorted_class_indices = np.argsort(relative_freq)[\n","                            ::-1]  # sort by \"importance\" of classes (i.e. occurance frequency)\n","\n","    last_line_heading = 'avg / total'\n","\n","    width = max(len(cn) for cn in existing_class_names)\n","    width = max(width, len(last_line_heading), digits)\n","\n","    headers = [\"precision\", \"recall\", \"f1-score\", \"rel. freq.\", \"abs. freq.\", \"biggest thieves\"]\n","    fmt = '%% %ds' % width  # first column: class name\n","    fmt += '  '\n","    fmt += ' '.join(['% 10s' for _ in headers[:-1]])\n","    fmt += '|\\t % 5s'\n","    fmt += '\\n'\n","\n","    headers = [\"\"] + headers\n","    report = fmt % tuple(headers)\n","    report += '\\n'\n","\n","    for i in sorted_class_indices:\n","        values = [existing_class_names[i]]\n","        for v in (precision[i], recall[i], f1[i],\n","                    relative_freq[i]):  # v is NOT a tuple, just goes through this list 1 el. at a time\n","            values += [\"{0:0.{1}f}\".format(v, digits)]\n","        values += [\"{}\".format(support[i])]\n","        thieves = np.argsort(ConfMatrix_normalized_row[i, :])[::-1][\n","                    :number_of_thieves + 1]  # other class indices \"stealing\" from class. May still contain self\n","        thieves = thieves[thieves != i]  # exclude self at this point\n","        steal_ratio = ConfMatrix_normalized_row[i, thieves]\n","        thieves_names = [\n","            existing_class_names[thief][:min(maxcharlength, len(existing_class_names[thief]))] for thief\n","            in thieves]  # a little inefficient but inconsequential\n","        string_about_stealing = \"\"\n","        for j in range(len(thieves)):\n","            string_about_stealing += \"{0}: {1:.3f},\\t\".format(thieves_names[j], steal_ratio[j])\n","        values += [string_about_stealing]\n","\n","        report += fmt % tuple(values)\n","\n","    report += '\\n' + 100 * '-' + '\\n'\n","\n","    # compute averages/sums\n","    values = [last_line_heading]\n","    for v in (np.average(precision, weights=relative_freq),\n","                np.average(recall, weights=relative_freq),\n","                np.average(f1, weights=relative_freq)):\n","        values += [\"{0:0.{1}f}\".format(v, digits)]\n","    values += ['{0}'.format(np.sum(relative_freq))]\n","    values += ['{0}'.format(np.sum(support))]\n","    values += ['']\n","\n","    # make last (\"Total\") line for report\n","    report += fmt % tuple(values)\n","\n","    return report\n","\n","\n","def action_evaluator(y_pred, y_true, class_names, excluded_classes=None, maxcharlength=35, print_report=True, show_plot=True):\n","    \"\"\"\n","    For an array of label predictions and the respective true labels, shows confusion matrix, accuracy, recall, precision etc:\n","    Input:\n","        y_pred: 1D array of predicted labels (class indices)\n","        y_true: 1D array of true labels (class indices)\n","        class_names: 1D array or list of class names in the order of class indices.\n","            Could also be integers [0, 1, ..., num_classes-1].\n","        excluded_classes: list of classes to be excluded from average precision, recall calculation (e.g. OTHER)\n","    \"\"\"\n","\n","    # Trim class_names to include only classes existing in y_pred OR y_true\n","    in_pred_labels = set(list(y_pred))\n","    in_true_labels = set(list(y_true))\n","    # print(\"predicted labels > \", in_pred_labels, \"in_true_labels > \", in_true_labels)\n","\n","    existing_class_ind = sorted(list(in_pred_labels | in_true_labels))\n","    # print(\"pred label\", in_pred_labels, \"true label\", in_true_labels)\n","    class_strings = [str(name) for name in class_names]  # needed in case `class_names` elements are not strings\n","    existing_class_names = [class_strings[ind][:min(maxcharlength, len(class_strings[ind]))] for ind in existing_class_ind]  # a little inefficient but inconsequential\n","\n","    # Confusion matrix\n","    ConfMatrix = metrics.confusion_matrix(y_true, y_pred)\n","\n","    # Normalize the confusion matrix by row (i.e by the number of samples in each class)\n","    ConfMatrix_normalized_row = metrics.confusion_matrix(y_true, y_pred, normalize='true') \n","\n","    if show_plot:\n","        plt.figure()\n","        plot_confusion_matrix(ConfMatrix_normalized_row, label_strings=existing_class_names,\n","                                title='Confusion matrix normalized by row')\n","        plt.show(block=False)\n","\n","    # Analyze results\n","    total_accuracy = np.trace(ConfMatrix) / len(y_true)\n","    print('Overall accuracy: {:.3f}\\n'.format(total_accuracy))\n","\n","    # returns metrics for each class, in the same order as existing_class_names\n","    precision, recall, f1, support = metrics.precision_recall_fscore_support(y_true, y_pred, labels=existing_class_ind, zero_division=0)\n","    # Print report\n","    if print_report:\n","        print(generate_classification_report(existing_class_names, precision, recall, f1, support, ConfMatrix_normalized_row))\n","\n","    # Calculate average precision and recall\n","    # prec_avg, rec_avg = get_avg_prec_recall(ConfMatrix, existing_class_names, excluded_classes)\n","    # if excluded_classes:\n","    #     print(\n","    #         \"\\nAverage PRECISION: {:.2f}\\n(using class frequencies as weights, excluding classes with no predictions and predictions in '{}')\".format(\n","    #             prec_avg, ', '.join(excluded_classes)))\n","    #     print(\n","    #         \"\\nAverage RECALL (= ACCURACY): {:.2f}\\n(using class frequencies as weights, excluding classes in '{}')\".format(\n","    #             rec_avg, ', '.join(excluded_classes)))\n","\n","    # Make a histogram with the distribution of classes with respect to precision and recall\n","    # prec_rec_histogram(precision, recall)\n","\n","    return {\"accuracy\": total_accuracy, \"precision\": precision.mean(), \"recall\": recall.mean(), \"f1\": f1.mean()}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"p0XIAvcQX3l0"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","def classname_id(class_name_list):\n","    id2classname = {k:v for k, v in zip(list(range(len(class_name_list))),class_name_list)}\n","    classname2id = {v:k for k, v in id2classname.items()}\n","    return id2classname, classname2id"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"y7gAeWUDX7X6"},"outputs":[],"source":["model_ident = \"Kinetics_skeleton_classifier\"\n","unique_iden = \"epoch25_emb1024_xy\"\n","\n","main_dir = \"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE\"\n","data_files = \"kinetics-skeleton\"\n","\n","train_dir = os.path.join(data_files,\"kinetics_train\")\n","train_info = os.path.join(data_files,\"kinetics_train_label.json\")\n","val_dir = os.path.join(data_files,\"kinetics_val\")\n","val_info = os.path.join(data_files,\"kinetics_val_label.json\")\n","\n","epoch_vids = os.path.join(main_dir,\"epoch_vids\")\n","models_saves = os.path.join(main_dir,\"model_saves\")\n","embeddings_save = os.path.join(main_dir,\"embedding_save\")\n","prototypes_save = os.path.join(main_dir,\"prototypes\")\n","test_vids = os.path.join(main_dir,\"test_vids\")\n","train_ratio = 0.9\n","val_ratio = 0.1\n","batch_size = 128\n","resume_epoch = 0\n","class_names = pd.read_csv(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/kinetics-skeleton/kinetic_ids.csv\")[\"name\"].to_list()\n","\n","os.makedirs(epoch_vids,exist_ok=True)\n","os.makedirs(models_saves,exist_ok=True)\n","os.makedirs(embeddings_save,exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IrsB7xhhaKF5"},"outputs":[],"source":["config = {\n","    \"n_epochs\":25,\n","    \"model_name\":\"BidirectionalLSTM\",\n","    \"dataset\":{\n","        \"num_person_in\":5,\n","        \"num_person_out\":3,\n","\n","    },\n","    \"model\":{\n","        \"seq_len\":200,\n","        \"input_size\":18*2,\n","        \"hidden_size\":1024,\n","        \"linear_filters\":[128,256,512,1024],\n","        \"embedding_size\":1024,\n","        \"num_classes\":len(class_names),\n","        \"num_layers\":1,\n","        \"bidirectional\":True,\n","        \"batch_size\":batch_size,\n","        \"dev\":device\n","    }\n","}\n","\n","id2clsname, clsname2id = classname_id(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Res0UAsOjjki"},"outputs":[],"source":["# sys\n","import os\n","import sys\n","import numpy as np\n","import random\n","import pickle\n","import json\n","# torch\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets, transforms\n","import argparse\n","from numpy.lib.format import open_memmap\n","\n","toolbar_width = 30\n","\n","\n","def downsample(data_numpy, step, random_sample=True):\n","    # input: C,T,V,M\n","    begin = np.random.randint(step) if random_sample else 0\n","    return data_numpy[:, begin::step, :, :]\n","\n","\n","def temporal_slice(data_numpy, step):\n","    # input: C,T,V,M\n","    C, T, V, M = data_numpy.shape\n","    return data_numpy.reshape(C, T / step, step, V, M).transpose(\n","        (0, 1, 3, 2, 4)).reshape(C, T / step, V, step * M)\n","\n","\n","def mean_subtractor(data_numpy, mean):\n","    # input: C,T,V,M\n","    # naive version\n","    if mean == 0:\n","        return\n","    C, T, V, M = data_numpy.shape\n","    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n","    begin = valid_frame.argmax()\n","    end = len(valid_frame) - valid_frame[::-1].argmax()\n","    data_numpy[:, :end, :, :] = data_numpy[:, :end, :, :] - mean\n","    return data_numpy\n","\n","\n","def auto_pading(data_numpy, size, random_pad=False):\n","    C, T, V, M = data_numpy.shape\n","    if T < size:\n","        begin = random.randint(0, size - T) if random_pad else 0\n","        data_numpy_paded = np.zeros((C, size, V, M), dtype=data_numpy.dtype)\n","        data_numpy_paded[:, begin:begin + T, :, :] = data_numpy\n","        return data_numpy_paded\n","    else:\n","        return data_numpy\n","\n","\n","def random_choose(data_numpy, size, auto_pad=True):\n","    # input: C,T,V,M\n","    C, T, V, M = data_numpy.shape\n","    if T == size:\n","        return data_numpy\n","    elif T < size:\n","        if auto_pad:\n","            return auto_pading(data_numpy, size, random_pad=True)\n","        else:\n","            return data_numpy\n","    else:\n","        begin = random.randint(0, T - size)\n","        return data_numpy[:, begin:begin + size, :, :]\n","\n","\n","def random_move(data_numpy,\n","                angle_candidate=[-10., -5., 0., 5., 10.],\n","                scale_candidate=[0.9, 1.0, 1.1],\n","                transform_candidate=[-0.2, -0.1, 0.0, 0.1, 0.2],\n","                move_time_candidate=[1]):\n","    # input: C,T,V,M\n","    C, T, V, M = data_numpy.shape\n","    move_time = random.choice(move_time_candidate)\n","    node = np.arange(0, T, T * 1.0 / move_time).round().astype(int)\n","    node = np.append(node, T)\n","    num_node = len(node)\n","\n","    A = np.random.choice(angle_candidate, num_node)\n","    S = np.random.choice(scale_candidate, num_node)\n","    T_x = np.random.choice(transform_candidate, num_node)\n","    T_y = np.random.choice(transform_candidate, num_node)\n","\n","    a = np.zeros(T)\n","    s = np.zeros(T)\n","    t_x = np.zeros(T)\n","    t_y = np.zeros(T)\n","\n","    # linspace\n","    for i in range(num_node - 1):\n","        a[node[i]:node[i + 1]] = np.linspace(\n","            A[i], A[i + 1], node[i + 1] - node[i]) * np.pi / 180\n","        s[node[i]:node[i + 1]] = np.linspace(S[i], S[i + 1],\n","                                             node[i + 1] - node[i])\n","        t_x[node[i]:node[i + 1]] = np.linspace(T_x[i], T_x[i + 1],\n","                                               node[i + 1] - node[i])\n","        t_y[node[i]:node[i + 1]] = np.linspace(T_y[i], T_y[i + 1],\n","                                               node[i + 1] - node[i])\n","\n","    theta = np.array([[np.cos(a) * s, -np.sin(a) * s],\n","                      [np.sin(a) * s, np.cos(a) * s]])\n","\n","    # perform transformation\n","    for i_frame in range(T):\n","        xy = data_numpy[0:2, i_frame, :, :]\n","        new_xy = np.dot(theta[:, :, i_frame], xy.reshape(2, -1))\n","        new_xy[0] += t_x[i_frame]\n","        new_xy[1] += t_y[i_frame]\n","        data_numpy[0:2, i_frame, :, :] = new_xy.reshape(2, V, M)\n","\n","    return data_numpy\n","\n","\n","def random_shift(data_numpy):\n","    # input: C,T,V,M\n","    C, T, V, M = data_numpy.shape\n","    data_shift = np.zeros(data_numpy.shape)\n","    valid_frame = (data_numpy != 0).sum(axis=3).sum(axis=2).sum(axis=0) > 0\n","    begin = valid_frame.argmax()\n","    end = len(valid_frame) - valid_frame[::-1].argmax()\n","\n","    size = end - begin\n","    bias = random.randint(0, T - size)\n","    data_shift[:, bias:bias + size, :, :] = data_numpy[:, begin:end, :, :]\n","\n","    return data_shift\n","\n","\n","def openpose_match(data_numpy):\n","    C, T, V, M = data_numpy.shape\n","    assert (C == 3)\n","    score = data_numpy[2, :, :, :].sum(axis=1)\n","    # the rank of body confidence in each frame (shape: T-1, M)\n","    rank = (-score[0:T - 1]).argsort(axis=1).reshape(T - 1, M)\n","\n","    # data of frame 1\n","    xy1 = data_numpy[0:2, 0:T - 1, :, :].reshape(2, T - 1, V, M, 1)\n","    # data of frame 2\n","    xy2 = data_numpy[0:2, 1:T, :, :].reshape(2, T - 1, V, 1, M)\n","    # square of distance between frame 1&2 (shape: T-1, M, M)\n","    distance = ((xy2 - xy1)**2).sum(axis=2).sum(axis=0)\n","\n","    # match pose\n","    forward_map = np.zeros((T, M), dtype=int) - 1\n","    forward_map[0] = range(M)\n","    for m in range(M):\n","        choose = (rank == m)\n","        forward = distance[choose].argmin(axis=1)\n","        for t in range(T - 1):\n","            distance[t, :, forward[t]] = np.inf\n","        forward_map[1:][choose] = forward\n","    assert (np.all(forward_map >= 0))\n","\n","    # string data\n","    for t in range(T - 1):\n","        forward_map[t + 1] = forward_map[t + 1][forward_map[t]]\n","\n","    # generate data\n","    new_data_numpy = np.zeros(data_numpy.shape)\n","    for t in range(T):\n","        new_data_numpy[:, t, :, :] = data_numpy[:, t, :,\n","                                                forward_map[t]].transpose(\n","                                                    1, 2, 0)\n","    data_numpy = new_data_numpy\n","\n","    # score sort\n","    trace_score = data_numpy[2, :, :, :].sum(axis=1).sum(axis=0)\n","    rank = (-trace_score).argsort()\n","    data_numpy = data_numpy[:, :, :, rank]\n","\n","    return data_numpy\n","\n","\n","def top_k_by_category(label, score, top_k):\n","    instance_num, class_num = score.shape\n","    rank = score.argsort()\n","    hit_top_k = [[] for i in range(class_num)]\n","    for i in range(instance_num):\n","        l = label[i]\n","        hit_top_k[l].append(l in rank[i, -top_k:])\n","\n","    accuracy_list = []\n","    for hit_per_category in hit_top_k:\n","        if hit_per_category:\n","            accuracy_list.append(\n","                sum(hit_per_category) * 1.0 / len(hit_per_category))\n","        else:\n","            accuracy_list.append(0.0)\n","    return accuracy_list\n","\n","\n","def calculate_recall_precision(label, score):\n","    instance_num, class_num = score.shape\n","    rank = score.argsort()\n","    confusion_matrix = np.zeros([class_num, class_num])\n","\n","    for i in range(instance_num):\n","        true_l = label[i]\n","        pred_l = rank[i, -1]\n","        confusion_matrix[true_l][pred_l] += 1\n","\n","    precision = []\n","    recall = []\n","\n","    for i in range(class_num):\n","        true_p = confusion_matrix[i][i]\n","        false_n = sum(confusion_matrix[i, :]) - true_p\n","        false_p = sum(confusion_matrix[:, i]) - true_p\n","        precision.append(true_p * 1.0 / (true_p + false_p))\n","        recall.append(true_p * 1.0 / (true_p + false_n))\n","\n","    return precision, recall\n","\n","\n","class KineticsFeeder(torch.utils.data.Dataset):\n","    \"\"\" Feeder for skeleton-based action recognition in kinetics-skeleton dataset\n","    Arguments:\n","        data_path: the path to '.npy' data, the shape of data should be (N, C, T, V, M)\n","        label_path: the path to label\n","        random_choose: If true, randomly choose a portion of the input sequence\n","        random_shift: If true, randomly pad zeros at the begining or end of sequence\n","        random_move: If true, perform randomly but continuously changed transformation to input sequence\n","        window_size: The length of the output sequence\n","        pose_matching: If ture, match the pose between two frames\n","        num_person_in: The number of people the feeder can observe in the input sequence\n","        num_person_out: The number of people the feeder in the output sequence\n","        debug: If true, only use the first 100 samples\n","    \"\"\"\n","    def __init__(self,\n","                 data_path,\n","                 label_path,\n","                 ignore_empty_sample=True,\n","                 random_choose=False,\n","                 random_shift=False,\n","                 random_move=False,\n","                 is_2d = True,\n","                 window_size=-1,\n","                 pose_matching=False,\n","                 num_person_in=5,\n","                 num_person_out=2,\n","                 seq_len=200,\n","                 debug=False):\n","        self.debug = debug\n","        self.seq_len = seq_len\n","        self.data_path = data_path\n","        self.label_path = label_path\n","        self.random_choose = random_choose\n","        self.random_shift = random_shift\n","        self.is_2d = is_2d\n","        self.random_move = random_move\n","        self.window_size = window_size\n","        self.num_person_in = num_person_in\n","        self.num_person_out = num_person_out\n","        self.pose_matching = pose_matching\n","        self.ignore_empty_sample = ignore_empty_sample\n","\n","        self.load_data()\n","\n","    def select_frames(self,sequence):\n","      if sequence.shape[1]<self.seq_len:\n","        times = self.seq_len//sequence.shape[1] + 1\n","\n","        sequence = sequence.repeat(1,times,1,1)\n","\n","\n","      sel_index = sorted(random.sample(range(sequence.shape[1]),self.seq_len))\n","      return sequence[:,sel_index,...]\n","    \n","    def select_person(self,sequence):\n","      person = random.randint(0,sequence.size()[0] - 1)\n","      return sequence[person,...]\n","\n","    def load_data(self):\n","        # load label\n","        label_path = self.label_path\n","        with open(label_path) as f:\n","            label_info = json.load(f)\n","        \n","        # load file list\n","        self.sample_name = [f\"{x}.json\" for x in list(label_info)]\n","        filter = []\n","        for sample in tqdm(self.sample_name,desc=\"Filtered\"):\n","          if os.path.exists(f\"{self.data_path}/{sample}\"):\n","            filter.append(sample)\n","\n","        self.sample_name = filter\n","\n","        if self.debug:\n","            self.sample_name = self.sample_name[0:2]\n","\n","        sample_id = [name.split('.')[0] for name in self.sample_name]\n","        self.label = np.array(\n","            [label_info[id]['label_index'] for id in sample_id])\n","        has_skeleton = np.array(\n","            [label_info[id]['has_skeleton'] for id in sample_id])\n","\n","        # ignore the samples which does not has skeleton sequence\n","        if self.ignore_empty_sample:\n","            self.sample_name = [\n","                s for h, s in zip(has_skeleton, self.sample_name) if h\n","            ]\n","            self.label = self.label[has_skeleton]\n","\n","        # output data shape (N, C, T, V, M)\n","        self.N = len(self.sample_name)  #sample\n","        self.C = 3  #channel\n","        self.T = 300  #frame\n","        self.V = 18  #joint\n","        self.M = self.num_person_out  #person\n","\n","    def __len__(self):\n","        return len(self.sample_name)\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __getitem__(self, index):\n","\n","        # output shape (M, T, V, C)\n","        # get data\n","        sample_name = self.sample_name[index]\n","        sample_path = os.path.join(self.data_path, sample_name)\n","        with open(sample_path, 'r') as f:\n","            video_info = json.load(f)\n","\n","        # fill data_numpy\n","        data_numpy = np.zeros((self.C, self.T, self.V, self.num_person_in))\n","        for frame_info in video_info['data']:\n","            frame_index = frame_info['frame_index']\n","            for m, skeleton_info in enumerate(frame_info[\"skeleton\"]):\n","                if m >= self.num_person_in:\n","                    break\n","                pose = skeleton_info['pose']\n","                score = skeleton_info['score']\n","                data_numpy[0, frame_index, :, m] = pose[0::2]\n","                data_numpy[1, frame_index, :, m] = pose[1::2]\n","                data_numpy[2, frame_index, :, m] = score\n","\n","        # centralization\n","        data_numpy[0:2] = data_numpy[0:2] - 0.5\n","        data_numpy[0][data_numpy[2] == 0] = 0\n","        data_numpy[1][data_numpy[2] == 0] = 0\n","\n","        # get & check label index\n","        label = video_info['label_index']\n","        assert (self.label[index] == label)\n","\n","        # data augmentation\n","        if self.random_shift:\n","            data_numpy = random_shift(data_numpy)\n","        if self.random_choose:\n","            data_numpy = random_choose(data_numpy, self.window_size)\n","        elif self.window_size > 0:\n","            data_numpy = auto_pading(data_numpy, self.window_size)\n","        if self.random_move:\n","            data_numpy = random_move(data_numpy)\n","\n","        # sort by score\n","        sort_index = (-data_numpy[2, :, :, :].sum(axis=1)).argsort(axis=1)\n","        for t, s in enumerate(sort_index):\n","            data_numpy[:, t, :, :] = data_numpy[:, t, :, s].transpose(\n","                (1, 2, 0))\n","        data_numpy = data_numpy[:, :, :, 0:self.num_person_out]\n","\n","        # match poses between 2 frames\n","        if self.pose_matching:\n","            data_numpy = openpose_match(data_numpy)\n","\n","        \n","        coords = torch.from_numpy(data_numpy).float()\n","        coords = torch.transpose(coords,0,-1)\n","\n","        if self.is_2d:\n","            coords = coords[...,0:2]\n","\n","        coords = self.select_frames(coords)\n","        coords = self.select_person(coords)\n","\n","        coords = torch.flatten(coords, start_dim=-2)\n","        return coords,coords, label,[340,256]\n","\n","    def top_k(self, score, top_k):\n","        assert (all(self.label >= 0))\n","\n","        rank = score.argsort()\n","        hit_top_k = [l in rank[i, -top_k:] for i, l in enumerate(self.label)]\n","        return sum(hit_top_k) * 1.0 / len(hit_top_k)\n","\n","    def top_k_by_category(self, score, top_k):\n","        assert (all(self.label >= 0))\n","        return top_k_by_category(self.label, score, top_k)\n","\n","    def calculate_recall_precision(self, score):\n","        assert (all(self.label >= 0))\n","        return calculate_recall_precision(self.label, score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["1fc780b0efb84494b41908807ddab917","e3248ba13f62428d868799a6868ad2ca"]},"id":"o0Mlr88WEjWt","outputId":"7361cce6-2110-4208-9b3d-e8cae33d46fa"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fc780b0efb84494b41908807ddab917","version_major":2,"version_minor":0},"text/plain":["Filtered:   0%|          | 0/246534 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e3248ba13f62428d868799a6868ad2ca","version_major":2,"version_minor":0},"text/plain":["Filtered:   0%|          | 0/19906 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_data = feeder = KineticsFeeder(data_path=train_dir,\n","                            label_path=train_info,\n","                            num_person_in=config[\"dataset\"][\"num_person_in\"],\n","                            num_person_out=config[\"dataset\"][\"num_person_out\"],\n","                            seq_len=config[\"model\"][\"seq_len\"])\n","\n","val_data = feeder = KineticsFeeder(data_path=val_dir,\n","                            label_path=val_info,\n","                            num_person_in=config[\"dataset\"][\"num_person_in\"],\n","                            num_person_out=config[\"dataset\"][\"num_person_out\"],\n","                            seq_len=config[\"model\"][\"seq_len\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iThGfDYvch4Q","outputId":"1dd5c9fe-2f27-49b3-8877-b8922eacd766"},"outputs":[{"data":{"text/plain":["(240436, 19796)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(train_data),len(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"B6mtux6OEmIk"},"outputs":[],"source":["train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n","val_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n","test_dl = DataLoader(val_data, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vvWK87kamZH5","outputId":"071ab6db-5b7b-47c1-986b-cfacc81413a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([128, 200, 36]) torch.Size([128, 200, 36]) torch.Size([128])\n"]}],"source":["for adata in train_dl:\n","  print(adata[0].size(), adata[1].size(), adata[2].size())\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNhRUvwyFUQb"},"outputs":[],"source":["class BiLSTMEncoder(nn.Module):\n","    def __init__(self,seq_len, input_size,num_classes, hidden_size,linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMEncoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev=dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","        self.num_classes = num_classes\n","\n","        # define LSTM layer\n","        self.layers = []\n","\n","        # add linear layers \n","        for __id,layer_out in enumerate(self.linear_filters):\n","            if __id == 0:\n","                self.layers.append(nn.Linear(self.input_size, layer_out))\n","            else:\n","                self.layers.append(nn.Linear(self.linear_filters[__id-1], layer_out))\n","\n","        # add lstm layer\n","        self.lstm = nn.LSTM(input_size = layer_out, hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=self.bidirectional,\n","                            batch_first=True)\n","        \n","        self.net = nn.Sequential(*self.layers)\n","\n","        self.classification_header = nn.Linear(self.embedding_size,self.num_classes)\n","\n","        #add embedding out\n","        if bidirectional:\n","            self.bn = nn.BatchNorm1d(self.hidden_size*4)\n","            self.out_linear = nn.Linear(self.hidden_size*4, self.embedding_size)\n","        else:\n","            self.bn = nn.BatchNorm1d(self.hidden_size*2)\n","            self.out_linear = nn.Linear(self.hidden_size*2, self.embedding_size)\n","\n","        \n","    def forward(self, x_input):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        x = self.net(x_input)\n","        lstm_out, self.hidden = self.lstm(x)\n","        hidden_transformed = torch.cat(self.hidden,0)\n","        hidden_transformed = torch.transpose(hidden_transformed,0,1)\n","        hidden_transformed = torch.flatten(hidden_transformed,start_dim=1)\n","\n","        #hidden_transformed = self.bn(hidden_transformed)\n","        hidden_transformed = self.out_linear(hidden_transformed)\n","\n","        label = self.classification_header(hidden_transformed)\n","        \n","        return label, hidden_transformed\n","\n","    \n","class BiLSTMDecoder(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size, linear_filters,embedding_size:int, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMDecoder, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.seq_len = seq_len\n","\n","        if bidirectional:\n","            self.input_linear = nn.Linear(self.embedding_size,4*self.hidden_size)\n","        else:\n","            self.input_linear = nn.Linear(self.embedding_size,2*self.hidden_size)\n","\n","        # define LSTM layer\n","        self.layers = []\n","        # add lstm\n","        self.lstm = nn.LSTM(input_size = self.linear_filters[0], hidden_size = self.hidden_size,\n","                            num_layers = self.num_layers, bidirectional=True,\n","                            batch_first=bidirectional)\n","\n","                        \n","        # add linear layers \n","        if bidirectional:\n","            self.layers.append(nn.Linear(2*hidden_size,self.linear_filters[0]))\n","        else:\n","            self.layers.append(nn.Linear(hidden_size,self.linear_filters[0]))\n","\n","        for __id,layer_in in enumerate(self.linear_filters):\n","            if __id == len(linear_filters)-1:\n","                self.layers.append(nn.Linear(layer_in,self.input_size))\n","            else:\n","                self.layers.append(nn.Linear(layer_in,self.linear_filters[__id+1]))\n","\n","        self.net = nn.Sequential(*self.layers)\n","\n","        \n","        \n","\n","    def forward(self,encoder_hidden):\n","        \"\"\"\n","        : param x_input:               input of shape (seq_len, # in batch, input_size)\n","        : return lstm_out, hidden:     lstm_out gives all the hidden states in the sequence; hidden gives the hidden state and cell state for the last element in the sequence\n","        \"\"\"\n","        \n","        \n","        hidden_shape = encoder_hidden.shape\n","        encoder_hidden = self.input_linear(encoder_hidden)\n","        \n","        if self.bidirectional:\n","            hidden = encoder_hidden.view((-1,4,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h1,h2,c1,c2 = torch.unbind(hidden,0)\n","            h,c = torch.stack((h1,h2)),torch.stack((c1,c2))\n","            bs = h.size()[1]\n","        else:\n","            hidden = encoder_hidden.view((-1,2,self.hidden_size))\n","            hidden = torch.transpose(hidden,1,0)\n","            h,c = torch.unbind(hidden,0)\n","            bs = h.size()[1]\n","        \n","        dummy_input = torch.rand((bs,self.seq_len,self.hidden_size), requires_grad=True).to(self.dev)\n","        \n","        lstm_out, self.hidden = self.lstm(dummy_input,(h,c))\n","        x = self.net(lstm_out)\n","        \n","        return x\n","\n","class BiLSTMEncDecModel(nn.Module):\n","    def __init__(self,seq_len, input_size, hidden_size,num_classes, linear_filters=[128,256,512],embedding_size:int=256, num_layers = 1,bidirectional=True,dev=device):\n","        super(BiLSTMEncDecModel, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.dev = dev\n","        self.num_layers = num_layers\n","        self.linear_filters = linear_filters[::-1]\n","        self.embedding_size = embedding_size\n","        self.bidirectional = bidirectional\n","        self.batch_size = batch_size\n","        self.seq_len = seq_len\n","        self.num_classes= num_classes\n","        \n","        self.encoder = BiLSTMEncoder(seq_len, input_size, num_classes,hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n","        self.decoder = BiLSTMDecoder(seq_len, input_size, hidden_size, linear_filters,embedding_size, num_layers = 1,bidirectional=True, dev=self.dev)\n","        \n","    def forward(self,x):\n","        label,embedding = self.encoder(x)\n","        decoder_out = self.decoder(embedding)\n","        \n","        return decoder_out, embedding, label\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxOl8YuW6f71"},"outputs":[],"source":["encoder = BiLSTMEncoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    num_classes = config[\"model\"][\"num_classes\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"]).to(device)\n","\n","decoder = BiLSTMDecoder(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"]).to(device)\n","\n","bilstm_model = BiLSTMEncDecModel(\n","    seq_len=config[\"model\"][\"seq_len\"],\n","    input_size=config[\"model\"][\"input_size\"],\n","    num_classes = config[\"model\"][\"num_classes\"],\n","    hidden_size=config[\"model\"][\"hidden_size\"],\n","    linear_filters=config[\"model\"][\"linear_filters\"],\n","    embedding_size=config[\"model\"][\"embedding_size\"],\n","    num_layers = config[\"model\"][\"num_layers\"],\n","    bidirectional=config[\"model\"][\"bidirectional\"],\n","    dev=config[\"model\"][\"dev\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1681588316700,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"RETBAwj46i3Z","outputId":"a46edca4-f654-44b9-cd94-343b29782ec4"},"outputs":[{"data":{"text/plain":["BiLSTMEncDecModel(\n","  (encoder): BiLSTMEncoder(\n","    (lstm): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n","    (net): Sequential(\n","      (0): Linear(in_features=36, out_features=128, bias=True)\n","      (1): Linear(in_features=128, out_features=256, bias=True)\n","      (2): Linear(in_features=256, out_features=512, bias=True)\n","      (3): Linear(in_features=512, out_features=1024, bias=True)\n","    )\n","    (classification_header): Linear(in_features=1024, out_features=400, bias=True)\n","    (bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (out_linear): Linear(in_features=4096, out_features=1024, bias=True)\n","  )\n","  (decoder): BiLSTMDecoder(\n","    (input_linear): Linear(in_features=1024, out_features=4096, bias=True)\n","    (lstm): LSTM(1024, 1024, batch_first=True, bidirectional=True)\n","    (net): Sequential(\n","      (0): Linear(in_features=2048, out_features=1024, bias=True)\n","      (1): Linear(in_features=1024, out_features=512, bias=True)\n","      (2): Linear(in_features=512, out_features=256, bias=True)\n","      (3): Linear(in_features=256, out_features=128, bias=True)\n","      (4): Linear(in_features=128, out_features=36, bias=True)\n","    )\n","  )\n",")"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["bilstm_model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1739,"status":"ok","timestamp":1681588318413,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"F5Kn20ZT6m4H","outputId":"855a2a5d-7eb3-4e2e-edfe-dcb18903fc6f"},"outputs":[{"data":{"text/plain":["(torch.Size([128, 1024]), torch.Size([128, 400]))"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["label, embedding = encoder(torch.randn((batch_size,config[\"model\"][\"seq_len\"],config[\"model\"][\"input_size\"])).to(device))\n","embedding.shape,label.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8t79v6xM6wx7"},"outputs":[],"source":["label_map = [(k,v) for k,v in id2clsname.items()]\n","labelToId = {x[0]: i for i, x in enumerate(label_map)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mfa8NjY57En3"},"outputs":[],"source":["def combined_loss(pred_sequence,pred_label,true_sequence,true_label,loss_module,alpha_target=1,alpha_recon=1):\n","    recon_loss = alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence)\n","    tar_loss = alpha_target*loss_module[\"target_loss\"](pred_label,true_label)\n","    loss =  recon_loss + tar_loss\n","\n","    #print(alpha_recon*loss_module[\"reconstruction_loss\"](pred_sequence,true_sequence))\n","    #print(alpha_target*loss_module[\"target_loss\"](pred_label,true_label))\n","\n","    return loss, {\n","        \"reconstruction_loss\":recon_loss.item(),\n","        \"target_loss\":tar_loss.item()\n","    }\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brTkTW9BEa3i"},"outputs":[],"source":["optimizer = torch.optim.Adam(bilstm_model.parameters(), lr=1e-3, weight_decay=0.01)\n","std_loss = {\n","    \"reconstruction_loss\" :nn.L1Loss(reduction='mean').to(device),\n","    \"target_loss\" :nn.CrossEntropyLoss(reduction=\"mean\").to(device)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RUigg8dNcWJO"},"outputs":[],"source":["def plot_curves(df):\n","    df['loss'] = df['loss']/df['samples']\n","    df['feat. loss'] = df['feat. loss']/df['samples']\n","    df['classi. loss'] = df['classi. loss']/df['samples']\n","    \n","    fig, axs = plt.subplots(nrows=4)\n","    sns.lineplot(data=df, x='epoch', y='loss', hue='phase', marker='o', ax=axs[2]).set(title=\"Loss\")\n","    sns.lineplot(data=df, x='epoch', y='feat. loss', hue='phase', marker='o', ax=axs[0]).set(title=\"Feature Loss\")\n","    sns.lineplot(data=df, x='epoch', y='classi. loss', hue='phase', marker='o', ax=axs[1]).set(title=\"Classification Loss\")\n","    sns.lineplot(data=df, x='epoch', y='accuracy', hue='phase', marker='o', ax=axs[3]).set(title=\"Accuracy\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DksWHRbgDgPQ"},"outputs":[],"source":["def train_step(model, dataloader, optimizer, loss_module, device, class_names):\n","    model = model.train()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    targets = []\n","    predicts = []\n","\n","    with tqdm(dataloader, unit=\"batch\", desc=\"train\") as tepoch:\n","          for input_sequence,target_sequence,target_action,target_vid_size in tepoch:\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = target_sequence.to(device)\n","            target_action = target_action.to(device)\n","            \n","\n","            # Zero gradients, perform a backward pass, and update the weights.\n","            optimizer.zero_grad()\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(True):\n","            # with autocast():\n","                predicted_sequence,_,predicted_label  = model(input_sequence)\n","                loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","\n","            \n","            class_output = torch.argmax(predicted_label,dim=1)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            metrics = {\"loss\": loss.item()}\n","            with torch.no_grad():\n","                total_samples += len(target_action)\n","                epoch_loss += loss.item()  # add total loss of batch\n","\n","            # convert feature vector into action class using cosine\n","            pred_class = class_output.cpu().detach().numpy()\n","            metrics[\"accuracy\"] = accuracy_score(y_true=target_action.cpu().detach().numpy(), y_pred=pred_class)\n","            tepoch.set_postfix(metrics)\n","\n","            targets.append(target_action.cpu().detach().numpy())\n","            predicts.append(pred_class)\n","\n","    \n","    predicts = np.concatenate(predicts)\n","    targets = np.concatenate(targets)\n","    #train_metrics = action_evaluator(predicts,targets,class_names=list(clsname2id.keys()),print_report=False)\n","\n","    epoch_loss = epoch_loss / total_samples  # average loss per sample for whole epoch\n","    return metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmXFlJJdEN9N"},"outputs":[],"source":["def eval_step(model, dataloader,loss_module, device, class_names,  print_report=False, show_plot=False):\n","    model = model.eval()\n","    epoch_loss = 0  # total loss of epoch\n","    total_samples = 0  # total samples in epoch\n","    per_batch = {'targets': [], 'predictions': [], 'metrics': []}\n","    metrics = {\"samples\": 0, \"loss\": 0, \"feat. loss\": 0, \"classi. loss\": 0}\n","\n","    with torch.no_grad():\n","      with tqdm(dataloader, unit=\"batch\", desc=\"eval\") as tepoch:\n","        for input_sequence,target_sequence,target_action,target_vid_size in tepoch:\n","\n","            input_sequence = input_sequence.to(device)\n","            target_sequence = target_sequence.to(device)\n","            target_action = target_action.to(device)\n","\n","            # forward track history if only in train\n","            with torch.set_grad_enabled(False):\n","            # with autocast():\n","                predicted_sequence,_,predicted_label  = model(input_sequence)\n","\n","                loss,loss_detail = combined_loss(predicted_sequence,predicted_label, target_sequence, target_action,std_loss)\n","            \n","            pred_action = torch.argmax(predicted_label,dim=1)\n","\n","            with torch.no_grad():\n","                metrics['samples'] += len(target_action)\n","                metrics['loss'] += loss.item()  # add total loss of batch\n","                metrics['feat. loss'] += loss_detail[\"reconstruction_loss\"]\n","                metrics['classi. loss'] += loss_detail[\"target_loss\"]\n","\n","            per_batch['targets'].append(target_action.cpu().numpy())\n","            per_batch['predictions'].append(pred_action.cpu().numpy())\n","            per_batch['metrics'].append([loss.cpu().numpy()])\n","\n","            tepoch.set_postfix({\"loss\": loss.item()})\n","\n","    all_preds = np.concatenate(per_batch[\"predictions\"])\n","    all_targets = np.concatenate(per_batch[\"targets\"])\n","    metrics_dict = action_evaluator(y_pred=all_preds, y_true=all_targets, class_names=class_names, print_report=print_report, show_plot=show_plot)\n","    metrics_dict.update(metrics)\n","    return metrics_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":857,"status":"ok","timestamp":1681588648509,"user":{"displayName":"Devin De Silva","userId":"10564767910270225856"},"user_tz":-330},"id":"gEfzDnIP81qB","outputId":"b3919e2b-295e-46bc-d834-ef2b35d5795d"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model_params, model_config, config = get_config(\"/content/drive/MyDrive/22_FYP42 - Zero-shot Explainable HAR/Devin/SkeletonAE/model_saves/temp_Kinetics_skeleton_classifier/1__epoch25_emb1024_xy.pt\")\n","resume_epoch = 1\n","bilstm_model.load_state_dict(model_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["89edad4adbfa43a3a55738f8af7a2ea4","4df0696ca7624d4fad1c9883facd9e0e","e576c4d12ea04494a20c0204860ecba3","04af5f61d5c14227b0b632015c72ff0c","7ee4fd90109348a4a48c02f6f8421072","a5e06007671a4f02b0cef9168a9b396f","eb76ef03d46544169f71588ef746500a","274fc54de6974314b079df7fab1ff394","8892c3361e9f469196d764d2f18f346b","190c32edd85241b38994f653a60e5021","940f5f1a72ba4eaeb2134ce321bff2f4","c63d0777a671423bb3fc6ffac21d0441","8204c0029d4346a7849bfaf1d5527d73","b25531b71d6f4f5496ac1dd96505db16","ccdc03c332304369b277f2ddfc3fc048","b73dd05586e240a1adf23b840c0f27a1","63c0f910f95749e6b7b87641c408d6c3","8c0d05e4db0b4de8a3971d0733fb1500","b399a60203cb41759f1d923c808cd248","2ede4720de404b26a1f38ede535c2a85","b5f05100ee13469dadeb49c33b1959a5","ce38059a901744b6b82dacc87440a1da"]},"id":"wyUEFUlp7HOt","outputId":"ba75873d-6b05-457f-a309-af1f63e1b88c"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"89edad4adbfa43a3a55738f8af7a2ea4","version_major":2,"version_minor":0},"text/plain":["Training Epoch:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c63d0777a671423bb3fc6ffac21d0441","version_major":2,"version_minor":0},"text/plain":["train:   0%|          | 0/1879 [00:00<?, ?batch/s]"]},"metadata":{},"output_type":"display_data"}],"source":["best_model_wts = copy.deepcopy(bilstm_model.state_dict())\n","best_acc = 0.0\n","\n","train_data = []\n","for epoch in tqdm(range(resume_epoch+1, config[\"n_epochs\"] + resume_epoch+1), desc='Training Epoch', leave=False):\n","  \n","  train_metrics = train_step(bilstm_model, train_dl, optimizer, std_loss, device, class_names)\n","  train_metrics['epoch'] = epoch\n","  train_metrics['phase'] = 'train'\n","  train_data.append(train_metrics)\n","  \n","  eval_metrics = eval_step(bilstm_model, val_dl,std_loss, device, class_names,  print_report=False, show_plot=False)\n","  eval_metrics['epoch'] = epoch \n","  eval_metrics['phase'] = 'valid'\n","  train_data.append(eval_metrics)\n","\n","  save_model(\n","    bilstm_model, \n","    f\"temp_{model_ident}\", \n","    f\"{epoch}__{unique_iden}\",\n","      models_saves, \n","      config)\n","    \n","  if eval_metrics['accuracy'] > best_acc:\n","    best_model = copy.deepcopy(bilstm_model.state_dict())\n","  \n","train_df = pd.DataFrame().from_records(train_data)\n","plot_curves(train_df)\n","\n","# replace by best model \n","bilstm_model.load_state_dict(best_model)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d43PGLU_7ijI"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"04af5f61d5c14227b0b632015c72ff0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_190c32edd85241b38994f653a60e5021","placeholder":"​","style":"IPY_MODEL_940f5f1a72ba4eaeb2134ce321bff2f4","value":" 0/25 [00:00&lt;?, ?it/s]"}},"190c32edd85241b38994f653a60e5021":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"274fc54de6974314b079df7fab1ff394":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ede4720de404b26a1f38ede535c2a85":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4df0696ca7624d4fad1c9883facd9e0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5e06007671a4f02b0cef9168a9b396f","placeholder":"​","style":"IPY_MODEL_eb76ef03d46544169f71588ef746500a","value":"Training Epoch:   0%"}},"63c0f910f95749e6b7b87641c408d6c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ee4fd90109348a4a48c02f6f8421072":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8204c0029d4346a7849bfaf1d5527d73":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63c0f910f95749e6b7b87641c408d6c3","placeholder":"​","style":"IPY_MODEL_8c0d05e4db0b4de8a3971d0733fb1500","value":"train:   0%"}},"8892c3361e9f469196d764d2f18f346b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89edad4adbfa43a3a55738f8af7a2ea4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4df0696ca7624d4fad1c9883facd9e0e","IPY_MODEL_e576c4d12ea04494a20c0204860ecba3","IPY_MODEL_04af5f61d5c14227b0b632015c72ff0c"],"layout":"IPY_MODEL_7ee4fd90109348a4a48c02f6f8421072"}},"8c0d05e4db0b4de8a3971d0733fb1500":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"940f5f1a72ba4eaeb2134ce321bff2f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5e06007671a4f02b0cef9168a9b396f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b25531b71d6f4f5496ac1dd96505db16":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b399a60203cb41759f1d923c808cd248","max":1879,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ede4720de404b26a1f38ede535c2a85","value":7}},"b399a60203cb41759f1d923c808cd248":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5f05100ee13469dadeb49c33b1959a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b73dd05586e240a1adf23b840c0f27a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c63d0777a671423bb3fc6ffac21d0441":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8204c0029d4346a7849bfaf1d5527d73","IPY_MODEL_b25531b71d6f4f5496ac1dd96505db16","IPY_MODEL_ccdc03c332304369b277f2ddfc3fc048"],"layout":"IPY_MODEL_b73dd05586e240a1adf23b840c0f27a1"}},"ccdc03c332304369b277f2ddfc3fc048":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5f05100ee13469dadeb49c33b1959a5","placeholder":"​","style":"IPY_MODEL_ce38059a901744b6b82dacc87440a1da","value":" 7/1879 [00:22&lt;1:59:01,  3.81s/batch, loss=6.17, accuracy=0]"}},"ce38059a901744b6b82dacc87440a1da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e576c4d12ea04494a20c0204860ecba3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_274fc54de6974314b079df7fab1ff394","max":25,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8892c3361e9f469196d764d2f18f346b","value":0}},"eb76ef03d46544169f71588ef746500a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}