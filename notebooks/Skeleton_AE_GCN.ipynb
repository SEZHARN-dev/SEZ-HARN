{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Skeleton Data Encoder Module \n",
        "> Base on 2019 paper SGN [github](https://github.com/microsoft/SGN/tree/master)"
      ],
      "metadata": {
        "id": "yTg_btdF20ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT0DIBORnMiO",
        "outputId": "3b917ba0-af4c-443a-8ee1-4198f33c774e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.9/dist-packages (1.7.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import math\n",
        "import torchinfo"
      ],
      "metadata": {
        "id": "NjF3zzjUkJpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z65-ZQj3kFBd"
      },
      "outputs": [],
      "source": [
        "class norm_data(nn.Module):\n",
        "    def __init__(self, dim=3, joints=20):\n",
        "        super(norm_data, self).__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(dim*joints)\n",
        "\n",
        "    def forward(self, x):\n",
        "        bs, c, num_joints, step = x.size()\n",
        "        x = x.view(bs, -1, step)\n",
        "        x = self.bn(x)\n",
        "        x = x.view(bs, -1, num_joints, step).contiguous()\n",
        "        return x\n",
        "\n",
        "class embed(nn.Module):\n",
        "    def __init__(self, dim=3, joint=20, hidden_dim=128, norm=True, bias=False):\n",
        "        super(embed, self).__init__()\n",
        "\n",
        "        if norm:\n",
        "            self.cnn = nn.Sequential(\n",
        "                norm_data(dim, joint),\n",
        "                cnn1x1(dim, 64, bias=bias),\n",
        "                nn.ReLU(),\n",
        "                cnn1x1(64, hidden_dim, bias=bias),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "        else:\n",
        "            self.cnn = nn.Sequential(\n",
        "                cnn1x1(dim, 64, bias=bias),\n",
        "                nn.ReLU(),\n",
        "                cnn1x1(64, hidden_dim, bias=bias),\n",
        "                nn.ReLU(),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "class cnn1x1(nn.Module):\n",
        "    def __init__(self, dim1 = 3, dim2 =3, bias = True):\n",
        "        super(cnn1x1, self).__init__()\n",
        "        self.cnn = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        return x\n",
        "\n",
        "class local(nn.Module):\n",
        "    def __init__(self, dim1 = 3, dim2 = 3, bias = False):\n",
        "        super(local, self).__init__()\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d((1, None))\n",
        "        self.cnn1 = nn.Conv2d(dim1, dim1, kernel_size=(1, 3), padding=(0, 1), bias=bias)\n",
        "        self.bn1 = nn.BatchNorm2d(dim1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.cnn2 = nn.Conv2d(dim1, dim2, kernel_size=1, bias=bias)\n",
        "        self.bn2 = nn.BatchNorm2d(dim2)\n",
        "        self.dropout = nn.Dropout2d(0.2)\n",
        "\n",
        "    def forward(self, x1):\n",
        "        x1 = self.maxpool(x1)\n",
        "        x = self.cnn1(x1)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.cnn2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class gcn_spa(nn.Module):\n",
        "    def __init__(self, in_feature, out_feature, bias = False):\n",
        "        super(gcn_spa, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(out_feature)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.w = cnn1x1(in_feature, out_feature, bias=False)\n",
        "        self.w1 = cnn1x1(in_feature, out_feature, bias=bias)\n",
        "\n",
        "\n",
        "    def forward(self, x1, g):\n",
        "        x = x1.permute(0, 3, 2, 1).contiguous()\n",
        "        x = g.matmul(x)\n",
        "        x = x.permute(0, 3, 2, 1).contiguous()\n",
        "        x = self.w(x) + self.w1(x1)\n",
        "        x = self.relu(self.bn(x))\n",
        "        return x\n",
        "\n",
        "class compute_g_spa(nn.Module):\n",
        "    def __init__(self, dim1 = 64 *3, dim2 = 64*3, bias = False):\n",
        "        super(compute_g_spa, self).__init__()\n",
        "        self.dim1 = dim1\n",
        "        self.dim2 = dim2\n",
        "        self.g1 = cnn1x1(self.dim1, self.dim2, bias=bias)\n",
        "        self.g2 = cnn1x1(self.dim1, self.dim2, bias=bias)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x1):\n",
        "\n",
        "        g1 = self.g1(x1).permute(0, 3, 2, 1).contiguous()\n",
        "        g2 = self.g2(x1).permute(0, 3, 1, 2).contiguous()\n",
        "        g3 = g1.matmul(g2)\n",
        "        g = self.softmax(g3)\n",
        "        return g\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SGN(nn.Module):\n",
        "    def __init__(self, num_classes, num_joint, seg, hidden_size=128, bs=32, train=True, bias=True, device='cpu'):\n",
        "        super(SGN, self).__init__()\n",
        "\n",
        "        self.dim1 = hidden_size\n",
        "        self.seg = seg\n",
        "        self.num_joint = num_joint\n",
        "        self.bs = bs\n",
        "        if train:\n",
        "            self.spa = self.one_hot(bs, num_joint, self.seg)\n",
        "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
        "            self.tem = self.one_hot(bs, self.seg, num_joint)\n",
        "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
        "        else:\n",
        "            self.spa = self.one_hot(bs * 5, num_joint, self.seg)\n",
        "            self.spa = self.spa.permute(0, 3, 2, 1).to(device)\n",
        "            self.tem = self.one_hot(bs * 5, self.seg, num_joint)\n",
        "            self.tem = self.tem.permute(0, 3, 1, 2).to(device)\n",
        "\n",
        "        self.tem_embed = embed(self.seg, joint=12, hidden_dim=64*4, norm=False, bias=bias)\n",
        "        self.spa_embed = embed(num_joint, joint=12, hidden_dim=64, norm=False, bias=bias)\n",
        "        self.joint_embed = embed(3, joint=12, hidden_dim=64, norm=True, bias=bias)\n",
        "        self.dif_embed = embed(3, joint=12, hidden_dim=64, norm=True, bias=bias)\n",
        "        self.maxpool = nn.AdaptiveMaxPool2d([1, 1])\n",
        "        self.cnn = local(self.dim1, self.dim1 * 2, bias=bias)\n",
        "        self.compute_g1 = compute_g_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
        "        self.gcn1 = gcn_spa(self.dim1 // 2, self.dim1 // 2, bias=bias)\n",
        "        self.gcn2 = gcn_spa(self.dim1 // 2, self.dim1, bias=bias)\n",
        "        self.gcn3 = gcn_spa(self.dim1, self.dim1, bias=bias)\n",
        "        self.fc = nn.Linear(self.dim1 * 2, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "\n",
        "        nn.init.constant_(self.gcn1.w.cnn.weight, 0)\n",
        "        nn.init.constant_(self.gcn2.w.cnn.weight, 0)\n",
        "        nn.init.constant_(self.gcn3.w.cnn.weight, 0)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        \n",
        "        # Dynamic Representation\n",
        "        # bs, step, dim = input.size()\n",
        "        # num_joints = dim //3\n",
        "        input = input.view((self.bs, self.seg, self.num_joint, 3))\n",
        "        input = input.permute(0, 3, 2, 1).contiguous()\n",
        "        dif = input[:, :, :, 1:] - input[:, :, :, 0:-1]\n",
        "        dif = torch.cat([dif.new(self.bs, dif.size(1), self.num_joint, 1).zero_(), dif], dim=-1)\n",
        "        # print(input.shape)\n",
        "        pos = self.joint_embed(input)\n",
        "        tem1 = self.tem_embed(self.tem)\n",
        "        spa1 = self.spa_embed(self.spa)\n",
        "        dif = self.dif_embed(dif)\n",
        "        dy = pos + dif\n",
        "        # Joint-level Module\n",
        "        input= torch.cat([dy, spa1], 1)\n",
        "        g = self.compute_g1(input)\n",
        "        input = self.gcn1(input, g)\n",
        "        input = self.gcn2(input, g)\n",
        "        input = self.gcn3(input, g)\n",
        "        # Frame-level Module\n",
        "        input = input + tem1\n",
        "        input = self.cnn(input)\n",
        "        output_feat = torch.squeeze(input)\n",
        "        # Classification\n",
        "        # output = self.maxpool(input)\n",
        "        # output = torch.flatten(output, 1)\n",
        "        # output = self.fc(output)\n",
        "\n",
        "        return output_feat\n",
        "\n",
        "    def one_hot(self, bs, spa, tem):\n",
        "\n",
        "        y = torch.arange(spa).unsqueeze(-1)\n",
        "        y_onehot = torch.FloatTensor(spa, spa)\n",
        "\n",
        "        y_onehot.zero_()\n",
        "        y_onehot.scatter_(1, y, 1)\n",
        "\n",
        "        y_onehot = y_onehot.unsqueeze(0).unsqueeze(0)\n",
        "        y_onehot = y_onehot.repeat(bs, tem, 1, 1)\n",
        "\n",
        "        return y_onehot"
      ],
      "metadata": {
        "id": "YkqdPvqBkRw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgn = SGN(num_classes=40, num_joint=12, seg=60, hidden_size=256, bs=64, train=True)\n",
        "# sgn\n",
        "# torchinfo.summary(sgn, input_size=(32, 50, 36), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"
      ],
      "metadata": {
        "id": "_0anpBJFnhZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXcTGWX9AeXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(sgn, input_size=(64, 60, 36), col_names = (\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUTa2EF_soh5",
        "outputId": "c30ea1b8-5e66-46aa-b9d2-befcc32ba47b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=====================================================================================================================================================================\n",
              "Layer (type:depth-idx)                   Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
              "=====================================================================================================================================================================\n",
              "SGN                                      [64, 60, 36]              [64, 512, 60]             20,520                    --                        --\n",
              "├─embed: 1-1                             [64, 3, 12, 60]           [64, 64, 12, 60]          --                        --                        --\n",
              "│    └─Sequential: 2-1                   [64, 3, 12, 60]           [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─norm_data: 3-1               [64, 3, 12, 60]           [64, 3, 12, 60]           72                        --                        4,608\n",
              "│    │    └─cnn1x1: 3-2                  [64, 3, 12, 60]           [64, 64, 12, 60]          256                       --                        11,796,480\n",
              "│    │    └─ReLU: 3-3                    [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─cnn1x1: 3-4                  [64, 64, 12, 60]          [64, 64, 12, 60]          4,160                     --                        191,692,800\n",
              "│    │    └─ReLU: 3-5                    [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "├─embed: 1-2                             [64, 60, 12, 60]          [64, 256, 12, 60]         --                        --                        --\n",
              "│    └─Sequential: 2-2                   [64, 60, 12, 60]          [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─cnn1x1: 3-6                  [64, 60, 12, 60]          [64, 64, 12, 60]          3,904                     --                        179,896,320\n",
              "│    │    └─ReLU: 3-7                    [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─cnn1x1: 3-8                  [64, 64, 12, 60]          [64, 256, 12, 60]         16,640                    --                        766,771,200\n",
              "│    │    └─ReLU: 3-9                    [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "├─embed: 1-3                             [64, 12, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    └─Sequential: 2-3                   [64, 12, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─cnn1x1: 3-10                 [64, 12, 12, 60]          [64, 64, 12, 60]          832                       --                        38,338,560\n",
              "│    │    └─ReLU: 3-11                   [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─cnn1x1: 3-12                 [64, 64, 12, 60]          [64, 64, 12, 60]          4,160                     --                        191,692,800\n",
              "│    │    └─ReLU: 3-13                   [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "├─embed: 1-4                             [64, 3, 12, 60]           [64, 64, 12, 60]          --                        --                        --\n",
              "│    └─Sequential: 2-4                   [64, 3, 12, 60]           [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─norm_data: 3-14              [64, 3, 12, 60]           [64, 3, 12, 60]           72                        --                        4,608\n",
              "│    │    └─cnn1x1: 3-15                 [64, 3, 12, 60]           [64, 64, 12, 60]          256                       --                        11,796,480\n",
              "│    │    └─ReLU: 3-16                   [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "│    │    └─cnn1x1: 3-17                 [64, 64, 12, 60]          [64, 64, 12, 60]          4,160                     --                        191,692,800\n",
              "│    │    └─ReLU: 3-18                   [64, 64, 12, 60]          [64, 64, 12, 60]          --                        --                        --\n",
              "├─compute_g_spa: 1-5                     [64, 128, 12, 60]         [64, 60, 12, 12]          --                        --                        --\n",
              "│    └─cnn1x1: 2-5                       [64, 128, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-19                 [64, 128, 12, 60]         [64, 256, 12, 60]         33,024                    [1, 1]                    1,521,745,920\n",
              "│    └─cnn1x1: 2-6                       [64, 128, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-20                 [64, 128, 12, 60]         [64, 256, 12, 60]         33,024                    [1, 1]                    1,521,745,920\n",
              "│    └─Softmax: 2-7                      [64, 60, 12, 12]          [64, 60, 12, 12]          --                        --                        --\n",
              "├─gcn_spa: 1-6                           [64, 128, 12, 60]         [64, 128, 12, 60]         --                        --                        --\n",
              "│    └─cnn1x1: 2-8                       [64, 128, 12, 60]         [64, 128, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-21                 [64, 128, 12, 60]         [64, 128, 12, 60]         16,384                    [1, 1]                    754,974,720\n",
              "│    └─cnn1x1: 2-9                       [64, 128, 12, 60]         [64, 128, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-22                 [64, 128, 12, 60]         [64, 128, 12, 60]         16,512                    [1, 1]                    760,872,960\n",
              "│    └─BatchNorm2d: 2-10                 [64, 128, 12, 60]         [64, 128, 12, 60]         256                       --                        16,384\n",
              "│    └─ReLU: 2-11                        [64, 128, 12, 60]         [64, 128, 12, 60]         --                        --                        --\n",
              "├─gcn_spa: 1-7                           [64, 128, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    └─cnn1x1: 2-12                      [64, 128, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-23                 [64, 128, 12, 60]         [64, 256, 12, 60]         32,768                    [1, 1]                    1,509,949,440\n",
              "│    └─cnn1x1: 2-13                      [64, 128, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-24                 [64, 128, 12, 60]         [64, 256, 12, 60]         33,024                    [1, 1]                    1,521,745,920\n",
              "│    └─BatchNorm2d: 2-14                 [64, 256, 12, 60]         [64, 256, 12, 60]         512                       --                        32,768\n",
              "│    └─ReLU: 2-15                        [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "├─gcn_spa: 1-8                           [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    └─cnn1x1: 2-16                      [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-25                 [64, 256, 12, 60]         [64, 256, 12, 60]         65,536                    [1, 1]                    3,019,898,880\n",
              "│    └─cnn1x1: 2-17                      [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "│    │    └─Conv2d: 3-26                 [64, 256, 12, 60]         [64, 256, 12, 60]         65,792                    [1, 1]                    3,031,695,360\n",
              "│    └─BatchNorm2d: 2-18                 [64, 256, 12, 60]         [64, 256, 12, 60]         512                       --                        32,768\n",
              "│    └─ReLU: 2-19                        [64, 256, 12, 60]         [64, 256, 12, 60]         --                        --                        --\n",
              "├─local: 1-9                             [64, 256, 12, 60]         [64, 512, 1, 60]          --                        --                        --\n",
              "│    └─AdaptiveMaxPool2d: 2-20           [64, 256, 12, 60]         [64, 256, 1, 60]          --                        --                        --\n",
              "│    └─Conv2d: 2-21                      [64, 256, 1, 60]          [64, 256, 1, 60]          196,864                   [1, 3]                    755,957,760\n",
              "│    └─BatchNorm2d: 2-22                 [64, 256, 1, 60]          [64, 256, 1, 60]          512                       --                        32,768\n",
              "│    └─ReLU: 2-23                        [64, 256, 1, 60]          [64, 256, 1, 60]          --                        --                        --\n",
              "│    └─Dropout2d: 2-24                   [64, 256, 1, 60]          [64, 256, 1, 60]          --                        --                        --\n",
              "│    └─Conv2d: 2-25                      [64, 256, 1, 60]          [64, 512, 1, 60]          131,584                   [1, 1]                    505,282,560\n",
              "│    └─BatchNorm2d: 2-26                 [64, 512, 1, 60]          [64, 512, 1, 60]          1,024                     --                        65,536\n",
              "│    └─ReLU: 2-27                        [64, 512, 1, 60]          [64, 512, 1, 60]          --                        --                        --\n",
              "=====================================================================================================================================================================\n",
              "Total params: 682,360\n",
              "Trainable params: 682,360\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 16.49\n",
              "=====================================================================================================================================================================\n",
              "Input size (MB): 0.55\n",
              "Forward/backward pass size (MB): 1205.45\n",
              "Params size (MB): 2.65\n",
              "Estimated Total Size (MB): 1208.65\n",
              "====================================================================================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-eC-I78N0yMt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}